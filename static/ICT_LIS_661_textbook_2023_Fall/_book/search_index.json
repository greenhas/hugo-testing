[["index.html", "Introduction to Data Science A Remixed Textbook for ICT/LIS 661 at the Uniersity of Kentucky [Fall 2023 Edition] 1 Introduction", " Introduction to Data Science A Remixed Textbook for ICT/LIS 661 at the Uniersity of Kentucky [Fall 2023 Edition] Spencer P. Greenhalgh, PhD 1 Introduction "],["module-1-understanding.html", "2 Module 1 Understanding", " 2 Module 1 Understanding "],["m1a-install-r-and-rstudio.html", "3 M1A: Install R and RStudio 3.1 Introduction 3.2 R 3.3 RStudio 3.4 References", " 3 M1A: Install R and RStudio This content draws on material from STAT 545 by Jenny Bryan, licensed under CC BY-SA 4.0 Changes to the source material include addition of new material; light editing; rearranging, removing, and combining original material; adding and changing links; and adding first-person language from current author. The resulting content is licensed under CC BY-SA 4.0. 3.1 Introduction My formal training is in education, so I have some strong opinions about what learning looks like and what good teaching ought to look like. In particular, I hold to a sociocultural view of learning that assumes that: knowledge is distributed in the world among individuals, the tools, artifacts, and books that they use, and the communities and practices in which they participate (Greeno et al., 1996, p. 20) In other words, I can’t teach you data science by merely rattling off a list of facts for you to memorize and then repeat at the appropriate time. Rather, if I’m going to effectively teach you data science, I need to introduce you to data science communities, have you use the tools that data scientists use, and have you act in the way that data scientists act. In relation to this second point, R and RStudio are software that are widely used in the world of data science, so becoming familiar with them is part of learning data science. Some data scientists prefer other software, and that’s fine, but this is what we’ve decided on teaching here in UK’s School of Information Science (and it’s what I personally use, so I’m better suited to teaching it anyway). This activity is about introducing you to this software and helping you set it up. Even if you have a pre-existing installation of R or RStudio, I highly recommend that you re-install both and get as current as possible. It can be considerably harder to run old software than new. 3.2 R R is an open source programming language designed for statistics. Two things are important about that initial description: First, R is open source, meaning that is freely available and that other programmers may add to it or modify it to their heart’s content. This is good news—it means that in addition to the basic features of R, it is possible (and relatively easy) to add new features by installing and loading packages. We’ll be doing plenty of that this semester. Second, R is designed for statistics. That doesn’t mean it can’t be used for other things: In my research, I regularly use R, but I rarely use it for traditional statistics. Nonetheless, R is built with statistical needs and tasks in mind. You can (and should now) install R from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system (as opposed to the source code). Follow the link for your operating system at the CRAN link in this paragraph. (You’ll probably notice that version names for R are… eccentric!). 3.3 RStudio Programming in R can be done in a number of ways, but in this class, we’ll be using an IDE (integrated development environment) called RStudio (developed by an organization called Posit). To download RStudio Desktop, navigate to this this link. It will provide you with a link for downloading R; since you’ve already done that, you can ignore it. What you shouldn’t ignore, though, is the link it will provide to download RStudio for your computer’s operatins system. It’s important to understand that RStudio is one of many interfaces available for working with the R programming language. Another interface (simply called R) will also be installed on your computer as a result of installing the R language, and it will be possible to open R code in either the R interface or the RStudio interface. Make sure that you always open code in RStudio—not only do I find it cleaner and easier to work with code there, but RStudio also has some extra features that we’ll be using throughout the semester. 3.4 References Greeno, J., Collins, A., &amp; Resnick, L. (1996). Cognition and learning. In D. Berliner &amp; R. Calfee (eds.), Handbook of educational psychology (pp. 15-46). Macmillan. "],["m1c-introduce-yourself-to-the-class.html", "4 M1C: Introduce Yourself to the Class", " 4 M1C: Introduce Yourself to the Class You will complete the Module 1 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt: One of the hardest parts about teaching online classes is getting to know my students! For this week’s connection activity, please introduce yourself to me and to your classmates. You might include details like your pronouns, what program you’re in, your current (or expected future) job, and what you like to do for fun. I’d also be interested to hear why you’re taking this class, what you’re hoping to learn, and any big questions or concerns that you have. As with all discussion posts in this course, I strongly encourage you to read over and respond to what your classmates have posted. However, as important as meaningful interaction is, I’m skeptical that it can be structured or required, so I will never require you to do so. That said, I do expect that you will make an effort to contribute to meaningful interaction within the class. To help with this, I’m experimenting with the “like” feature in Canvas discussions (and with sorting posts by the number of “likes” they receive, though you won’t be able to see others’ posts before adding your own). Feel free to use “liking” to add to this interaction! "],["m2u-the-new-and-shiny-science-of-data.html", "5 M2U: The New(?) and Shiny(?) Science of Data 5.1 Data, Data Science, and Big Data 5.2 The Many Skills of Data Science 5.3 References", " 5 M2U: The New(?) and Shiny(?) Science of Data This chapter draws on material from: Introduction: Why Data Science Needs Feminism by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0 1: The Power Chapter by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0 Version 3: An Introduction to Data Science by Jeffrey Stanton, licensed under CC BY-NC-SA 3.0 Changes to the source material include adding new material; editing, reformatting, and rearranging of original material; adding links; adding or replacing images; changing the citation style; changing original authors’ voice to third person; and adding first-person language from current author. The resulting content is licensed under CC BY-NC-SA 3.0. 5.1 Data, Data Science, and Big Data Data science refers to an emerging area of work concerned with the collection, preparation, analysis, visualization, management, and preservation of large collections of information, sometimes referred to as big data. 5.1.1 Data To introduce data science, it makes sense that we ought to talk about data first. The word data is the plural of the the Latin word datum. One quick word before we continue: Because the word data is the plural of datum, I (and many people) prefer data as a plural noun—hence “What are Data?” for the section title. (In fact, I think it’s funny to define data science as “the science of datums,” but that’s a terrible joke and I promise I won’t do it again in this book). However, it’s quite common in American English to treat data as a singular word—so common in fact, that you might notice me trip up and write “What is Data?” at some point. My opinion here is strong enough that I won’t mind if you point out when I’m inconsistent but not so strong that I’m going to get picky about how you treat the word—go with whatever comes more naturally to you. Even though we rarely use the singular datum, it’s worth briefly exploring its etymology. The word means “a given”—that is, something taken for granted. That’s important: The word data was introduced in the mid-seventeenth century to supplement existing terms such as evidence and fact. Identifying information as data, rather than as either of those other two terms, served a rhetorical purpose (Poovey, 1998; Posner &amp; Klein, 2017; Rosenberg, 2013). It converted otherwise debatable information into the solid basis for subsequent claims. Modern usage of the word data started in the 1940s and 1950s as practical electronic computers began to input, process, and output data. When computers work with data, all of that data has to be broken down to individual bits as the “atoms” that make up data. A bit is a binary unit of data, meaning that it is only capable of representing one of two values: 0 and 1. That doesn’t carry a lot of information by itself (at best, “yes” vs. “no” or TRUE vs. FALSE). However, by combining bits, we can increase the amount of information that we transmit. For example, even a combination of just two bits can express four different values: 00, 01, 10 and 11. Every time you add a new bit you double the number of possible messages you can send. So three bits would give eight options and four bits would give 16 options. When we get up to eight bits—which provides 256 different combinations—we finally have something of a reasonably useful size to work with. Eight bits is commonly referred to as a byte—this term probably started out as a play on words with the word bit (and four bits is sometimes referred to as a nibble or a nybble, because nerds like jokes). A byte offers enough different combinations to encode all of the letters of the (English) alphabet, including capital and small letters. There is an old rulebook called ASCII—the American Standard Code for Information Interchange—which matches up patterns of eight bits with the letters of the alphabet, punctuation, and a few other odds and ends. For example the bit pattern 0100 0001 represents the capital letter A and the next higher pattern 0100 0010 represents capital B. This is more background than anything else—most of the time (but not all of the time!) you don’t need to know the details of what’s going on here to carry out data science. However, it is important to have a foundational understanding that when we’re working with data in this class, the computer is ultimately dealing with everything as bits and translating combinations of bits into words, pictures, numbers, and other formats that makes sense for humans. This background is also helpful for pointing out that just like the word data has connotations related to trustworthiness, it also has connotations of things that are digital and quantitative. While all of these connotations are reasonable, it’s important that we understand their limits. For example, while many people think of data as numbers alone, data can also consist of words or stories, colors or sounds, or any type of information that is systematically collected, organized, and analyzed. Some folks might resist that broad definition of data because “words or stories” told by a person don’t feel as trustworthy or objective as numbers stored in a computer. However, one of the recurring themes of this course is to emphasize that data and data systems are not objective—even when they’re digital and quantitative. When I was introducing ASCII a few paragraphs ago, there were two details in there that might have passed you by but that actually have pretty important consequences. First, I noted that ASCII can “encode all the letters of the (English) alphabet)”; second, I mentioned that the “A” in ASCII stood for “American.” Early computer systems in the United States were built around American English assumptions for what counts as a letter. This makes sense… but it has had consequences! While most modern computer systems have moved on to more advanced character encoding systems (ones that include Latin letters, Chinese characters, Arabic script, and emoji, for example), there are still some really important computer systems that use limited encoding schemes like ASCII. In 2015, Tovin Lapin wrote a newspaper article about this, noting that: Every year in California thousands of parents choose names such as José, André, and Sofía for their children, often honoring the memory of a deceased grandmother, aunt or sibling. On the state-issued birth certificates, though, those names will be spelled incorrectly. California, like several other states, prohibits the use of diacritical marks or accents on official documents. That means no tilde (~), no accent grave (`), no umlaut (¨) and certainly no cedilla (¸). Although more than a third of the state population is Hispanic, and accents are used in the names of state parks and landmarks, the state bars their use on birth records. There were attempts in 2014 to change this, but when lawmakers realized it would cost $10 million to update computer systems, things stalled. Moral of the story: even though ASCII is a straightforward technical system built on digital data with no real wiggle room for what means what, it’s still subjective and biased. How we organize data and data systems matters! So, even digital and quantitative data (systems) can be biased, which means that we ought to push lightly back against the rhetorical connotations of data as trustworthy. I’m not suggesting we throw data, science, and data science out the window and go with our gut and our opinions, but we shouldn’t take for granted that a given dataset doesn’t have its own subjectivity. Likewise, we ought to ask ourselves what information needs to become data before it can be trusted—or, more precisely, whose information needs to become data before it can be considered as fact and acted upon (Lanius, 2015; Porter, 1996). 5.1.2 Data Science If you think about it, data science is somewhat of an unintuitive name. As we noted earlier, data are more than numbers: they can be any type of information that is systematically collected, organized, and analyzed. Likewise, science simply implies a commitment to systematic methods of observation and experiment. Given these definitions, data science clearly means something more limited than science that uses data. History, for example, is a commitment to systematic methods of observation and relies on information that is systematically collected, organized, and analyzed, but we (generally) shouldn’t expect a historian to describe themselves (or be accepted) as a data scientist. Also relevant here is a critique by data and statistics expert Nate Silver, who once quipped that: I think data-scientist is a sexed up term for a statistician (Stats &amp; Data Science Views, 2013) There’s a certain amount of hype, prestige, and even sexiness associated with the term data science, and this ought to encourage us to be critical about how the term is used. Is data science just statistics gussied up to sound cooler? If all scientists use data, who is allowed to have access to the hype, prestige, and sexiness associated with data science? It is also important to acknowledge the elephant in the server room: the demographics of data science (and related occupations like software engineering and artificial intelligence research) do not represent the population as a whole. According to 2018 data from the US Bureau of Labor Statistics, released in 2018, only 26 percent of those in “computer and mathematical occupations” are women (US Bureau of Labor Statistics, 2019). Across all of those women, only 12 percent are Black or Latinx women, even though Black and Latinx women make up 22.5 percent of the US population. (Women of Color in Computing Collaborative, 2018). A report by the research group AI Now about the diversity crisis in artificial intelligence notes that women comprise only 15 percent of AI research staff at Facebook and 10 percent at Google (Myers, Whittaker, &amp; Crawford, 2019). These numbers are probably not a surprise. The more surprising thing is that those numbers are getting worse, not better. According to a research report published by the American Association of University Women in 2015, women computer science graduates in the United States peaked in the mid-1980s at 37 percent, and we have seen a steady decline in the years since then to 26 percent today (Corbett &amp; Hill, 2015). As “data analysts” (low-status number crunchers) have become rebranded as “data scientists” (high status researchers), women are being pushed out in order to make room for more highly valued and more highly compensated men (Fouad, 2014). Disparities in the higher education pipeline aren’t only along gender lines. The same report noted specific underrepresentation for Native American women, multiracial women, white women, and all Black and Latinx people. So is it really a surprise that each day brings a new example of data science being used to disempower and oppress minoritized groups? In 2018, it was revealed that Amazon had been developing an algorithm to screen its first-round job applicants. But because the model had been trained on the resumes of prior applicants, who were predominantly male, it developed an even stronger preference for male applicants. It downgraded resumes with the word women and graduates of women’s colleges. Ultimately, Amazon had to cancel the project (Gershgorn, 2018; Kraus, 2018). This example reinforces the work of Safiya Umoja Noble (2018), whose book, Algorithms of Oppression, has shown how both gender and racial biases are encoded into some of the most pervasive data-driven systems—including Google search, which boasts over five billion unique web searches per day. Noble describes how, as recently as 2016, comparable searches for “three Black teenagers” and “three white teenagers” turned up wildly different representations of those teens. The former returned mugshots, while the latter returned wholesome stock photography. Unequal representation among data scientists is all the worse for the way it can lead to biases in data science projects. 5.1.3 Big Data One possible distinction between data science and statistics is the amount of data we’re working with. Technology coverage in the 2010s (and continuing to the present) made it hard to resist the idea that big data represents some kind of revolution that has turned the whole world of information and technology topsy-turvy. But is this really true? Does big data change everything? Business analyst Doug Laney suggested that three characteristics make big data different from what came before: volume, velocity, and variety. Volume refers to the sheer amount of data. Velocity focuses on how quickly data arrives as well as how quickly those data become “stale.” Finally, variety reflects the fact that there may be many different kinds of data. Together, these three characteristics are often referred to as the “three Vs” model of big data. Note, however, that even before the dawn of the computer age we’ve had a variety of data, some of which arrives quite quickly, and that can add up to quite a lot of total storage over time. Think, for example, of the large variety and volume of data that has arrived annually at Library of Congress since the 1800s! So, it is difficult to tell that big data is fundamentally a brand new thing. Furthermore, there are some concerns that we should exercise when it comes to big data. For example, when a data set gets to a certain size (into the range of thousands of rows), conventional tests of statistical significance are meaningless, because even the most tiny and trivial results are statistically significant. We’ll talk more about statistical significance later in the semester; for the time being, though, it suffices to say that statistical significance is how researchers have traditionally determined whether their results are important or not. If big data makes statistical significance more likely, then researchers who have access to more data will get more important results, whether or not that’s actually true in practical terms! Besides that, the quality and suitability of the data matters a lot: More data does not always mean better data. 5.2 The Many Skills of Data Science Data science owes a lot to statistics and mathematics, so you’d be forgiven for thinking of a data scientist as a statistician in a white lab coat staring fixedly at blinking computer screens filled with scrolling numbers. This isn’t quite the case, though: There is much to be accomplished in the world of data science for those of us who are more comfortable working with words, lists, photographs, sounds, and other kinds of information. In addition, data science is much more than simply analyzing data. There are many people who enjoy analyzing data and who could happily spend all day looking at histograms and averages, but for those who prefer other activities, data science offers a range of roles and requires a range of skills. Here are some skills that are particularly useful: Learning the application domain: A data scientist must quickly learn how the data will be used in a particular context. Communicating with data users: A data scientist must possess strong skills for learning the needs and preferences of users. Translating back and forth between the technical terms of computing and statistics and the vocabulary of the application domain is a critical skill. Seeing the big picture of a complex system: After developing an understanding of the application domain, a data scientist must imagine how data will move around among all of the relevant systems and people. Knowing how data can be represented: A data scientist must have a clear understanding about how data can be stored and linked, as well as about “metadata” (data that describes how other data are arranged). Data transformation and analysis: When data become available for the use of decision makers, a data scientist must know how to transform, summarize, and make inferences from the data. As noted above, being able to communicate the results of analyses to users is also a critical skill here. Visualization and presentation: Although numbers often have the edge in precision and detail, a good data display (e.g., a bar chart) can often be a more effective means of communicating re- sults to data users. Attention to quality: No matter how good a set of data may be, there is no such thing as perfect data. A data scientist must know the limitations of the data they work with, know how to quantify its accuracy, and be able to make suggestions for improving the quality of the data in the future. Ethical reasoning: If data are important enough to collect, they are often important enough to affect people’s lives. A data scientist must understand important ethical issues such as privacy and must be able to communicate the limitations of data to try to prevent misuse of data or analytical results. While a keen understanding of numbers and mathematics is important, particularly for data analysis, a data scientist also needs to have excellent communication skills, be a great systems thinker, have a good eye for visual displays, and be highly capable of thinking critically about how data will be used to make decisions and affect people’s lives. Of course there are very few people who are good at all of these things, so some of the people interested in data will specialize in one area, while others will become experts in another area. Of course, this also highlights the importance of teamwork. We can’t possibly cover all these skills in depth this semester, and even these skills are just the tip of the iceberg, which just emphasizes what a wide range is represented here. I hope their importance is clear, though—for example, which of these skills could have anticipated and responded to the problems involved with María showing up as Maria on a California birth certificate? 5.3 References Corbett, C., &amp; Hill, C. (2015). Solving the equation: The variables for women’s success in engineering and computing. American Association of University Women. Fouad, N. A. (2014, August). Learning in, but getting pushed back (and out). Paper presented at the American Psychological Association. https://www.apa.org/news/press/releases/2014/08/pushed-back.pdf Gershgorn, D. (2018, October 22). Companies are on the hook if their hiring algorithms are biased. Quartz. https://qz.com/1427621/companies-are-on-the-hook-if-their-hiring-algorithms-are-biased Kraus, R. (2018, October 10). Amazon used AI to promote diversity. Too bad it’s plagued with gender bias. Mashable. https://mashable.com/article/amazon-sexist-recruiting-algorithm-gender-bias-ai#VSsbMcGmvqqa Lanius, C. (2015, January 12). Fact check: Your demand for statistical proof is racist. Cyborgology. https://thesocietypages.org/cyborgology/2015/01/12/fact-check-your-demand-for-statistical-proof-is-racist/ Lapan, T. (2015, April 11). California birth certificates and accents: O’Connor alright, Ramón and José is not. The Guardian. https://www.theguardian.com/us-news/2015/apr/11/california-birth-certifcates-accents-marks Myers, S. W., Whittaker, M., &amp; Carwford, K. (2019). Discriminating systems: Gender, race and power in AI. AI Now Institute. https://ainowinstitute.org/publication/discriminating-systems-gender-race-and-power-in-ai-2 Noble, S. U. (2018). Algorithms of oppression: How search engines reinforce racism. NYU Press. Poovey, M. (1998). A history of the modern fact: Problems of knowledge in the sciences of wealth and society. University of Chicago Press. Porter, T. M. (1996). Trust in numbers: The pursuit of objectivity in science and public life. Princeton University Press. Posner, M., &amp; Klein, L. F. (2017). Editor’s introduction: Data as media. Feminist Media Histories, 3(3), 1-8. Rosenberg, D. (2013). Data before the fact. In L. Gitelman (Ed.), “Raw” data is an oxymoron. MIT Press. Stats &amp; Data Science Views. (2013, August 23). Nate Silver: What I need from statisticians [blog post]. https://www.statisticsviews.com/article/nate-silver-what-i-need-from-statisticians/ US Bureau of Labor Statistics (2019). BLS Data Viewer. https://beta.bls.gov/dataViewer/view/timeseries/LNU02070002Q Women of Color in Computing Collaborative. (2018). Data brief: Women and girls of color in computing. https://www.wocincomputing.org/wp-content/uploads/2018/08/WOCinComputingDataBrief.pdf "],["getting-started.html", "6 M2A: Getting Started with Data in R 6.1 Introduction 6.2 What are R and RStudio? 6.3 How do I code in R? 6.4 What are R packages? 6.5 Explore your first datasets 6.6 Conclusion", " 6 M2A: Getting Started with Data in R This content draws on material from Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim, licensed under CC BY-NC-SA 4.0 Changes to the source material include light editing of original material, removing original material, changing citation style, adding new material, and replacing original authors’ “we” with an “I” for the current author. The resulting content is licensed under CC BY-NC-SA 4.0. 6.1 Introduction Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? If you are already somewhat familiar with these concepts, feel free to skip to Section 6.5 where we’ll introduce our first dataset: all domestic flights departing one of the three main New York City (NYC) airports in 2013. This is a dataset we will explore in depth for much of the rest of this book. 6.2 What are R and RStudio? Throughout this book, I will expect that you are using R via RStudio. First time users often confuse the two. At its simplest, R is like a car’s engine while RStudio is like a car’s dashboard as illustrated in Figure 6.1. Figure 6.1: Analogy of difference between R and RStudio. More precisely, R is a programming language that does the actual work of computing and calculating, while RStudio is (as we covered last week) an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well. 6.2.1 Using R via RStudio Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we will use RStudio’s interface instead of interacting with R directly. When you installed R and RStudio on your computer last week, you added two new programs (also called applications) you can open. We’ll always work in RStudio and not in the R application. Figure 6.2 shows what icon you should be clicking on your computer. Figure 6.2: Icons of R versus RStudio on your computer. After you open RStudio, you should see something similar to Figure 6.3. (Note that there may be slight differences, since the interface is updated over time.) Figure 6.3: RStudio interface to R. Note the three panes (that is, the three panels dividing the screen): the console pane, the files pane, and the environment pane. Over the course of this chapter, you’ll learn what purpose each of these panes serves. 6.3 How do I code in R? Now that you’re set up with R and RStudio, you are probably asking yourself, “OK, so how do I actually use R?”. The first thing to note is that while other statistical software programs like Excel, SPSS, or Minitab provide point-and-click interfaces, R is an interpreted language. This means you have to type in commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book. It is tempting to think that point-and-click software is superior to software that requires you to write your own code. We’re used to thinking of point-and-click software as newer and more modern than writing text commands! However, writing your own code typically gives you more control and power over what your software is doing; plus, by asking you to be more involved in the statistical analysis, it actually encourages you to learn more along the way. Speaking personally, I found that I learned a lot more about statistical analysis once I started using R than I had when I was using point-and-click interfaces. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that new R users need to understand. Consequently, while this course is not a course on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 6.3.1 Basic programming concepts and terminology In my opinion and experience, completing actual projects in R does much more to help you learn the software than a written description. That is, rather than ask you to memorize terms and concepts, I’ll ask you throughout the semester to complete walkthroughs (like this one) so that you’ll “learn by doing.” One important thing to help you with any of these walkthroughs is text formatting: I will always use a different font to distinguish regular text from computer_code. That said, it can be helpful to lay out some basic definitions and describe some basic concepts; don’t worry about committing all of this to memory, but you can come back here for clarification if you aren’t sure what a term means later on. Basics: console pane: where you enter in commands. running code: the act of telling R to perform an act by giving it commands in the console. objects: where values are saved in R. We’ll show you how to assign values to objects and how to display the contents of objects. data types: integers, doubles/numerics, logicals, and characters. : integers are values like -1, 0, 2, 4092 doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8 logicals are either TRUE or FALSE characters are text such as “muesli”, “They Might Be Giants”, “Homestar Runner is the greatest thing ever”, and “this chocolate mint tea is delicious”; note that characters are often denoted with the quotation marks around them. Vectors: a series of values. These are created using the c() function, where c() stands for “combine” or “concatenate.” For example, c(6, 11, 13, 31, 90, 92) creates a six element series of positive integer values . Factors: categorical data are commonly represented in R as factors. Categorical data can also be represented as strings. We’ll study this difference as we progress through the book. Data frames: rectangular spreadsheets. They are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. We’ll cover data frames later in Section 6.5. Conditionals: You need to test for equality in R using == (and not =, which is typically used for assignment). For example, 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). For example, 4 + 2 &gt;= 3 will return TRUE, but 3 + 5 &lt;= 1 will return FALSE. Logical operators: &amp; representing “and” as well as | representing “or.” For example, (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function’s arguments or use the function’s default values. For example, the function seq() in R generates a sequence of numbers. If you just run seq() it will return the value 1. That doesn’t seem very useful! This is because the default arguments are set as seq(from = 1, to = 1). Thus, if you don’t pass in different values for from and to to change this behavior, R just assumes all you want is the number 1. You can change the argument values by updating the values after the = sign. If we try out seq(from = 2, to = 5) we get the result 2 3 4 5 that we might expect. We’ll work with functions a lot throughout this book and you’ll get lots of practice in understanding their behaviors. To further assist you in understanding when a function is mentioned in the book, I’ll also include the () after them as I did with seq() above. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, I feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. You can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more. 6.3.2 Errors, warnings, and messages One thing that intimidates new R and RStudio users is how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and will try to explain what went wrong. Generally when there’s an error, the code will not run. For example, we’ll see in Subsection 6.4.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter ?? if you create a scatterplot based on a dataset where two of the rows of data have missing entries that would be needed to create points in the scatterplot, you will see this warning: Warning: Removed 2 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren’t there. Messages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages in the upcoming Subsection 6.4.2 or when you read data saved in spreadsheet files with the read_csv() function as you’ll see in Chapter ??. These are helpful diagnostic messages and they don’t stop your code from working. Additionally, you’ll see these messages when you install packages too using install.packages() as discussed in Subsection 6.4.1. Remember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather: If the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong! If the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise, the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine and keep on going! 6.3.3 Tips on learning to code Learning to code/program is quite similar to learning a foreign language. It can be daunting and frustrating at first. Such frustrations are common and it is normal to feel discouraged as you learn. However, just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn and improve. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone is “smart,” but really people spent a lot of time and energy designing them to appear “smart.” In reality, you have to tell a computer everything it needs to do. Furthermore, the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way. Take the “copy, paste, and tweak” approach: Especially when you learn your first programming language or you need to understand particularly complicated code, it is often much easier to take existing code that you know works and modify it to suit your ends. This is as opposed to trying to type out the code from scratch. I call this the “copy, paste, and tweak” approach. So early on, I suggest not trying to write code from memory, but rather take existing examples from the book, then copy, paste, and tweak them to suit your goals. In fact, this will be an explicit part of many activities! After you start feeling more confident, you can slowly move away from this approach and write code from scratch. Think of the “copy, paste, and tweak” approach as training wheels for a child learning to ride a bike. After getting comfortable, they won’t need them anymore. The best way to learn to code is by doing: Rather than learning to code for its own sake, I find that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in and that is important to you. Practice is key: Just as the only method to improve your foreign language skills is through lots of practice and speaking, the only method to improving your coding skills is through lots of practice. Don’t worry, however, you’ll have plenty of opportunities to do so! 6.4 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization in Chapter ??, the dplyr package for data wrangling in Chapter ??, the moderndive package that accompanies this book these walkthroughs are taken from, and the infer package for “tidy” and transparent statistical inference in Chapters ??, ??, and ??. A helpful analogy for R packages is they are like apps you can download onto a mobile phone: Figure 6.4: Analogy of R versus R packages. In this analogy, R is like a new smartphone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play. Let’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set for the time being. You might need to do this again in the future when there is an update to the app. Open the app: Installing Instagram is great, but just because you’ve installed it doesn’t mean that you can use it. To use it, you need to open it; in fact, every time you want to use it, you need to open it. Once Instagram is on your phone and opened, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version. “Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio. Let’s perform these two steps for the ggplot2 package for data visualization. 6.4.1 Package installation There are two ways to install an R package: an easy way and a more advanced way. Let’s install the ggplot2 package the easy way first as shown in Figure 6.5. In the Files pane of RStudio: Click on the “Packages” tab. Click on “Install” next to Update. Type the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2. Click “Install.” Figure 6.5: Installing packages in R the easy way. An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package. Let’s recap our analogy from earlier, because it’s important: Much like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps. Now, on your own, repeat the earlier installation steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package for data wrangling, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for generating easy-to-read tables in R. We’ll use these packages in the next section. Note that your output might be slightly different than the output displayed throughout the book, because packages get updated over time. This typically won’t be cause for concern, but if you’re worried about something, you can reach out to me and I can take a look. 6.4.2 Package loading Recall that after you’ve installed a package, you need to “load it.” In other words, you need to “open it.” We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the console pane in RStudio. What do we mean by “run the following code”? Either type or copy-and-paste the following code into the console pane and then hit the Enter key. library(ggplot2) If after running the earlier code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If, however, you get a red “error message” that reads… Error in library(ggplot2) : there is no package called ‘ggplot2’ … it means that you didn’t successfully install it. This is an example of an “error message” we discussed in Subsection 6.3.2. If you get this error message, go back to Subsection 6.4.1 on R package installation and make sure to install the ggplot2 package before proceeding. Now, “load” the dplyr, nycflights13, and knitr packages as well by repeating the earlier steps. 6.4.3 Package use One very common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to: Error: could not find function This is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been “loaded.” R doesn’t know where to find the function you are using. Almost all new users forget to do this when starting out, and it is a little annoying to get used to doing it. However, you’ll remember with practice and after some time it will become second nature for you. 6.5 Explore your first datasets Let’s put everything we’ve learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we’ll focus on datasets that are saved in “spreadsheet”-type format. This is probably the most common way data are collected and saved in many fields. Remember from Subsection 6.3.1 that these “spreadsheet”-type datasets are called data frames in R. We’ll focus on working with data saved as data frames throughout this book. Let’s first load all the packages needed for this chapter, assuming you’ve already installed them. Read Section 6.4 for information on how to install and load R packages if you haven’t already. library(nycflights13) library(dplyr) library(knitr) At the beginning of all subsequent chapters in this book, we’ll always have a list of packages that you should have installed and loaded in order to work with that chapter’s R code. 6.5.1 nycflights13 package Many of us have flown on airplanes or know someone who has. Air travel has become an ever-present aspect of many people’s lives. If you look at the Departures flight information board at an airport, you will frequently see that some flights are delayed for a variety of reasons. Are there ways that we can understand the reasons that cause flight delays? We’d all like to arrive at our destinations on time whenever possible. (Unless you secretly love hanging out at airports. If you are one of these people, pretend for a moment that you are very much anticipating being at your final destination.) Throughout this book, we’re going to analyze data related to all domestic flights departing from one of New York City’s three main airports in 2013: Newark Liberty International (EWR), John F. Kennedy International (JFK), and LaGuardia Airport (LGA). We’ll access this data using the nycflights13 R package, which contains five datasets saved in five data frames: flights: Information on all 336,776 flights. airlines: A table matching airline names and their two-letter International Air Transport Association (IATA) airline codes (also known as carrier codes) for 16 airline companies. For example, “DL” is the two-letter code for Delta. planes: Information about each of the 3,322 physical aircraft used. weather: Hourly meteorological data for each of the three NYC airports. This data frame has 26,115 rows, roughly corresponding to the \\(365 \\times 24 \\times 3 = 26,280\\) possible hourly measurements one can observe at three locations over the course of a year. airports: Names, codes, and locations of the 1,458 domestic destinations. 6.5.2 flights data frame We’ll begin by exploring the flights data frame and get an idea of its structure. Run the following code in your console, either by typing it or by copying-and-pasting it. Make sure that you only type (or copy-and-paste) the top of the two boxes below this paragraph. When walkthroughs in this book provide you with example code, there will typically be two boxes: The first will provide the code, and the second will provide the output from that code (preceded with ## before each line to show that it’s output, not code). In this case, that’s the contents of the flights data frame, which will also show in your console once you’ve run the code yourself. However, note that depending on the size of your monitor, the output may vary slightly. flights # A tibble: 336,776 × 22 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 2013 1 1 517 515 2 830 819 2 2013 1 1 533 529 4 850 830 3 2013 1 1 542 540 2 923 850 4 2013 1 1 544 545 -1 1004 1022 5 2013 1 1 554 600 -6 812 837 6 2013 1 1 554 558 -4 740 728 7 2013 1 1 555 600 -5 913 854 8 2013 1 1 557 600 -3 709 723 9 2013 1 1 557 600 -3 838 846 10 2013 1 1 558 600 -2 753 745 # ℹ 336,766 more rows # ℹ 14 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, gain &lt;dbl&gt;, hours &lt;dbl&gt;, # gain_per_hour &lt;dbl&gt; Let’s unpack this output: A tibble: 336,776 x 22: A tibble is a specific kind of data frame in R. This particular data frame has 336,776 rows corresponding to different observations. Here, each observation is a flight. 22 columns corresponding to 22 variables describing each observation. year, month, day, dep_time, sched_dep_time, dep_delay, and arr_time are the different columns; that is, they’re the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 flights. R is only showing the first 10 rows, because if it showed all 336,776 rows, it would overwhelm your screen. 336,766 more rows indicating to us that there are 336,766 more rows of data 14 more variables: and then a list of the 14 that could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well, but it does give a nice preview. Let’s look at some different ways to explore data frames. 6.5.3 Exploring data frames There are many ways to get a feel for the data contained in a data frame such as flights. Here are three functions that take as their “argument” (their input) the data frame in question. I’ve also included a fourth method for exploring one particular column of a data frame: using the View() function, which brings up RStudio’s built-in data viewer using the glimpse() function, which is included in the dplyr package using the kable() function, which is included in the knitr package using the $ “extraction operator,” which is used to view a single variable/column in a data frame 1. View(): Run View(flights) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(flights) instead of View(flights). By running View(flights), we can explore the different variables listed in the columns. Observe that there are many different types of variables. Some of the variables like distance, day, and arr_delay are what we will call quantitative variables. These variables are numerical in nature. Other variables here are categorical. Note that if you look in the leftmost column of the View(flights) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row is representing. This will allow you to identify what object is being described in a given row by taking note of the values of the columns in that specific row. This is often called the observational unit. The observational unit in this example is an individual flight departing from New York City in 2013. You can identify the observational unit by determining what “thing” is being measured or described by each of the variables. We’ll talk more about observational units in Subsection 6.5.4 on identification and measurement variables. 2. glimpse(): The second way we’ll cover to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package by running library(dplyr). This function provides us with an alternative perspective for exploring a data frame than the View() function: glimpse(flights) Rows: 336,776 Columns: 22 $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2… $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, … $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, … $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1… $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,… $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,… $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1… $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;, &quot;… $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4… $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N394… $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LGA&quot;,… $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IAD&quot;,… $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1… $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, … $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6… $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0… $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0… $ gain &lt;dbl&gt; -9, -16, -31, 17, 19, -16, -24, 11, 5, -10, 0, 1, -9, 1… $ hours &lt;dbl&gt; 3.783, 3.783, 2.667, 3.050, 1.933, 2.500, 2.633, 0.883,… $ gain_per_hour &lt;dbl&gt; -2.38, -4.23, -11.62, 5.57, 9.83, -6.40, -9.11, 12.45, … Observe that glimpse() will give you the first few entries of each variable in a row after the variable name. In addition, the data type (see Subsection 6.3.1) of the variable is given immediately after each variable’s name inside &lt; &gt;. Here, int and dbl refer to “integer” and “double”, which are computer coding terminology for quantitative/numerical variables. “Doubles” take up twice the size to store on a computer compared to integers. In contrast, chr refers to “character”, which is computer terminology for text data. In most forms, text data, such as the carrier or origin of a flight, are categorical variables. The time_hour variable is another data type: dttm. These types of variables represent date and time combinations. However, we won’t work with dates and times this semester; you can always read more about this in other data science books like Introduction to Data Science by Tiffany-Anne Timbers, Melissa Lee, and Trevor Campbell or R for Data Science by Hadley Wickham and Garrett Grolemund. 3. kable(): The final way to explore the entirety of a data frame is using the kable() function from the knitr package. Let’s explore the different carrier codes for all the airlines in our dataset two ways. Run both of these lines of code in the console: airlines kable(airlines) At first glance, it may not appear that there is much difference in the outputs. However, when using tools for producing reproducible reports such as R Markdown (as we will be doing for our projects in this class), the latter code produces output that is much more legible and reader-friendly. You’ll see us use this reader-friendly style in many places in the book when we want to print a data frame as a nice table. 4. $ operator Lastly, the $ operator allows us to extract and then explore a single variable within a data frame. For example, run the following in your console airlines$name We used the $ operator to extract only the name variable and return it as a vector of length 16. We’ll only be occasionally exploring data frames using the $ operator, instead favoring the View() and glimpse() functions. 6.5.4 Identification and measurement variables There is a subtle difference between the kinds of variables that you will encounter in data frames. There are identification variables and measurement variables. For example, let’s explore the airports data frame by showing the output of glimpse(airports): glimpse(airports) Rows: 1,458 Columns: 8 $ faa &lt;chr&gt; &quot;04G&quot;, &quot;06A&quot;, &quot;06C&quot;, &quot;06N&quot;, &quot;09J&quot;, &quot;0A9&quot;, &quot;0G6&quot;, &quot;0G7&quot;, &quot;0P2&quot;, &quot;… $ name &lt;chr&gt; &quot;Lansdowne Airport&quot;, &quot;Moton Field Municipal Airport&quot;, &quot;Schaumbur… $ lat &lt;dbl&gt; 41.1, 32.5, 42.0, 41.4, 31.1, 36.4, 41.5, 42.9, 39.8, 48.1, 39.6… $ lon &lt;dbl&gt; -80.6, -85.7, -88.1, -74.4, -81.4, -82.2, -84.5, -76.8, -76.6, -… $ alt &lt;dbl&gt; 1044, 264, 801, 523, 11, 1593, 730, 492, 1000, 108, 409, 875, 10… $ tz &lt;dbl&gt; -5, -6, -6, -5, -5, -5, -5, -5, -5, -8, -5, -6, -5, -5, -5, -5, … $ dst &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;U&quot;, &quot;A&quot;, &quot;A&quot;, &quot;U&quot;, &quot;A&quot;,… $ tzone &lt;chr&gt; &quot;America/New_York&quot;, &quot;America/Chicago&quot;, &quot;America/Chicago&quot;, &quot;Ameri… The variables faa and name are what we will call identification variables, variables that uniquely identify each observational unit. In this case, the identification variables uniquely identify airports. Such variables are mainly used in practice to uniquely identify each row in a data frame. faa gives the unique code provided by the FAA for that airport, while the name variable gives the longer official name of the airport. The remaining variables (lat, lon, alt, tz, dst, tzone) are often called measurement or characteristic variables: variables that describe properties of each observational unit. For example, lat and long describe the latitude and longitude of each airport. Furthermore, sometimes a single variable might not be enough to uniquely identify each observational unit: combinations of variables might be needed. While it is not an absolute rule, for organizational purposes it is considered good practice to have your identification variables in the leftmost columns of your data frame. ### Help files Another nice feature of R are help files, which provide documentation for various functions and datasets. You can bring up help files by adding a ? before the name of a function or data frame and then run this in the console. You will then be presented with a page showing the corresponding documentation if it exists. For example, let’s look at the help file for the flights data frame. ?flights The help file should pop up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away. 6.6 Conclusion Does this chapter contain everything you need to know? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn’t be useful! Instead, we’ve covered a minimally viable set of tools to explore data in R. As I said earlier, the best way to add to your toolbox is to get into RStudio and run and write code as much as possible. If you are new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, you can check out the short book, Getting Used to R, RStudio, and R Markdown, by Chester Ismay and Patrick C. Kennedy. It includes screencast recordings that you can follow along and pause as you learn. This book also contains an introduction to R Markdown, a tool used for reproducible research in R. Figure 6.6: Preview of Getting Used to R, RStudio, and R Markdown. "],["m2c-set-up-github.html", "7 M2C: Set up GitHub 7.1 Introduction 7.2 Git—and GitHub—in Data Science 7.3 GitHub in ICT/LIS 661 7.4 Setting Up GitHub 7.5 Moving Forward", " 7 M2C: Set up GitHub This chapter draws on material from: Happy Git and GitHub for the useR by Jenny Bryan, licensed under CC BY-NC 4.0 Changes to the source material include addition of new material; light editing; rearranging and removing original material; adding and changing links; and adding first-person language from current author. The resulting content is licensed under CC BY-NC 4.0 7.1 Introduction Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files—called a repository—in a sane, highly structured way. Think of it as the more powerful cousin of the “Track Changes” features from Microsoft Word. 7.2 Git—and GitHub—in Data Science Git has been re-purposed by the data science community to manage the motley collection of files that make up typical data analytical projects, which often consist of data, figures, reports, and source code. A solo data scientist, working on a single computer, will benefit from adopting version control—but not nearly enough to justify the pain of installation and workflow upheaval. There are much easier ways to get versioned back ups of your files, if that’s all you’re worried about. This is where hosting services like GitHub, Bitbucket, and GitLab come in. They provide a home for your Git-based projects on the internet. Think of this as Dropbox or Google Drive for code. The service acts as a distribution channel or clearinghouse for your Git-managed project. It allows other people to see your stuff, sync up with you, and perhaps even make changes. These hosting providers improve upon traditional Unix Git servers with well-designed web-based interfaces. There are clear advantages to using one of these services: First, if someone needs to see your work or if you want them to try out your code, they can easily get it from GitHub. If they use Git, they can clone or fork your repository. If they don’t use Git, they can still browse your project on GitHub like a normal website and even grab everything by downloading a zip archive. Second, if you care deeply about someone else’s project, such as an R package you use heavily, you can track its development on GitHub. You can watch the repository to get notified of major activity. You can fork it to keep your own copy. You can modify your fork to add features or fix bugs and send them back to the owner as a proposed change. 7.3 GitHub in ICT/LIS 661 In this class, you will be accessing and submitting your projects for this class through GitHub. If you need feedback or help with your code, we’ll also use GitHub to send it back and forth. We won’t be using GitHub to its fullest extent, but this will at least give you some basic familiarity with the software. We are using GitHub—and not Bitbucket or GitLab—because GitHub is widely used in the R community, and because it’s what I’m most familiar with. However, it’s worth pointing out that the GitHub company has a history of controversy and that it has recently come under increased criticism from the same open source community that GitHub claims to serve. For the time being, I am continuing to use GitHub (personally and for this class), but as we’ll discuss throughout the semester, it’s important to be attentive to ethics and values in data science, and I don’t think this is any exception. 7.4 Setting Up GitHub The remainder of this walkthrough is focused on helping you get GitHub set up for this class. 7.4.1 Creating a GitHub Account First things first, head on over to github.com and create an account. If you aren’t already using GitHub and are not sure you’ll be using GitHub in the future, I recommend creating an account with your @uky.edu address, but you’re welcome to use a GitHub account you already have or to use a personal address if you think you’ll want to use this service more in the future. Here are some tips for picking a GitHub username: Incorporate your actual name! People like to know who they’re dealing with. Also makes your username easier for people to guess or remember. Reuse your username from other contexts, e.g., Twitter or Slack. But, of course, someone with no GitHub activity will probably be squatting on that. Pick a username you will be comfortable revealing to your future boss. Shorter is better than longer. Be as unique as possible in as few characters as possible. In some settings GitHub auto-completes or suggests usernames. Make it timeless. Don’t highlight your current university, employer, or place of residence. Avoid the use of upper vs. lower case to separate words. I highly recommend all lowercase. Usernames aren’t case sensitive in GitHub, but using all lowercase is kinder to people who may be working with GitHub data. A better strategy for word separation is to use a hyphen (-) or underscore (_). 7.4.2 Setting up a GitHub Repository for Class In this next section, I’ve tried to be as specific as possible in providing instructions. However, past experience has shown me that what I see on my computer while walking through these steps isn’t always exactly what other people see due to operating system or other differences. If something doesn’t match perfectly what you see, use common sense to figure out the next best option. Once you’ve created a GitHub account, please navigate to this GitHub repository, which contains some crucial files for our class. If you have your browser window open wide enough, you should see a green Use this template button to the right of the menu above the list of files and directories (or folders) Click that button and, if prompted, click on Create a new repository. The next interface has a few different sections in it. You can ignore the first few options and then go down to where it says Owner: Make sure it lists your username there! If/as you continue to work with RStudio and GitHub, you’ll get a feel for what kind of directory names are most helpful to you, but I’m mandating a consistent format for selfish reasons: So that I can easily keep track of dozens of different respositories this semester. Give this project a name that follows this format: yourlastname_yourfirstname_661 That is, my project would be named Greenhalgh_Spencer_661. You can skip the Description line, but make sure to set your repository to Private. This is important so that your homework for this class isn’t publicly accessible! Once you’ve finished all the steps on this interface, GitHub will take you to the page for your repository. Find the Settings button near the top right of the page (if you can’t see it, you may need to make your browser window larger), and click on that. Once you’re in settings, navigate to Collaborators and then Add people. Then, enter my GitHub username, greenhas. Having access to your repository will make helping you with your code much easier in the future! 7.4.3 Installing GitHub Desktop Once you’ve done all this, navigate to desktop.github.com and download the GitHub Desktop client. Open it up and make sure to associate it with your GitHub account. There are plenty of other ways to use Git and GitHub, including the command line and RStudio itself, and if you’re serious about data science, I recommend learning more about both of those options (the Happy Git and GitHub for the useR website is helpful and thorough). You’re even free to try them this semester, but remember that my instructions are always going to assume you’re using this client. That’s because I find GitHub Desktop to be the most user-friendly approach to GitHub, and since our goals for this class are just a basic introduction to the service, it’s going to do the trick. Once you’ve signed in to GitHub Desktop, navigate to File and then Clone Repository. This will bring up a list of repositories that you have on GitHub for you to choose from; in this context, “clone” simply means to create a local copy of the repository on your computer. Pay attention to the Local Path option. By default, GitHub Desktop will create the local copy of your files within: a folder that has the same name as the repository, which is placed within: either a Documents folder or the main user folder of your computer You’re free to change this, but I strongly recommend sticking with the default folder name, and unless you have strong opinions about where to keep your files, it’s probably a good idea to stick with the default filepath, too. Wherever you end up storing your repository folder, pay close attention—you’ll be navigating back to it to complete your work throughout the semester. 7.5 Moving Forward As things currently stand, your repository isn’t really doing a whole lot, but that’s okay. We’ll return to GitHub next week as we learn some more about file and project management in RStudio, and practice good GitHub habits throughout the semester. "],["m3u-research-paradigms-and-reproducibility.html", "8 M3U: Research Paradigms and Reproducibility 8.1 Introduction 8.2 Reproducibility and Paradigms 8.3 Supporting Reproducibility 8.4 Conclusion 8.5 References", " 8 M3U: Research Paradigms and Reproducibility This chapter draws on material from: Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim, licensed under CC BY-NC-SA 4.0 Open Education Science by Tim van der Zee and Justin Reich, licensed under CC BY-NC 4.0 Changes to the source material include removal of original material, the addition of new material, combining of sources, editing of original material for a different audience, and addition of first-person language from current author.. The resulting content is licensed under CC BY-NC-SA 4.0. 8.1 Introduction One goal of this reading is helping readers understand the importance of reproducible analyses. The hope is to get readers into the habit of making their data science analyses reproducible from the very beginning. Reproducibility means a lot of things in terms of different scientific fields, but they are all concerned with a common goal: Is research conducted in a way that another researcher could follow the steps and get similar results? The importance placed on reproducibility comes in part from a growing belief that a substantial portion of published research findings may actually report false positives—or, at the very least, overestimate the importance of a study’s findings (Simmons et al., 2011). Ioannidis (2005) went so far as to title an article “Why Most Published Research Findings Are False,” describing several problems in medical research that lead to false positive rates. If you can reproduce someone else’s research and get the same results, that suggests that the findings of the original study are on solid ground. However, if you try to follow their steps and get different results, there may be reason to question what the original researchers had to say. This is pretty straight forward, but reproducing someone else’s work isn’t as easy as it might seem. Most researchers in training eventually realize that short summaries of research methods in journal articles tidy over much of the complexity and messiness in research—especially research that involves humans. These summaries spare readers from trivial details, but they can also misrepresent important elements of the research process. However, if our goal is to provide practitioners, policymakers, and other researchers with data, theory, and explanations that lead to practical improvements, we need to provide them with a way to judge the quality and contextual relevance of research. That judgment depends in tern on how researchers choose to share the methods and processes behind their work. In data science, there is a particular emphasis on what is known as computational reproducibility. This refers to being able to pass all of one’s data analysis, datasets, and conclusions to someone else and have them get exactly the same results on their machine. This has obvious benefits for scientific concerns about reproducibility! In fact, one of my friends in graduate school had a job doing reproducibility for a political science journal. Anyone submitting research to be published in the journal also had to submit all of their data analysis, datasets, and conclusions. The paper to be published would be submitted to traditional peer review, but my friend would also review all of the submitted materials to make sure that there weren’t any discrepancies or errors in the data, analysis, and results described in the paper. However, there are also practical and personal benefits to computational reproducibility. In a traditional scientific workflow, you might do your statistical work and data visualization in RStudio and then write up your results in a word processor, pasting in helpful plots that you’d generated in RStudio. This isn’t a terrible demand on your time, but what if you discover a mistake partway through? Or what if someone gives you good advice on a late draft of your paper that leads you to make some changes to your data analysis? In a traditional workflow, you’d need to step through the entire process again: redo the analyses, redo the data visualization, rewrite the paper, repaste the plots. This is error prone and a frustrating use of time. In contrast, with the right software—and, more importantly, the right mindset—there are ways to include analysis and writing in the same document. Not only does this allow a colleague to easily reproduce all of the steps you’ve taken, but it might actually save you from having to redo work! This allows for data scientists to spend more time interpreting results and considering assumptions instead of spending time restarting an analysis from scratch or following a list of steps that may be different from machine to machine. Really committing to reproducibility means building new habits; it takes practice and will be difficult at times. Hopefully, though, you’ll see just why it is so important for you to keep track of your code and document it well to help yourself later and any potential collaborators as well. 8.2 Reproducibility and Paradigms I should make it clear from the beginning that I am an imperfect example of reproducibility. Part of this is because this is a difficult process—I’m still developing necessary habits and trying to get rid of bad habits. However, I also have philosophical differences with reproducibility as I’ve described it here. This isn’t to say that I disagree with the importance of reproducibility itself—I think it’s a fine idea, and I’m even going to enforce a certain level of it in this class! However, the reasons that reproducibility is important are often tied to basic assumptions that data scientists typically make about how the world works. While I draw heavily from data science techniques in my research, I also draw from research traditions that do not share the same assumptions as data science. 8.2.1 An Example Paradigm To explain more what I mean by this, let’s think about some of the assumptions that are implicitly present in my description of reproducibility in the previous section: Everyone ought to get the same results every time: As described above, reproducibility is related to a belief that different scientists should get similar results if individually performing the same analysis. If two scientists don’t come to the same answer when asking similar questions, that’s seen as a problem. People who work separately ought to come to the same conclusions. Research should be a series of nice, neat decisions: The basic idea of reproducibility is that it should be possible to provide a clear checklist that describes the steps that scientists took when carrying out an analysis. This only works if individual decisions can be clearly distinguished from each other and then communicated to another in a methodical way. Research is meant to inform practitioners and policymakers: Part of the reasoning behind making a research study reproducible is giving people confidence in the study’s findings; part of the reasoning behind giving people confidence in the study’s findings is so that other people can act on those findings. This perspective sees research as useful when it translates into specific actions that other stakeholders can take. Each of these three assumptions about research can be understood as connecting to a deeper assumption about how the world works and the resulting role of research: The world—including things and people in it—work in consistent, predictable ways; the point of research (including data science) is to figure out how the world works so that we can get the outcomes we want. In research settings, these collections of assumptions (and underlying worldview) are often referred to as a paradigm. These aren’t terrible assumptions, and this isn’t a bad paradigm! They’ve driven most of scientific progress and a lot of effective decision-making over the past couple of centuries. However, they’re not the only way of thinking about the world. Let me provide a recent example from my data science-adjacent research that rejects these assumptions in favor of a different set of assumptions. 8.2.2 Another Example Paradigm In mid-2019, I used R (and other software) to collect screenshots of about 1,400 tweets associated with a specific Twitter hashtag. The organizers of the hashtag claim that people who use the hashtag are simply members of a particular church and that their goal is to defend that church against its critics. However, outside observers have argued that participants in the hashtag are heavily influenced by toxic, far-right online communities and believe that these tweets are more motivated by white nationalism than religious beliefs. Over the course of a couple of years, I worked with a colleague to carefully study each of these tweets so that we could come to some conclusions about what’s happening in this hashtag. Here are some of the assumptions that we had about our research: Different people might come to different conclusions: My co-researcher and I understood while studying these tweets that we might understand them differently. I grew up in the church that this hashtag is associated with, so I pick up on a lot of subtle references that my colleague doesn’t. However, as a woman, she picks up on subtle misogyny that sometimes sails right over my (masculine) head. We have different lived experiences, and we know that if we analyzed this data separately, we would probably come to different conclusions—that’s exactly why we’re working together. Research involves complex, messy decisions: What’s the point at which two researchers become confident that a 280-character post on Twitter includes a reference to white nationalism? The group that my colleague and I are studying is skilled at ambiguity and subtle references, so it’s not often the case that they clearly embrace extreme political views. It would be a mistake to ignore these views just because they’re not always easy to pin down, but it would also be irresponsible to exaggerate our conclusions. There’s no clear checklist for determining whether a tweet includes references to white nationalism—instead, we have to engage in lengthy discussions and make difficult calls. The goal of research is to develop understanding: I hope that our research makes some kind of difference in the world, and I know that my colleague does too. However, after reviewing our data and our conclusions, it’s not as if we have a list of specific actions that someone can take in response to our research. Who is that “someone” anyway? Twitter users? The church that this group claims to be loyal to? The group itself? Any of those populations could hypothetically learn something from what we have to say, but it would be a stretch to say that there’s a clear and immediate takeaway from our research. Our goal is to simply to document our findings in the hopes of increasing readers’ understanding of this kind of phenomenon. Each of our three assumptions about researchers can be understood as connecting to a paradigm we share: The world—including things and people in it—are rich, complex, and difficult to perfectly summarize; the point of research is to try to capture some of that richness so that we can better appreciate details and context. Some scientists are hostile to the idea of any paradigm except the first one and would be dismissive of the kind of research that I’ve just described. However, I’m confident that many data scientists acknowledge the existence and value of different research paradigms. Their response might be that the first paradigm works particularly well for data science, but that there’s no harm in using the second paradigm in other kinds of research. In fact, they might suggest, even if I did use R for that research project, it doesn’t really count as data science (to be honest, I’d agree with this evaluation, but remember what we’ve already read about what gets to count as data science!). 8.2.3 Why Do Paradigms Matter? In that case, what’s the point of making a big deal about paradigms here? If reproducibility is based on a paradigm about the predictability of the world, and if data science embraces that paradigm, why emphasize the existence of other paradigms? That’s a good question, but there’s an answer that’s just as good. In short, while it’s very useful to assume that the world is neatly ordered and predictable (remember all of those scientific and technological developments over the past couple of centuries?), it’s also possible to overemphasize that assumption—and overemphasis carries with it important ethical consequences. Let’s consider a concrete example related to data scientist. In recent years, there’s been considerable interest in sentencing algorithms. In other words, human judges are fallible and biased and might hand out different sentences for the same crime based not on an objective interpretation of the law but on their subjective responses to the accused. Wouldn’t it be great if we could come up with a computer program that would take in the facts of a case and information about the criminal and spit out an appropriate, fair sentence that a potentially biased judge couldn’t interfere with? The influence of the first paradigm ought to be clear here: Decisions ought to be consistent, so let’s boil down sentencing to a neat checklist-like algorithm that uses data to make a recommendation and make a practical difference. In theory, this sounds great—an application of data science that could make a real difference in the world. However, there are plenty of critics of this approach (including me). No one disputes that human judges are fallible and biased, and it would be fantastic to come up with a way of sentencing criminals that wouldn’t be as influenced by personal prejudices. The issue isn’t with the intent, it’s with the underlying paradigm. Critics of sentencing algorithms argue that in valuing consistency, predictable algorithms, and clear-cut recommendations, they overlook ways in which the world doesn’t actually work like that! For example, a sentencing algorithm would likely be trained on sentences given by human judges, and use those as the basis for the supposedly objective decision. However, as we’ve already established, human judges are fallible and biased; for example, we might (and almost certainly would!) find that judges give harsher sentences to criminals of color than white criminals. A sentencing algorithm that used race as a factor in determining sentences would therefore treat “non-white” as a statistically-valid reason to recommend a harsher sentence, simply because that’s the pattern that it was trained on. Rather than replace human bias, the algorithm would learn from it—but because humans tend to believe computers are more objective than humans, that bias would be harder to argue against! In a sense, this is a worse outcome, because it inherits bias but disguises it as an objective measure. This criticism implicitly adopts the second paradigm that I’ve described in this sentence: The process of sentencing a criminal is too rich and complex to distill down to a computational decision. More importantly, it points out a weakness of the first paradigm: A belief that the world is consistent and predictable is actually sometimes just a preference for things that seem consistent and predictable. That is, data scientists sometimes fall into the trap of believing that a consistent and predictable solution to a problem is better, when a solution that acknowledges complexity and nuance might feel less efficient but would do better at avoiding harm. 8.3 Supporting Reproducibility If I am sometimes resistant to calls for reproducibility, it is because I think the paradigm that these calls assume doesn’t always hold up. However, there’s nothing half-hearted about my teaching you about reproducibility in this class—it’s impossible to deny that there are amazing benefits to making your data science work reproducible (so long as data scientists critically examine their own paradigms). Let’s consider an example that I like a lot. Josh Rosenberg is one of my data science heroes. We went to grad school together, and 90% of what I know about R is thanks to him; if I have questions about R or data science, Josh is the first person I go to. When we were still in grad school, Josh participated in a study in which: independent analysts used the same dataset to test two hypotheses regarding the effects of scientists’ gender and professional status on verbosity during group meetings (Schweinsberg et al., 2021, p. 230) In other words, Josh and dozens of other researchers (the list of contributors to this study is literally a page long) were all given the same data and encouraged to test the same hypotheses. However, apart from the data and the hypotheses, the organizers of the study were intentionally skimpy on details. For example, it was up to individual researchers to determine how to best measure “professional status” using the available data, what statistical tests to use, etc. As a result, Researchers reported radically different analyses and dispersed empirical outcomes, in a number of cases obtaining significant effects in opposite directions for the same research question. (Schweinsberg et al., 2021, p. 230) In short, even though they were all given the same prompts and the same data, researchers made very different analytical decisions and came to very different results, sometimes producing conflicting (but individually compelling) findings. The article reports that Subjective researcher decisions play a critical role in driving the reported empirical results, underscoring the need for open data, systematic robustness checks, and transparency regarding both analytic paths taken and not taken. (Schweinsberg et al., 2021, p. 230) That is, scientists make lots of decisions when analyzing data. Even if they’re using the same data and asking the same questions, different, equally-qualified scientists might make wildly different decisions. In theory, any research article is supposed to contain a detailed section of how any analysis was completed, but as we’ve already determined, word limits and researchers’ attention spans are too short for articles to cover every decision that could potentially make a difference. This is why reproducibility is important. If you can carefully document every decision you made, every line of code you wrote, and every other aspect of your analysis, other people will know not just what data you used and what questions you asked, but how—in very specific terms—you asked those questions. This is an undeniably good thing. So, how do we support reproducibility in practical terms? Here are a few—with a focus on how they play out in traditional academic research: 8.3.1 Open Design Research design is essential to any study as it dictates the scope and use of the study. This phase includes formulating the key research question(s), designing methods to address these questions, and making decisions about practical and technical aspects of the study. Typically, this entire phase is the private affair of the involved researchers. In many studies, the hypotheses are obscured or even unspecified until the authors are preparing an article for publication. Readers often cannot determine how hypotheses and other aspects of the research design have changed over the course of a study since usually only the final version of a study design is published. Moreover, there is compelling evidence that much of what does get published is misleading or incomplete in important ways. A meta-analysis (that is, research on other research) found that 33% of authors admitted to questionable research practices, such as “dropping data points based on a gut feeling,” “concealment of relevant findings,” and/or “withholding details of methodology” (Fanelli, 2009). Given that these numbers are based on self-reports and thus suspect to social desirability bias, it is plausible that these numbers are underestimates. For Open Design, researchers make every reasonable effort to give readers access to a truthful account of the design of a study and how that design changed over the duration of the study. Since study designs can be complex, this often means publishing different elements of a study design in different places. For instance, many academic journals publish short methodological summaries in the full text of an article and allow more detailed supplementary materials of unlimited length online. In addition, analytic code might be published in a linked GitHub account, and data might be published in an online repository. These various approaches allow for more detail about methods to be published, with convenient summaries for general readers and more complete specifics for specialists and those interested in replication and reproduction. There are also a variety of approaches for increasing transparency by publishing a time-stamped record of methodological decisions before publication: a strategy known as preregistration. Preregistration is the practice of documenting and sharing the hypotheses, methodology, analytic plans, and other relevant aspects of a study before it is conducted (Gehlbach &amp; Robinson, 2018; Nosek et al., 2015). 8.3.2 Open Analysis Open Analysis is the systematic reproduction of analytic methods conducted by other researchers—it’s the main support for reproducibility that has come up in this reading. Reproducing research is central to scientific progress because any individual study is generally insufficient to make robust or generalizable claims—the kinds that others could clearly act on. It is only after ideas are tested and replicated in various conditions and contexts and results “meta-analyzed” across studies that more durable scientific principles and precepts can be established. One form of replication is a reproduction study, where researchers attempt to faithfully reproduce the results of a study using the same data and analyses. Such studies are dependent on Open Design, so that replication researchers can use the same methodological techniques but also the same exclusion criteria, coding schemes, and other analytic steps that allow for faithful replication. In recent years, perhaps the most famous reproduction study was by Thomas Herndon, a graduate student at UMass Amherst who discovered that two Harvard economists, Carmen Reinhart and Kenneth Rogoff, had failed to include five columns in an averaging operation in an Excel spreadsheet (The Data Team, 2016). After averaging across the full data set, the claims in the study had a much weaker empirical basis. Ouch! In data science, where statistical code is central to conducting analyses, the sharing of that code is one way to make analytic methods more transparent. GitHub and similar repositories allow researchers to store code, track revisions, and share with others—GitHub’s importance for reproducibility is a major reason that we’re using it in this course. At a minimum, these repositories allow researchers to publicly post analytic code in a transferable, machine-readable platform. Used more fully, GitHub repositories can allow researchers to share preregistered codebases that present a proposed implementation of hypotheses, final code as used in publication, and all of the changes in between. Simply making code “available on request” will not be as powerful as creating additional mechanisms that encourage researchers to proactively share their analytic code: as a requirement for journal or conference submissions, as an option within study preregistrations, or in other venues. Reinhart and Rogoff’s politically consequential error might have been discovered much sooner if their analyses had been made available along with publication rather than after the idiosyncratic query of an individual researcher. 8.3.3 Open Publication Open Access (OA) literature is digital, online, available to read free of charge, and free of most copyright and licensing restrictions (Suber, 2004). Most for-profit publishers obtain all the rights to a scholarly work and give back limited rights to the authors. With Open Access, the authors retain copyright for their article and the right to allow anyone to download and reprint provided that the authors and source are cited, for example under a Creative Commons Attribution License (CC BY 4.0). Opening access increases the ability of researchers, policymakers, and practitioners to leverage scientific findings for the public good. Sharing a publicly accessible preprint can also be used to receive comments and feedback from fellow scientists, a form of informal peer review. Preprints are publicly shared manuscripts that have not (yet) been peer reviewed. A variety of peer-reviewed journals acknowledge the benefits of preprints. Across the physical and computer sciences, repositories such as ArXiv have dramatically changed publication practices and instituted a new form of public peer review across blogs and social media. In the social sciences, the Social Science Research Network and SocArXiv offer additional repositories for preprints and white papers. Preprints enable more iterative feedback from the scientific community and provide public venues for work that address timely issues or otherwise would not benefit from formal peer review. Whereas historically peer review has been considered a major advantage over these forms of nonreviewed publishing, the limited amount of available evidence suggests that the typically closed peer-review process has no to limited benefits (Bruce et al., 2016; Jefferson et al., 2002). Public scholarly scrutiny may prove to be an excellent complement, or perhaps even an alternative, to formal peer review. 8.4 Conclusion Data science is a field that values reproducibility, and we’re going to practice being reproducible as part of this course. It’s important to recognize that the value of reproducibility is based off of certain assumptions about what good research is and how the world works; those assumptions do not always hold up, and they can even be dangerous if we’re not careful about them. However, that doesn’t change the fact that there are real, powerful benefits to reproducibility. 8.5 References Bruce, R., Chauvin, A., Trinquart, L., Ravaud, P., &amp; Boutron, I. (2016). Impact of interventions to improve the quality of peer review of biomedical journals: A systematic review and meta- analysis. BMC Medicine, 14(1), 85. https://doi.org/10.1186/s12916-016-0631-5 The Data Team. (2016, September 7). Excel Errors and science papers. The Economist. Retrieved from https://www.economist .com/blogs/graphicdetail/2016/09/daily-chart-3 Fanelli, D. (2009). How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data. PloS One, 4(5), e5738. https://doi.org/10.1371/journal.pone.0005738 Gehlbach, H., &amp; Robinson, C. D. (2018). Mitigating illusory results through preregistration in education. Journal of Research on Educational Effectiveness, 11, 296–315. https://doi.org/10.1080/1934574 7.2017.1387950 Ioannidis, J. P. (2005). Why most published research findings are false. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124 Jefferson, T., Alderson, P., Wager, E., &amp; Davidoff, F. (2002). Effects of editorial peer review: A systematic review. JAMA, 287(21), 2784–2786. https://doi.org/10.1001/jama.287.21.2784 Nosek, B. A., &amp; Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192 Schweinsberg, M., Feldman, M., Staub, N., van den Akker, O. R., van Aert, R. C. M., van Assen, M. A. L. M., Liu, Y., Althoff, T., Heer, J., Kale, A., Mohamed, Z., Amireh, H., Prasad, V. V., Bernstein, A., Robinson, E., Snellman, K., Sommer, S. A., Otner, S. M. G., … Ulhmann, E. L. (2021). Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis. Organizational Behavior and Human Decision Processes, 165, 228-249. https://doi.org/10.1016/j.obhdp.2021.02.003 Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 Suber, P. (2004, June 21). Open Access overview. Retrieved from https://legacy.earlham.edu/~peters/fos/overview.htm "],["m3a-using-projects-and-scripts-in-r.html", "9 M3A: Using Projects and Scripts in R 9.1 Introduction 9.2 More About R and RStudio 9.3 The Console and the Workspace 9.4 Holding Onto Code 9.5 Conclusion", " 9 M3A: Using Projects and Scripts in R This content draws on material from: * STAT 545 by Jenny Bryan, licensed under CC BY-SA 4.0 Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim, licensed under CC BY-NC-SA 4.0 Changes to the source material include addition of new material; light editing; rearranging, removing, and combining original material; adding and changing links; and adding first-person language from current author. The resulting content is licensed under CC BY-NC-SA 4.0. 9.1 Introduction A couple of years ago, I had to make an official correction to my most-cited research study. My co-author and I were revisiting some of the calculations in the study as part of a separate project, and we found some discrepancies between the published results and the results that my co-author was getting. It turned out that back in 2017, we hadn’t been working off of the same data file when working on separate parts of the paper, and that created small-but-embarrassing inconsistencies between what we had published and what the actual results were. When you’re doing data science work, it’s very important to keep things organized! We’ve run some code in R already, but how we manage that code can be really important. In this walkthrough, we’ll learn some more R and RStudio features before learning more about scripts and Projects in R. By way of reminder, whenever you see a “code chunk,” you should type (or copy) the code and run it on your own; however, remember also that output looks a lot like code chunks but doesn’t need to be run! Any box that follows a code chunk and whose lines begin with ## is just output: It’s there for you to compare your results against, not for you to copy and paste. Likewise, text that is marked as code but is in the middle of a paragraph doesn’t need to be run; it’s just a way of showing that something is code. 9.2 More About R and RStudio Begin this walkthrough by starting RStudio. By way of reminder, as you open up the program, you should see something similar to Figure 9.1. Figure 9.1: RStudio interface to R. Note again the three panes (that is, the three panels dividing the screen): the console pane, the files pane, and the environment pane. Let’s spend some more time in the Console, which is where we interact with the live R process. The following code tells R to assign the result of 3 * 4 (that is, 3 times 4) to an object called x and then asks R to “inspect” the object (that is, retrieve its contents). Go ahead and run it on your own: x &lt;- 3 * 4 x [1] 12 All R statements where you create object —“assignments”—have this form: objectName &lt;- value. It is technically possible to use = to make assignments, too, but it’s a pretty bad idea, for reasons we don’t need to go into here. Yes, it takes a bit more effort, but always make assignments with &lt;-. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. It’s important to get into good habits with naming your objects. Many people use some kind of regular pattern in naming objects with multiple words: i_use_snake_case, other.people.use.periods, and evenOthersUseCamelCase Make another assignment by running this code: this_is_a_really_long_name &lt;- 2.5 To inspect this object, try out RStudio’s completion facility: type the first few characters of this_is_a_really_long_name, press TAB, add characters until you disambiguate, then press return. Now, make another assignment: stromae_rocks &lt;- 2 ^ 3 What happens if we try to run the following code to inspect our new object? stromaerocks Error in eval(expr, envir, enclos): object &#39;stromaerocks&#39; not found There’s an implicit contract when you work with a scripting language like R: The computer will do tedious computation for you, but you must be completely precise in your instructions. Typos matter. Case matters. Details matter. I am always happy to help you troubleshoot, but please make sure that you’ve checked the details of your code first. It’s amazing how often errors come down to mistyping something. R has a mind-blowing collection of built-in functions that are accessed like so: functionName(argument1 = value1, argument2 = value2, and so on). The details look different from function to function, but they typically follow this pattern. Let’s try functions using seq(), which makes regular sequences of numbers. While we’re at it, we’ll demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a function’s arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. As we saw earlier, you can also run a function with a ? in front of it (for example, ?seq()) to bring up the same help documentation. Now type the open parenthesis (() and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. These little features are the things that can make RStudio really useful (though I’ll also admit that they sometimes get in the way, too). seq(1, 10) [1] 1 2 3 4 5 6 7 8 9 10 Note that even though we ran this function, we didn’t assign the results to an object. That means that we can see the results in the Console, but we won’t be able to retrieve them later! If we want to hold onto the results of a function, we need to assign it to an object. Make this assignment and notice that RStudio helps with quotation marks in the same way that it helps with parentheses. yo &lt;- &quot;hello world&quot; If you just make an assignment, you don’t get to see the value, so then you’re tempted to immediately inspect. y &lt;- seq(1, 10) y [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen. (y &lt;- seq(1, 10)) [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() [1] &quot;Mon Jul 24 14:36:19 2023&quot; Now look at your workspace in the upper right Environment pane. The workspace is where user-defined objects accumulate. You ought to see all of the objects that we’ve created so far in this walkthrough. You can also get a listing of these objects with commands: objects() [1] &quot;airline_safety_smaller&quot; &quot;by_origin&quot; [3] &quot;by_origin_monthly&quot; &quot;by_origin_monthly_incorrect&quot; [5] &quot;ch4_scenarios&quot; &quot;chap&quot; [7] &quot;cut_levels&quot; &quot;dem_score&quot; [9] &quot;df&quot; &quot;drinks_smaller&quot; [11] &quot;drinks_smaller_tidy&quot; &quot;drinks_smaller_tidy_plot&quot; [13] &quot;flights&quot; &quot;flights_cols&quot; [15] &quot;flights_rows&quot; &quot;freq_dest&quot; [17] &quot;gain_summary&quot; &quot;guat_dem&quot; [19] &quot;guat_dem_tidy&quot; &quot;lc&quot; [21] &quot;n_alcohol_types&quot; &quot;n_countries&quot; [23] &quot;n_row_drinks&quot; &quot;named_dests&quot; [25] &quot;needed_CRAN_pkgs&quot; &quot;new_pkgs&quot; [27] &quot;stocks&quot; &quot;stocks_tidy&quot; [29] &quot;stromae_rocks&quot; &quot;summary_monthly_temp&quot; [31] &quot;summary_temp&quot; &quot;this_is_a_really_long_name&quot; [33] &quot;weather&quot; &quot;x&quot; [35] &quot;y&quot; &quot;yo&quot; ls() [1] &quot;airline_safety_smaller&quot; &quot;by_origin&quot; [3] &quot;by_origin_monthly&quot; &quot;by_origin_monthly_incorrect&quot; [5] &quot;ch4_scenarios&quot; &quot;chap&quot; [7] &quot;cut_levels&quot; &quot;dem_score&quot; [9] &quot;df&quot; &quot;drinks_smaller&quot; [11] &quot;drinks_smaller_tidy&quot; &quot;drinks_smaller_tidy_plot&quot; [13] &quot;flights&quot; &quot;flights_cols&quot; [15] &quot;flights_rows&quot; &quot;freq_dest&quot; [17] &quot;gain_summary&quot; &quot;guat_dem&quot; [19] &quot;guat_dem_tidy&quot; &quot;lc&quot; [21] &quot;n_alcohol_types&quot; &quot;n_countries&quot; [23] &quot;n_row_drinks&quot; &quot;named_dests&quot; [25] &quot;needed_CRAN_pkgs&quot; &quot;new_pkgs&quot; [27] &quot;stocks&quot; &quot;stocks_tidy&quot; [29] &quot;stromae_rocks&quot; &quot;summary_monthly_temp&quot; [31] &quot;summary_temp&quot; &quot;this_is_a_really_long_name&quot; [33] &quot;weather&quot; &quot;x&quot; [35] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) You can also click the broom in RStudio’s Environment pane to remove everything. Sometimes this is helpful when troubleshooting code. 9.3 The Console and the Workspace One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What from your analysis needs to be saved? Where does your analysis need to be saved? In the R code that we’ve written so far, we’ve run all of our code in the Console, and it’s been saved to the workspace. This has been sufficient for how we’ve done things so far, but it’s not a great long-term solution. Anything that we type into the Console gets saved into your “R history.” You can retrieve code from your R history by pressing the up arrow when your cursor is in the Console. This can be helpful for something you just typed, but it’s also a hassle to go all the way through your R history to find something that you typed hours (or even just minutes) ago. When you quit RStudio, it will generally prompt you to save your workspace; if you do, it will load it back automatically for you when you next open RStudio. This is also handy, but even though you have the results of your work, you don’t have easy access to the process that you took to get there. (That is, objects in your workspace are not easily reproducible). If you need to redo analysis, you’re going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history (using those arrow keys in the console) for the commands you used. A better use of your time and psychic energy is to keep your “good” R code in a script for future reuse—and to keep your related scripts bundled together into a project. In other words, you can do one-off, temporary, or unimportant things in the Console, but most of your code is worth saving—especially if you need help troubleshooting it! 9.4 Holding Onto Code The rest of this walkthrough will demonstrate how to write code in a way that lets you hold onto it later. This can be really helpful for you (for example, when you want to adapt code from a class activity for one of your projects), and it’s also really helpful for me (for example, when I want to see all the code you ran when helping you troubleshoot an activity). 9.4.1 RStudio Projects Keeping all the files associated with a project organized together—input data, R scripts, analytical results, figures—is such a wise and common practice that RStudio has built-in support for this. Let’s make a project for you to use during the rest of this semester. In RStudio, navigate to File and then New Project. When prompted, click on Existing Directory and then navigate to the folder for the GitHub repository that you created last week. Then, quit RStudio, open up the file interface for your operating system, and make your way to that folder. You should now see a .Rproj file within that folder (with the same name as the foler). If you click on that .Rproj file,it will open up a project-specific window for RStudio, including an interface in the bottom-right corner that shows all the files and folders associated with your project. Since all of your work for this class will all be stored in this single project, it may not be clear during this semester just how useful this can be—trust me, it’s darn useful. Now that you’ve created this project file, I always recommend opening it instead of just RStudio. This is the only project file that you need to create for this class! 9.4.2 Using Scripts The point of a project is to keep multiple files together. Any file that contains code is what we call a script. Again, holding onto your code (instead of just running it through the console) will make your life easier and make it easier for me to help you when you need it. Unless I tell you otherwise, whenever you’re starting a new activity in R, you should navigate through File, New File, and New R Script in RStudio, give that file a clear name, and save that file to your project folder. If you save this file anywhere else, it’s going to make your life harder. It is traditional to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Use helpful names for your scripts so that you can find them afterward! Running code within a script file works differently than running it in the console, so you ought to get familiar with these extra steps: to execute code within a script, you should highlight the code you want to execute and either click the Run button at the top of script window or use the keyboard shortcut Ctrl+Enter (Windows) or CMD+Enter (macOS). In theory, scripts are meant to be run all at once, and if you click the Run button without highlighting code, that’s what it will do. However, I (and many other R users) frequently use scripts in this kind of piecemeal way, running bits of code at a time, but keeping the whole script as a backup. 9.4.3 Backing Up Your Project Folder Make sure to save your scripts and other files regularly! However, unlike cloud services like Dropbox or Google Drive, simply saving a file to your project folder is not sufficient to sync it to GitHub. At regular intervals, you should open GitHub Desktop, navigate to your project repository, and click the button in the top right corner (which will read either Fetch origin or Pull origin—if, after clicking Fetch origin, it now reads Pull origin, you’ll want to click it again). If I’ve made changes to your repository, those changes will now sync to your local files. Then, if there are changed files in the appropriate window of the interface, you should enter a Summary in the appropriate field, click the Commit button, and then click Push origin in the top-right corner. It’s not until you’ve successfully pushed your files to GitHub that they are backed up and available for me to request and review! It’s important to note Git and GitHub are notorious for running into syncing issues. We are using it lightly enough that I don’t expect to run into anything major, but please don’t hesitate to reach out if you run into something you can’t solve on your own! 9.5 Conclusion When helping you figure out an issue you’re having with code, I will usually insist on getting access to your data and scripts through your GitHub repository and will usually not look over scripts and data that you send me via email. This means that you need to be in the habit of: saving your code to scripts, saving your scripts to your project folder, and backing up your project folder through GitHub. Please get into this habit early—it will be helpful for you throughout the semester and even more helpful for any data science work you do after this class. "],["m3c-writing-in-r-markdown.html", "10 M3C: Writing in R Markdown 10.1 Introduction 10.2 Markdown syntax 10.3 Trying out R Markdown", " 10 M3C: Writing in R Markdown This content draws on material from: * R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, and Garrett Grolemund, licensed under CC BY-NC-SA 4.0 Changes to the source material include addition of new material; light editing; removing original material; and adding first-person language from current author. The resulting content is licensed under CC BY-NC-SA 4.0. 10.1 Introduction Scripts are helpful for saving code for later, but to be truly reproducible, we often need to save more than just code. An R Markdown document can help us write a combination of text and code and then export it as a single document. This can be helpful for when you want to share your code, your results, and your thoughts on your results all together. This is helpful for this class in a number of ways! First, this entire textbook is written in R Markdown. Understanding chapters typically don’t have any code to go with them, but I can write text just fine. As we’ll see shortly, formatting the text requires me to use code rather than the kind of buttons that we’d use in Microsoft Word or Google Docs, but otherwise, it’s not that much different than writing in another kind of word processor. For walkthroughs, though, I not only need to write out instructions, but I also need to show you what code to run and what output you should expect to see. I tried doing that in another platform for the first edition of this textbook, and it was miserable; R Markdown is making me a much happier camper! Second, your class projects will all be submitted in R Markdown documents. It does mean asking you to learn a little bit about R Markdown, but it’s really useful for submitting and grading your assessments. Imagine that for each project, I asked you to submit your code, your data, and a separate Word file that contained your reflections. That would just be a hassle for both of us! With an R Markdown file, you can include your code, automatically show your results, and comment on what you found, all in a single document that you submit through Canvas. R Markdown builds on Markdown, a simple markup language (notice the pun?) that is popular in many ICT communities, including data scientists. A markup language is a way of using code to format text—HTML and XML are markup languages that you may be familiar with. Precisely speaking, R Markdown is based on Pandoc-flavored Markdown. There are many “flavors” of Markdown invented by different people, and Pandoc’s flavor is the best suited for data science work for a number of reasons. You can find the full documentation of Pandoc’s Markdown at https://pandoc.org/MANUAL.html. If you really want to learn about R Markdown, you ought to consult that document; however, the truth is that we’re using it pretty lightly this semester, so this short walkthrough ought to do the trick. 10.2 Markdown syntax Syntax is a programming term that refers to what code needs to be used how in order to produce a desired result. Of course, syntax is also used to refer to human languages—this is one of the many ways that learning to program resembles learning a new language. R syntax and Markdown syntax are two different things, and an R script and an R Markdown (.Rmd) document work in two different ways. In an .Rmd document, RStudio will expect that you are writing normal text (not code) unless and until you specify that you want to write some code. In the rest of the document, it’s assumed that you’re writing with Markdown. The following sections go over some of the options that you have available when writing with Markdown. Pay close attention to syntax—we established earlier in this module that programming in R requires paying close attention to detail. There is often no room for typos or doing things “slightly differently.” That is also true of Markdown, but if you pay very close attention to the examples and advice below, things should go pretty smoothly. 10.2.1 Inline formatting Inline text will be italic if surrounded by underscores or asterisks, e.g., _text_ or *text*. Bold text is produced using a pair of double asterisks (**text**). A pair of tildes (~) turn text to a subscript (e.g., H~3~PO~4~ renders H3PO4). A pair of carets (^) produce a superscript (e.g., Cu^2+^ renders Cu2+). To mark text as inline code, use a pair of backticks, e.g., `code`. Hyperlinks are created using the syntax [text](link); for example, [here's a link to our Canvas](https://uk.instructure.com/courses/2074926/). The syntax for images is similar: just add an exclamation mark, e.g., ![alt text or image title](path/to/image). Footnotes are put inside the square brackets after a caret ^[], e.g., ^[This is a footnote.]. 10.2.2 Block-level elements Section headers can be written after a number of pound signs, e.g., # First-level header ## Second-level header ### Third-level header If you do not want a certain heading to be numbered, you can add {-} or {.unnumbered} after the heading, e.g., # Preface {-} Unordered list items start with *, -, or +, and you can nest one list within another list by indenting the sub-list, e.g., - one item - one item - one item - one more item - one more item - one more item The output is: one item one item one item one more item one more item one more item Ordered list items start with numbers (you can also nest lists within lists), e.g., 1. the first item 2. the second item 3. the third item - one unordered item - one unordered item The output does not look too much different with the Markdown source: the first item the second item the third item one unordered item one unordered item Blockquotes are written after &gt;, e.g., &gt; &quot;I thoroughly disapprove of duels. If a man should challenge me, I would take him kindly and forgivingly by the hand and lead him to a quiet place and kill him.&quot; &gt; &gt; --- Mark Twain The actual output: “I thoroughly disapprove of duels. If a man should challenge me, I would take him kindly and forgivingly by the hand and lead him to a quiet place and kill him.” — Mark Twain In general, you’d better leave at least one empty line between adjacent but different elements, e.g., a header and a paragraph. This is to avoid ambiguity to the Markdown renderer. For example, does “#” indicate a header below? In R, the character # indicates a comment. And does “-” mean a bullet point below? The result of 5 - 3 is 2. Different flavors of Markdown may produce different results if there are no blank lines. 10.2.3 R code chunks You can insert an R code chunk either using the RStudio toolbar (the Insert button) or the keyboard shortcut Ctrl + Alt + I (Cmd + Option + I on macOS). That will create a little block in your document that begins with ```{r} and ends with ```. You can also manually type those characters onto two separate lines to create the code chunk. Once the code chunk is in place, you can type regular R code in there, run it, and see the output. There are a lot of things you can do in a code chunk, and you have fine control over all these output via chunk options. However, we don’t need to worry about those here—or even later on, since I have your code chunks set up for you in your project files. 10.3 Trying out R Markdown This walkthrough hasn’t been very interactive thus far! Let’s fix that by trying out some of the Markdown that we’ve been reading about. In RStudio, navigate to File, New File, and then R Markdown. On the next interface, just click on Create Empty Document in the bottom left corner. Once you have that file open, try writing a document that incorporates some of the formatting above. At regular intervals, click the Knit button to ask RStudio to transform your Markdown code into a formatted document. If something didn’t work as you expected, go back to the documentation above and pay close attention to details! "],["m4u-the-value-of-open-data.html", "11 M4U: The Value of Open Data 11.1 Introduction 11.2 Positivism and the Need for Data 11.3 Sharing Data 11.4 Sharing Data and Privacy 11.5 How to Share Data 11.6 Benefits of Sharing Data 11.7 Downsides of Sharing Data 11.8 Incentives for Sharing Data 11.9 Conclusion 11.10 References", " 11 M4U: The Value of Open Data This chapter draws on material from Open Education Science by Tim van der Zee and Justin Reich, licensed under CC BY-NC 4.0. Changes to the source material include removal of original material, the addition of new material, combining of sources, and editing of original material for a different audience. The resulting content is licensed under CC BY-NC 4.0. 11.1 Introduction This reading’s focus on open data is closely related to a previous reading’s focus on reproducibility, which means that there’s value in revisiting the idea of paradigms. I’ll use some different examples and language here, but you ought to be able to see how what I’ve written here connects with our previous discussion. A great way to remember what paradigms are and how they differ is to compare molecular biology (my spouse’s major in college) and French (my major in college). A professor of molecular biology (or of any of the other “hard sciences”) assumes that there is a predictable reality behind what they’re studying. That is, organic molecules work in particular, quasi-universal ways, and if you can figure that out, you can introduce established causes to bring about desired effects. This is often called a positivist paradigm, with “positivist” having connotations of “rational” and “data-driven.” In contrast, a professor of French (or any of the other disciplines in the humanities) assumes that what they’re studying is important, but largely arbitrary. French is decidedly not universal—it’s something that humans made up instead of discovered. Plus, it doesn’t apply to most humans and isn’t consistent across the humans that it does apply to (for example, French-speaking people from Switzerland count differently than French-speaking people from Canada—they can’t even agree on numbers). Thus, while you could try to talk about French in terms of cause and effect, most French professors are more interested in understanding and describing French than in predicting it. This is often called an interpretivist paradigm, underlining its focus on trying to understand humans’ semi-arbitrary meaning-making. In between the hard sciences and the humanities, there’s a large, important mega-discipline we can call the social sciences, which includes library and information sciences, technology studies, education research, and many applications of data science. There’s a fair bit of dispute within the social sciences about whether they ought to be modeled after the hard sciences (with a positivist paradigm) or the humanities (with an interpretivist paradigm—or one of a few others we haven’t covered here). That is, when we’re studying people and people-related phenomena, can we assume that there are quasi-universal laws that govern and therefore predict human behavior in the same way we can of atoms, chemicals, and molecules? Or, is it safer to assume that people’s behavior is context-dependent rather than universal and something to be understood rather than predicted? Based on what I’ve already shared, it shouldn’t surprise you that I am more of an interpretivist than a positivist—my research is much more interested in describing and understanding phenomena because I’m skeptical about the possibility or value of predicting human behavior. That said, I have a healthy respect for positivism, and I’ll be the first to admit that trying to determine cause and effect is more directly “useful” than trying to document contextual variations in human behavior. I prefer the interpretivist paradigm, but the fact is that we need positivist views, too. 11.2 Positivism and the Need for Data Even the most committed positivist will acknowledge that even if there are universal laws governing the behavior of atoms, bacteria, humans, or organizations, it can be tremendously difficult to determine what those laws are, especially since a lot of the easy stuff was figured out centuries or decades ago by folks like Newton, Mendel, and Curie. To identify a cause and effect relationship with a great degree of confidence requires a few things, including access to appropriate data. Appropriate data can cost a lot of money to gather, be difficult to gather, take a lot of time to gather, or all of the above. As we’ve touched on previously, modern information and communication technologies make it a lot easier to collect data than ever before, but just because one researcher has collected data doesn’t necessarily mean that they will share it with others. Many researchers—understandably—take the stance that “if I put all the effort into gathering this data, I’m not going to give it to others to analyze for free.” Nonetheless, governments and research funding agencies are increasingly requiring the scientists they fund to share their data as a condition of that funding. In parallel, work is being done to think more comprehensively about how to collect, manage, and share data in appropriate, responsible ways. These open science efforts overlap considerably with the push for reproducibility that we’ve already discussed; if there is a difference, I would describe it as a difference between quality and quantity. Reproducibility largely argues that: testable science leads to better science, sharing scientific materials allows for testing science, and sharing scientific materials therefore makes for better science. Open science largely argues that more science leads to better science, sharing data allows for more science, and sharing data therefore makes for better science. In general terms, I think that both of these arguments are sound and that locking down scientific data is morally questionable—however, we’ll see later that there are privacy issues closely connected with data sharing. 11.3 Sharing Data In this chapter, I will use the term Open Data to the practice of proactively sharing the data associated with a study (often along with materials, analysis code, and other important elements of a study, in the spirit of reproducibility) on a public repository such as GitHub, the Open Science Framework, or others. Research data include all data that are collected or generated during scientific (or other) research. Although data science is chiefly concerned with quantitative data (that is, data in the form of numbers), many scientists will also share non-numeric qualitative data that they have collected, including texts, visual stimuli, interview transcripts, log data, diaries, and any other materials that were used or produced in a study. I mentioned above that there’s a great deal of overlap between open science efforts and reproducibility efforts, so there shouldn’t be any surprise that Open Data supports both! In line with the statement that scholarly work should be verifiable, Open Data is the philosophy that authors should, as much as is practical, make all the relevant data publicly available so they can be inspected, verified, reused, and further built on. That is, you might use someone else’s research data in order to check their work—or you might be researching something similar, so the fact that they’ve already collected relevant data is going to save you time and effort. There are strong scientific ideals underpinning both: The U.S. National Research Council (1997) stated that “Freedom of inquiry, the full and open availability of scientific data on an international basis, and the open publication of results are cornerstones of basic research” (p. 2). 11.4 Sharing Data and Privacy Sharing data is not a dichotomous issue: That is, it’s not a simple decision of “yes, I’m sharing my data” or “no, I’m not sharing my data.” Rather, researchers have to decide 1) what data to share 2) with whom and 3) when. Let’s look at some examples from the discipline of education, which is my professional background. In the U.S., the National Center for Education Statistics (NCES) makes a wide variety of data sets publicly available for policymakers and researchers. This is good news! For several years, I’ve done research on how teachers use social media, and a few years ago, I put my head together with some other researchers to collectively ask whether teachers’ Twitter use in different U.S. states might have a connection with what education looks like in those states. We had the Twitter data, but it would have been really difficult for the three of us (especially with our barely existent budget) to come up with meaningful measures of what education policy looked like across different states. Luckly, there were gobs of data just waiting for us to download and consider on the NCES website! However, it’s not hard to imagine educational data that shouldn’t be available for any researcher who wants it. Examples related to the National Assessment of Education Progress (a regular assessment of what students in the United States know) demonstrate the need to balance privacy and opennness. For example, school-level data (which contain no personally identifiable information) are made easily accessible through public data sets. In contrast, student-level data are maintained with far stricter guidelines for accessibility and use—as they should be. However, statistical summaries, documentation of data collection methods, and codebooks of data are made easily and widely available. 11.5 How to Share Data The most common approach to open data is simply making data “available on request.” A scientist mentions somewhere that they’ve collected some interesting data and asks people to get in touch with them if they want access to it. However, this approach simply doesn’t work. In one study, researchers requested data from 140 authors with articles published in journals that required authors to share data on request, but only 25.7% of these data sets were actually shared (Wicherts, Borsboom, Kats, &amp; Molenaar, 2006). What is even more worrying is that reluctance to share data is associated with weaker evidence and a higher prevalence of apparent errors in the reporting of statistical results (Wicherts, Bakker, &amp; Molenaar, 2011). To increase the transparency of research, data should be shared proactively on a publicly accessible repository. GitHub can—and is—used for this, but other services are better equipped for it. Long-term and discoverable storage is advisable for data that are unique (i.e., can be produced just once) or that involved a considerable amount of resources to generate. These features are often true for qualitative and quantitative data alike. The value of shared data depends on quality of its documentation. Simply placing a data set online somewhere, without any explanation of its content, structure, and origin, is of limited value. A critical aspect of Open Data is ensuring that research data are findable (in a certified repository) as well as clearly documented by metadata and process documents (this is why GitHub, which doesn’t explicitly support this, isn’t as good for this sort of thing). In case research data cannot be shared at all, due to privacy issues or legal requirements, it is typically still possible to at least share metadata: information about the scope, structure, and content of the data set. For example, even though I’m generally excited about the idea of open science, I’m generally reluctant to share my Twitter data. For example, in the study I described above, we didn’t ask teachers for permission to collect their tweets before I studied them; that seems to me like a (perhaps justifiable) violation of privacy, so it’s important to me to protect these teachers’ privacy in other ways. So, I would never share the full set of data that I have, but when my co-authors and I did publish on this, we shared a summary of the data that leaves out any individual-level considerations. In addition, researchers can share “process documents,” which outline how, when, and where the data were collected and processed. In both cases (metadata and process documentation), transparency can be increased even when the research data themselves are not shared. New data-sharing repositories like Dataverse allow institutions or individual researchers to create data projects and share different elements of that project under different requirements so that some elements are accessible publicly and others require data use agreements (King, 2007). 11.6 Benefits of Sharing Data Open Data can improve the scientific process both during and after publication, in keeping with the connections to reproducibility and open science that we’ve made earlier. Without access to the data underlying a paper that is to be reviewed, peer reviewers are substantially hindered in their ability to assess the evidential value of the claims. Allowing reviewers to audit statistical calculations will have a positive effect on reducing the number of calculation errors, unsupported claims, and erroneous descriptive statistics that are later found in the published literature (Nuijten, Hartgerink, van Assen, Epskamp, &amp; Wicherts, 2016; Van der Zee, Anaya, &amp; Brown, 2017). Open Data also enables secondhand analyses and increases the value of gathering data, which require direct access to the data and cannot be performed using only the summary statistics typically presented in a paper. Data collection can be a lengthy and costly process, which makes it economically wasteful to not share this valuable commodity. Open Data is a research accelerator that can speed up the process of establishing new important findings (Pisani et al., 2016; Woelfle, Olliaro, &amp; Todd, 2011). 11.7 Downsides of Sharing Data Perhaps the strongest objection to Open Data sharing concerns issues of privacy protection. Safeguarding the identity and other valuable information of research participants is of utmost importance and takes priority over data sharing, but these are not mutually exclusive endeavors. Sharing data is not a binary decision, and there is a growing body of research around differential privacy that suggests a variegated approach to data sharing (Daries et al., 2014; Gaboardi et al., 2016; Wood et al., 2014). Even when a data set cannot be shared publicly in its entirety, it may be possible to share de-identified data or, as a minimum, information about the shape and structure of the data (i.e., meta-data). Daries et al. (2014) provided one case study of a de-identified data set from MOOC learners, which was too “blurred” for accurately estimating distributions or correlations about the population but could provide useful insights about the structure of the data set and opportunities for hypothesis generation. However, it should also be noted that it is sometimes easier to “re-identify” data than people think—especially in the world of big data According to a paper … in Scientific Reports … researchers at MIT and the Université Catholique de Louvain, in Belgium, analyzed data on 1.5 million cellphone users in a small European country over a span of 15 months and found that just four points of reference, with fairly low spatial and temporal resolution, was enough to uniquely identify 95 percent of them. In other words, to extract the complete location information for a single person from an “anonymized” data set of more than a million people, all you would need to do is place him or her within a couple of hundred yards of a cellphone transmitter, sometime over the course of an hour, four times in one year. A few Twitter posts would probably provide all the information you needed, if they contained specific information about the person’s whereabouts. (Hardesty, 2013) For textual data, such as transcripts from interviews and other forms of qualitative research, there are tools that allow researchers to quickly de-identify large bodies of texts, but textual data can also be deeply personal, so I’d personally have misgivings about sharing that data with others. Even when a whole data set cannot be shared, subsets might be shareable to provide more insight into coding techniques or other analytic approaches. Privacy concerns should absolutely shape decisions about what researchers choose to share, and researchers should pay particular attention to implications for informed consent and data collection practices, but research into differential privacy shows that openness and privacy can be balanced in thoughtful ways. Another concern with data sharing is “scooping” and problems with how research production is incentivized. Researchers are often under a lot of pressure to produce a lot of research, and research is nearly always judged on whether it contributes something new to human understanding. So, sharing your own data could potentially allow someone else to do an analysis that you were hoping to publish, and I don’t blame people for worrying about that. Furthermore, data scientists often work in the corporate world, and companies are probably even more protective of their data, since they don’t want competitors to see it. However, it’s also possible to go too far with this concern. For example, in an editorial in the New England Journal of Medicine, Longo and Drazen (2016) stated that: “There is concern among some front-line researchers that the system will be taken over by what some researchers have characterized as ‘research parasites’” (para. 3). Specifically, these authors were concerned that scholars might “parasitically” use data gathered by others; they suggested that data should instead be shared “symbiotically,” for example by demanding that the original researchers will be given co-author status on all papers that use data gathered by them. This editorial, and especially the framing of scholars as “parasites” for reusing valuable data, sparked considerable discussion. In fact, this discussion resulted in the “Research Parasite Award,” which reclaimed the derisive reference in the service of genuinely celebrating rigorous secondary analysis. 11.8 Incentives for Sharing Data These concerns—and their connection to how scientific research works—demonstrates the need to provide incentives for sharing data. The U.S. National Research Council (1997) has argued: “The value of data lies in their use. Full and open access to scientific data should be adopted as the international norm for the exchange of scientific data derived from publicly funded research” (p. 10). There are various ways to make better use of the data that we have already generated, such as data sets with persistent identifiers, so they can be properly cited by whoever has reused the data. This way, the data collectors continue to benefit from sharing their data as they will be repeatedly cited and have proof of how their data have been fundamental to others’ research. There is evidence that Open Data increase citation rates (Piwowar, Day, &amp; Fridsma, 2007), and other institutional actors could play a role in elevating the status of Open Data. An increasing number of academic journals have started to award special badges that will be shown on a paper that is accompanied by publicly available data in an Open Access repository. Journal policies can also have a strong positive effect on the prevalence of Open Data (Nuijten et al., 2017). Scholarly societies and research foundations could create new awards for the contribution of valuable data sets in education research. Perhaps most importantly, promotion and tenure committees in universities should recognize the value of contributing data sets to the public good and ensure that young scholars can be recognized for those contributions. 11.9 Conclusion Open data is an important component of the data science community, and it’s also important for supporting reproducibility and advancing science. Like other components of reproducibility, the value of open data emerges from a particular set of assumptions about how science works, and other scientific perspectives can help raise concerns about privacy and other issues. Nonetheless, so long as it is done responsibly, sharing data is a good and important thing. 11.10 References Daries, J. P., Reich, J., Waldo, J., Young, E. M., Whittinghill, J., Ho, A. D., . . . Chuang, I. (2014). Privacy, anonymity, and big data in the social sciences. Communications of the ACM, 57(9), 56–63. https://doi.org/10.1145/2643132 Gaboardi, M., Honaker, J., King, G., Nissim, K., Ullman, J., &amp; Vadhan, S. (2016). PSI: A private data sharing interface. Retrieved from https://arxiv.org/abs/1609.04340 Hardesty, L. (2013). How hard is it to ‘de-anonymize’ cellphone data? MIT News. https://news.mit.edu/2013/how-hard-it-de-anonymize-cellphone-data King, G. (2007). An introduction to the dataverse network as an infrastructure for data sharing. Sociological Methods &amp; Research, 36(2), 173-199. https://doi.org/10.1177/0049124107306660 Longo, D. L., &amp; Drazen, J. M. (2016). Data sharing. New England Journal of Medicine, 374(3). https://doi.org/10.1056/NEJMe1516564 National Research Council. (1997). Bits of power: Issues in global access to scientific data. Washington, DC: National Academy Press. Nuijten, M. B., Borghuis, J., Veldkamp, C. L. S., Alvarez, L. D., van Assen, M. A. L. M., &amp; Wicherts, J. M. (2017, July 13). Journal data sharing policies and statistical reporting inconsistencies in psychology. Retrieved from https://osf.io/preprints/psyarxiv/sgbta Nuijten, M. B., Hartgerink, C. H., van Assen, M. A., Epskamp, S., &amp; Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226. https://doi.org/10.3758/s13428-015-0664-2 Piwowar, H. A., Day, R. S., &amp; Fridsma, D. B. (2007). Sharing detailed research data is associated with increased citation rate. PloS One, 2(3), e308. https://doi.org/10.1371/journal.pone.0000308 Pisani, E., Aaby, P., Breugelmans, J. G., Carr, D., Groves, T., Helinski, M., . . . Mboup, S. (2016). Beyond open data: Realising the health benefits of sharing data. BMJ, 355. https://doi.org/10.1136/bmj.i5295 Van der Zee, T., Anaya, J., &amp; Brown, N. J. L. (2017). Statistical heartburn: An attempt to digest four pizza publications from the Cornell Food and Brand Lab. BMC Nutrition, 3(54). https://doi.org/10.1186/s40795-017-0167-x Wicherts, J. M., Bakker, M., &amp; Molenaar, D. (2011). Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results. PloS One, 6(11), e26828. https://doi.org/10.1371/journal.pone.0026828 Wicherts, J. M., Borsboom, D., Kats, J., &amp; Molenaar, D. (2006). The poor availability of psychological research data for reanalysis. American Psychologist, 61(7), 726–728. https://doi.org/10.1037/0003-066X.61.7.726 Woelfle, M., Olliaro, P., &amp; Todd, M. H. (2011). Open science is a research accelerator. Nature Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149 Wood, A., O’Brien, D., Altman, M., Karr, A., Gasser, U., Bar- Sinai, M., . . . Wojcik, M. J. (2014). Integrating approaches to privacy across the research lifecycle: Long-term longitudinal studies. Retrieved from https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2469848 "],["m4a-find-a-dataset-relevant-to-you.html", "12 M4A: Find a Dataset Relevant To You 12.1 Introduction 12.2 Signs of a Good Dataset 12.3 Finding a Good Dataset 12.4 Loading Your Dataset", " 12 M4A: Find a Dataset Relevant To You 12.1 Introduction Connection activities play a big part in this class. Understanding and application of data science are both great, but neither is very useful if you can’t bring them into a specific situation and make them work for you. With that in mind, most future connection activities (and all of your projects) are going to expect that you have access to a data set that you feel a personal or professional connection to. Ideally, you’d find a data set that is connected to your (present, past, or hoped-for future) professional context, whether that’s retrieved from your workplace (with permission, of course) or at least contextually relevant. However, I’ve also enjoyed seeing 661 students work with personally meaningful datasets related to Pokémon, Spotify, sports, or other interests. While I’m going to encourage you to pick something professionally relevant, the most important thing is that you work with something interesting. This activity is all about finding one such relevant data set. If you find a solid data set today (one that meets the explicit and implicit requirements of projects and activities), you may be able to stick with it throughout the rest of the semester. However, if you get bored with a data set—or realize that your go-to data set won’t work for a particular activity—you can always find something else. 12.2 Signs of a Good Dataset Before we get into the details, here are some important characteristics you’ll want to look for in data sets: at least 100 observations and 5 variables depending on the activity, you’ll probably want at least 5 quantitative variables (i.e., numbers); other variables are great, but they aren’t compatible with a lot of the statistical techniques we’ll be talking about although we challenge this idea in our class, more data is often better from a statistical point of view; if you can find a dataset with at least 1,000 observations and 10 variables, that will probably make things easier for you stored in a spreadsheet file format personally, I prefer .csv; I find it easier to work with, and it’s standard in data science circles .xsl and .xslx Excel spreadsheets are also common and supported by data import functions; however, I’ll be focusing on .csv when providing assistance, so you will either have to do some extra research to adapt my code, or you’ll want to convert your spreadsheet to .csv “tidy” data if you can find datasets that advertise themselves as tidy (1 observation per row, 1 variable per column), that will probably save you some hassle that said, we won’t cover tidy data until later in the semester, and it’s not always easy to tell whether another party’s dataset is tidy, so don’t stress too much about this 12.3 Finding a Good Dataset Okay, now for the finding! There are plenty of sources for interesting data on the internet. The Kaggle collection of datasets is full of freely-available data ranging from the serious to the silly. It’s a great place to look if you don’t know where else to go. Governments of all kinds and sizes also collect data: You might search for federal, state, or local collections of “open data” if you think there might be something interesting for you there. A workplace is a good place to check for data so long as you can be sure that you have permission to do so. Finally, there are a lot of datasets associated with R packages. You can do some searching for one of those right now or pay close attention to the ones that we meet over the next few weeks; you might find one that you want to return to! Once you’ve found the dataset you want to work with (at least for now), give it a short-but-descriptive name. If it’s a file format other than a .csv, you should either convert it into a .csv or be ready to look up how to load a file in another format into R, since I won’t be providing those directions. Once the file is ready to be used, move it to the activity_data folder inside the project folder that you previously created for this class. Then, open up GitHub Desktop to commit this change to your project and push the changes so that they sync with the GitHub website. I mentioned above that you’re free to stick with this data throughout the rest of the semester or that you can switch to new datasets whenever you feel like it. If you ever switch to a new dataset, make sure that you follow the steps above (giving it a short-but-descriptive name, ensuring it’s a .csv or that you know how to load other data types, placing it in your activity_data folder, and committing and pushing your changes in GitHub Desktop) every time. 12.4 Loading Your Dataset I’ve already emphasized in a couple of walkthroughs up to this point that it’s important to be attentive to details when working in R. This is especially true when it comes to loading data into R. Up to this point, we have only worked with data that is preloaded in R packages; so long as you’ve properly loaded the package in question, you’ve loaded the data as well. A lot of our walkthroughs will only ever ask you to work with this data. However, later in the course, you’ll be adapting code from the walkthroughs to work with your own data; more importantly, you’ll be working with your own data when completing course projects. When you’re working with your own data, you need to make sure it gets loaded into RStudio before your code can work with it. Let’s practice doing that now. We’re going to learn to do this through code. There is also a way to do this through the RStudio interface instead of typing out the code, but I’m not going to cover it here. My reasons for doing so are multiple: First, I always load my data through code, so I honestly don’t know how to do it the other way off the top of my head. Now, I could look it up, but I also have other strong reasons for not covering this. Loading your data through the interface may be more straightforward when you’re completing walkthroughs and one-off activities, but: it is not reproducible, and it makes it harder for me to troubleshoot your code if you need any help, and it does not play nice with the .Rmd documents that you will use to submit your class projects, so you will have to load through code for the projects To practice loading data, start by creating a new script in RStudio. Last week, we emphasized that saving your code in scripts is really helpful for referencing it later. You might want to give this script a name like data_loading.R or something like that, to make it easy to reference in the future in case you need a refresher on how to load your own data. Please note that with future activities, I’m going to assume that you’re creating a new script for each activity and that you’re giving it a descriptive name—you won’t get advice like this in all walkthroughs! However, we’re still in the early stages of working with R, so a reminder here might be helpful! We’re going to use two packages as part of our loading process: tidyverse and here. Chances are that you don’t have these packages installed yet, so you can do that now by running install.packages(\"tidyverse\") and install.packages(\"here\"). I recommend running this code in the Console. I’ve put a lot of emphasis on saving code in a script, but there’s always code that is so unimportant that it doesn’t need to be saved. This includes installing packages, which is almost always a one-and-done thing and doesn’t need to be repeated; besides, if you include installation code in the .Rmd files for your class projects, it creates all sorts of problems. Now, let’s go ahead and load those packages: library(tidyverse) library(here) We’re going to cover the tidyverse package in more detail in a couple of weeks, so for now, just trust me that it’s important and necessary. The here package, on the other hand, is worth getting into now. For any program on a computer to work with a file, it has to know where that file is so that it can access it. Modern programs on modern computers make this process so easy for us that we often don’t have to pay attention to the details of where a file is. R, on the other hand, is not so generous; it usually needs specific instructions for where to find a file before it can load it. The point of the here package is to make giving those specific instructions a bit easier for us. Let’s see what I mean by running the function here() now that we have the package of the same name loaded: here() [1] &quot;/Users/spencergreenhalgh/spg_website/static/ICT_LIS_661_textbook_2023_Fall&quot; Now, this is one case where the output that you see in the book is different than the output you’ll see where you run this code—and that’s the point of here()! What this function does is to figure out the filepath for the folder that contains the R project you’re working in. So, the output that I get when I run here() is the location of the R project associated with this textbook; it’s also formatted the way that macOS (the operating system I’m currently using) formats locations. You ought to get output that tells you the location of the R project that you set up for this class and that formats it in the way that your operating system formats locations. Think of here() as a shortcut that takes you to the main folder that you’re working in; that way, you only have to provide the extra detail beyond that. For example, if you’ve followed the directions in the previous section, you should have your dataset for this activity saved in the activity_data folder. I also have a dataset—called Twitter_hashtags.csv—saved in my activity_data folder. If I run the following code, it tells R that I I’m looking inside my project folder (what the here() function figures out on its own) to find a subfolder called activity_data inside that project folder, to find a file called Twitter_hashtags.csv inside that subfolder. The result is the exact location of the file on my computer, providing a lot more information that I didn’t have to type out on my own! here(&quot;activity_data&quot;,&quot;Twitter_hashtags.csv&quot;) [1] &quot;/Users/spencergreenhalgh/spg_website/static/ICT_LIS_661_textbook_2023_Fall/activity_data/Twitter_hashtags.csv&quot; When you run this code, it’s unlikely that your dataset is named Twitter_hashtags.csv, so you should replace that with the name of your file. Likewise, as we saw above, your output should also look different because the location of your project folder is different than the location of mine! No matter how different your output looks, though, you ought to notice how much shorter your here() argument is than the exact location that you might otherwise have to type out. That’s why here() is useful! In fact, let’s get to the good stuff. Now that I have the exact location of my dataset on my computer, I can put that inside the read_csv() function, which (as the name suggests), loads .csv files into RStudio. With the following code, I’m going to read my data in and save it as an object called df (short for dataframe—it’s a handy default name for the object containing your data). df &lt;- read_csv(here(&quot;activity_data&quot;,&quot;Twitter_hashtags.csv&quot;)) Rows: 60 Columns: 22 ── Column specification ──────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr (1): hashtags dbl (21): avg_hashtags_per_tweet, avg_mentions_per_tweet, proportion_origina... ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. You should run this code yourself. You can give your object a more descriptive name than df if you want, but you absolutely have to change Twitter_hashtags.csv to the name of your file if you want this to work! Once again, you should expect your output to look different than mine; my output is a quick summary of my data; your output will be a quick summary of your data. The explanation-to-code ratio of this walkthrough is pretty high—higher than is ideal. The important part is that you remember this last line of code that you ran; so long as you save your data to the activity_data subfolder in your project folder and replace Twitter_hashtags.csv with the name of your dataset, this code will be a reliable way for you to load data into R. Knowing how to do this will be very useful for you for class projects and other activities. The rest of the explanation is just to emphasize how important this line of code is! "],["m4c-show-your-work.html", "13 M4C: Show Your Work 13.1 Introduction 13.2 Invisible Labor 13.3 The Invisible Labor of Data Science 13.4 Time and Money 13.5 Crediting Data Work 13.6 Crediting Emotional Labor and Care Work 13.7 Show Your Work 13.8 References", " 13 M4C: Show Your Work This chapter draws on material from 7. Show Your Work by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0. Changes to the source material include light editing, adding new material, deleting original material, rearranging material, changing the citation style, adding links, replacing images, changing original authors’ voice to third person, and adding first-person language from current author. The resulting content is licensed under CC BY 4.0. 13.1 Introduction Earlier this semester, each of you set up an account with GitHub. As of June 2018, this online code-management platform had over twenty-eight million users worldwide. By allowing users to create web-based repositories of source code (among other forms of content) to which project teams of any size can then contribute, GitHub makes collaborating on a single piece of software or a website or even a book much easier than it has ever been before (this book is hosted in GitHub, as is a book on data science in education written by my friend Josh and his co-authors). Well, at least if you’re a man. A 2016 study found that female GitHub users were less likely to have their contributions accepted if they identified themselves in their user profiles as women. The authors do not appear to have considered nonbinary genders, although they do maintain a category for “gender-neutral” usernames that cannot be categorized as either men’s or women’s names (Terrell et al,. 2017). Critics of GitHub’s commitment to inclusivity, or the lack thereof, also point to the company’s internal politics. In 2014, GitHub’s cofounder was forced to resign after allegations of sexual harassment were brought to light (Miller, 2014). More recently, in 2018, Agnes Pak, a former top attorney at GitHub, sued the company for allegedly altering her performance reviews after she complained about her gender and race contributing to a lower compensation package, giving them the grounds to fire her (Baron, 2018). Pak’s suit came only shortly after transgender software developer Coraline Ada Ehmke, in 2017, declined a significant severance package so that she could talk publicly about her negative experience of working at GitHub (Ehmke, 2017). Clearly, GitHub has several major issues of corporate culture that it must address. But a corporate culture that is hostile to women does not necessarily preclude other feminist interventions, and GitHub makes one important one: its platform helps show the work of writing collaborative code. In addition to basic project management tools, like bug tracking and feature requests, the GitHub platform also generates visualizations of each team member’s contributions to a project’s codebase. Area charts, arranged in small multiples, allow viewers to compare the quantity, frequency, and duration of any particular member’s contributions. A line graph reveals patterns in the day of the week when those contributions took place. And a flowchart-like diagram of the relationships between various branches of the project’s code helps to acknowledge any sources for the project that might otherwise go uncredited, as well as any additional projects that might build upon the project’s initial work. Coding is work, as anyone who’s ever programmed anything knows well. But it’s not always work that is easy to see. The same is true for collecting, analyzing, and visualizing data. We tend to marvel at the scale and complexity of an interactive visualization like the Ship Map, which plots the paths of the global merchant fleet over the course of the 2012 calendar year. By showing every single sea voyage, the Ship Map exposes the networks of waterways that constitute our global product supply chain. However, we are less often exposed to the networks of processes and people that help constitute the visualization itself—from the seventy-five corporate researchers at Clarksons Research UK who assembled and validated the underlying dataset, to the academic research team at University College London’s Institute that developed the data model, to the design team at Kiln that transformed the data model into the visualization that we see. And that is to say nothing of the tens of thousands of commercial ships that served as the source of data in the first place. Visualizations like the Ship Map involve the work of many hands. Unfortunately, however, when releasing a data product to the public, we tend not to credit the many hands who perform this work. We often cite the source of the dataset, and the names of the people who designed and implemented the code and graphic elements. But we rarely dig deeper to discover who created the data in first place, who collected the data and processed them for use, and who else might have labored to make creations like the Ship Map possible. Admittedly, this information is sometimes hard to find. Likewise, when project teams (or individuals) are already operating at full capacity, or under budgetary strain, this information can—ironically—simply be too much additional work to pursue (see Martin, 2010). Even in cases in which there are both resources and desire, information about the range of the contributors to any particular project sometimes can’t be found at all. 13.2 Invisible Labor But the various difficulties we encounter when trying to acknowledge this work reflects a larger problem in what information studies scholar Miriam Posner (2018) calls our data supply chain. Like the contents of the ships visualized on the Ship Map, about which we only know sparse details—the map can tell us if a shipping container was loaded onto the boat, but not what the shipping container contains—the invisible labor involved in data work, as Posner argues, is something that corporations have an interest in keeping out of public view. To put it more simply, it’s not a coincidence that much of the work that goes into designing a data product—visualization, algorithm, model, app—remains invisible and uncredited. In our capitalist society, we tend to value work that we can see. This is the result of a system in which the cultural worth of any particular form of work is directly connected to the price we pay for it; because a service costs money, we recognize its larger value. More often than not, though, the reverse also holds true: We fail to recognize the larger value of the services we get for free. When, in the early 1970s, the International Feminist Collective launched the Wages for Housework campaign, it was this phenomenon of invisible labor—labor that was unpaid and therefore unvalued—that the group was trying to expose (see Federici, 1975). The precise term they used to describe this work was reproductive labor, which comes from the classical economic distinction between the paid and therefore economically productive labor of the marketplace, and the unpaid and therefore economically unproductive labor of everything else. By reframing this latter category of work as reproductive labor, rather than simply (and inaccurately) unproductive labor, groups like the International Feminist Collective sought to emphasize how the range of tasks that the term encompassed, like cooking and cleaning and child-rearing, were precisely the tasks that enabled those who performed “productive” labor, like office or factory work, to continue to do so. The Wages for Housework movement began in Italy and migrated to the United States with the help of labor organizer and theorist Silvia Federici. It eventually claimed chapters in several American cities, and did important consciousness-raising work. Still, as prominent feminists like Angela Davis pointed out, while housework might have been unpaid for white women, women of color—especially Black women in the United States—had long been paid, albeit not well, for their housework in other people’s homes: “Because of the added intrusion of racism, vast numbers of Black women have had to do their own housekeeping and other women’s home chores as well” (Davis, 1983). Here, Davis is making an important point about racialized labor: just as housework is structured along the lines of gender, it is also structured along the lines of race and class. The domestic labor of women of color was and remains underwaged labor, as feminist labor theorists would call it, and its low cost was what permitted (and continues to permit) many white middle- and upper-class women to participate in the more lucrative waged labor market instead. Since the 1970s, the term invisible labor has come to encompass the various forms of labor, unwaged, underwaged, and even waged, that are rendered invisible because they take place inside of the home, because they take place out of sight, or because they lack physical form altogether. Visit WagesforFacebook.com and you’ll find a version of the Wages for Housework argument updated for a new form of invisible work. This invisible labor can be found all over the web, as digital labor theorists such as Tiziana Terranova (2000) have helped us to understand. “They call it sharing. We call it stealing,” is one of the statements that scrolls down the screen in large black type. The word it refers to work that most of us perform every day, in the form of our Facebook likes, Instagram posts, and Twitter tweets. The point made by Laurel Ptak, the artist behind Wages for Facebook—a point also made by Terranova—is that the invisible unpaid labor of our likes and tweets is precisely what enables the Facebooks and Twitters of the world to profit and thrive. 13.3 The Invisible Labor of Data Science The world of data science is able to profit and thrive because of unpaid invisible labor as well. Perhaps my “favorite” example of this is from the very geeky xkcd webcomic, which once ran the following comic: Self Driving by Randall Munroe is licensed under CC BY-NC 2.5 Have you ever wondered why these distinctive CAPTCHA tests require you to do image identification? Or why they’re so often related to roads, signs, and vehicles? The idea behind a CAPTCHA test is to require a human to do a task that an automated system can’t do; ironically, though, those aggregated human tasks are often used to train automated systems to work better. Google (who owns the popular reCAPTCHA system) uses CAPTCHA tests to train systems for automated image recognition—which has obvious benefits if you’re developing self-driving cars. Ever since reading this comic, I’ve gotten angry whenever I complete one of these. Not only does such-and-such a website believe that I’m a robot, but the only way to prove my humanity is to do free work for Google. All I get out of it is to continue to the website I want, but Google is in a position to make gobs of money based on my (and millions of others’) free labor. Other crowdsourcing projects (helping Netflix with its recommendation algorithm, helping a newspaper go through interesting documents, etc.) are framed as acts of benevolence (and, in the case of Netflix, an opportunity to win a million-dollar prize). People should want to contribute to these projects, their proponents claimed, since their labor would further the public good. It’s also worth pointing out, though, that when he coined the term in the mid-2000s, Jeff Howe argued that crowdsourcing was a powerful way to tap networked knowledge for corporate use. “The labor isn’t always free, but it costs a lot less than paying traditional employees,” he wrote (Howe, 2006). Plus, Ashe Dryden, the software developer and diversity consultant, points out that people can only help crowdsource if they have the inclination and the time (Dryden, 2014). Think back to that study of GitHub. If you were a woman and you knew your contributions to a programming project were less likely to be accepted than if you were a man, would that motivate you to contribute the project? Or, for another example, consider Wikipedia. Although the exact gender demographics of Wikipedia contributors are unknown, numerous surveys have indicated that those who contribute content to the crowdsourced encyclopedia are between 84 percent and 91.5 percent men (Hills &amp; Shaw, 2013; Wikimedia, n.d.). Why? It could be that there, too, edits are less likely to be accepted if they come from women editors (Whose Knowledge, n.d.). It could also be attributed to Wikipedia’s exclusionary editing culture and technological infrastructure, as science and technology studies (STS) scholars Heather Ford and Judy Wajcman (2017) have argued. And there is also reason to go back to the housework argument. Dryden cites a 2011 study showing that women in twenty-nine countries spend more than twice as much time on household tasks than men do, even when controlling for women who hold full-time jobs (Miranda, 2011). Like the other estimates of gender cited here, the study did not consider nonbinary genders or same-sex (or other non-heterotypical) households. But even as a rough estimate, it seems that women simply don’t have as much time. 13.4 Time and Money In capitalist societies, it’s very often the case that time is money. But it’s also important to remember to ask whose time is being spent and whose money is being saved. The premise behind Amazon’s Mechanical Turk—or MTurk, as the crowdsourcing platform is more commonly known—is that data scientists want to save their own time and their own bottom line. The MTurk website touts its access to a global marketplace of “on-demand Workers,” who are advertised as being more “scalable and cost-effective” than the “time consuming [and] expensive” process of hiring actual employees (Amazon Mechanical Turk, n.d.) Thus, MTurk is a marketplace for crowdsourced human labor—individuals and businesses that have “human intelligence tasks” can set up jobs for online workers to complete. Often repetitive, like describing images or taking surveys, the jobs are compensated in tiny financial increments that don’t add up to a minimum hourly wage—not to mention a living wage. However, a recent study by the Pew Research Center showed that 51 percent of US-based Turkers, as they are known, hold college degrees, and 88 percent are below the age of fifty, among other metrics that would otherwise rank them among the most desired demographic for salaried employees (Hitlin, 2016). This form of underwaged work is also increasingly outsourced from the United States to countries with fewer (or worse) labor laws and fewer (or worse) opportunities for economic advancement. A 2010 University of California, Irvine study measured a 20 percent drop in the number of US-based Turkers over the eighteen months that it monitored (Ross et al., 2010). This trend has continued, the real-time MTurk tracker shows. (The gender split, interestingly, has evened out over time.) Even at resource-rich companies like Amazon and Google, the work of data entry is profoundly undervalued in proportion to the knowledge it helps to create. Andrew Norman Wilson’s 2011 documentary Workers Leaving the Googleplex exposes how the workers tasked with scanning the books for the Google Books database are hired as a separate but unequal class of employee, with ID cards that restrict their access to most of the Google campus and that prevent them from enjoying the company’s famed employee perks. 13.5 Crediting Data Work The emphasis on giving formal credit for a broad range of work derives from feminist practices of citation. Feminist theorist Sara Ahmed describes this practice as a way of resisting how certain types of people—usually cis and white and male—“take up spaces by screening out others” (Ahmed, 2013). When those other people are screened out, they become invisible, and their contributions go unrecognized. The screening techniques (as Ahmed terms them) that lead to their erasure are not always intentional, but they are, unfortunately, self-perpetuating. Ahmed gives the example of sinking into a leather armchair that is comfortable because it’s molded to the shape of your body over time. You probably wouldn’t notice how the chair would be uncomfortable for those who haven’t spent time sitting in it—those with different bodies or with different demands on their time. Which is why those of us who occupy those comfortable leather seats—or, more likely in the design world, molded plastic Eames chairs—must remain vigilant in reminding ourselves of the additional forms of labor, and the additional people, that our own data work rests upon. This gets complicated quickly even on the scale of a single data science project. The names of all the people and the work they perform are not always easy to locate—if they can be located at all. But taking steps to document all the people who work on a particular project at the time that it is taking place can help to ensure that a record of that work remains after the project has been completed. In fact, this is among the four core principles that comprise the Collaborators’ Bill of Rights, a document developed by an interdisciplinary team of librarians, staff technologists, scholars, and postdoctoral fellows in 2011 in response to the proliferation of types of positions, at widely divergent ranks, that were being asked to contribute to data-based (and other digital) projects (“Collaborators’ Bill of Rights”, 2011; see also Di Pressi et al., 2018). When designing data products from a feminist perspective, we must similarly aspire to show the work involved in the entire lifecycle of the project. This remains true even as it can be difficult to name each individual involved or when the work may be collective in nature and not able to be attributed to a single source. In these cases, we might take inspiration from the Next System Project, a research group aimed at documenting and visualizing alternative economic systems (Gibson-Graham &amp; the Community Economies Collective, 2017). In one report, the group compiled information on the diversity of community economies operating in locations as far-ranging as Negros Island, in the Philippines; Québec province, in Canada; and the state of Kerala, in India. The report employs the visual metaphor of an iceberg, in which wage labor is positioned at the tip of the iceberg, floating above the water, while dozens of other forms of labor—informal lending, consumer cooperatives, and work within families, among others—are positioned below the water, providing essential economic ballast but remaining out of sight. But in more instances than you might think, the labor associated with data work can be surfaced through the data themselves. For instance, historian Benjamin Schmidt (2017), whose research centers on the role of government agencies in shaping public knowledge, decided to visualize the metadata associated with the digital catalog of the US Library of Congress, the largest library in the world. Schmidt’s initial goal was to understand the collection and the classification system that structured the catalog. But in the process of visualizing the catalog records, he discovered something else: a record of the labor of the cataloguers themselves. When he plotted the year that each book’s record was created against the year that the book was published (you can see the plot here, he saw some unusual patterns in the image: shaded vertical lines, step-like structures, and dark vertical bands that didn’t match up with what one might otherwise assume would be a basic two-step process of (1) acquire a book and (2) enter it in. The shaded vertical lines, Schmidt soon realized, showed the point at which the cataloguers began to turn back to the books that had been published before the library went digital, filling in the online catalogue with older books. The step-like patterns indicated the periods of time, later in the process, when the cataloguers returned to specific subcollections of the library, entering in the data for the entire set of books in a short period of time. And the horizontal lines? Well, given that they appear only in the years 1800 and 1900, Schmidt inferred that they indicated missing publication information, as best practices for library cataloguing dictate that the first year of the century be entered when the exact publication date is unknown. With an emphasis on showing the work, these visual artifacts should also prompt us to consider just how much physical work was involved in converting the library’s paper records to digital form. The darker areas of the chart don’t just indicate a larger number of books entered into the catalog, after all. They also indicate the people who typed them all in. (Schmidt estimates the total number of records at ten million and growing.) Similarly, the step-like formations don’t just indicate a higher volume of data entry. They indicate strategic decisions made by library staff to return to specific parts of the collection and reflect those staff members’ prior knowledge of the gaps that needed to be filled—in other words, their intellectual labor as well. Schmidt’s visualization helps to show how the dataset always points back to the data setting—to use Yanni Loukissas’s helpful phrase—as well as to the people who labored in that setting to produce the data that we see. 13.6 Crediting Emotional Labor and Care Work In addition to the invisible labor of data work, there is also labor that remains hidden because we are not trained to think of it as labor at all. This is what is known as emotional labor, and it’s another form of work that feminist theory has helped to bring to light. As described by feminist sociologist Arlie Hochschild (2012), emotional labor describes the work involved in managing one’s feelings, or someone else’s, in response to the demands of society or a particular job. Hochschild coined the term in the late 1970s to describe the labor required of service industry workers, such as flight attendants, who are required to manage their own fear while also calming passengers during adverse flight conditions, and generally work to ensure that flight passengers feel cared for and content. In the decades that followed, the notion of emotional labor was supplemented by a related concept, affective labor, so that the work of projecting a feeling (the definition of emotion) could be distinguished from the work of experiencing the feeling itself (the definition of affect). We can see both emotional and affective labor at work all across the technology industry today. Consider, for instance, how call center workers and other technical support specialists must exert a combination of affective and emotional labor, as well as technical expertise, to absorb the rage of irate customers (affective labor), reflect back their sympathy (emotional labor), and then help them with—for instance—the configuration of their wireless router (technical expertise; see Brophy &amp; Woodcock, 2019). In the workplace, we might also consider the affective labor required by women and minoritized groups, in all situations, who must take steps to disprove (or simply ignore) the sexist, racist, or otherist assumptions they face—about their technical ability or about anything else. And they must do so while also performing the emotional labor that ensures that they do not threaten those who hold those assumptions, who often also hold positions of power over them. Are there ways to visualize these forms of labor, giving visual presence—and therefore acknowledgement and credit—to these outlays of work? 13.7 Show Your Work Data work is part of a larger ecology of knowledge, one that must be both sustainable and socially just. Like the ship paths visualized on the Ship Map or the source code stored on GitHub, the network of people who contribute to data projects is vast and complex. Showing this work is an essential component of feminist data science. An emphasis on labor opens the door to the interdisciplinary area of data production studies: taking a data visualization, model, or product and tracing it back to its material conditions and contexts, as well as to the quality and character of the work and the people required to make it. This kind of careful excavation can be undertaken in academic, journalistic, or general contexts, in all cases helping to make more clearly visible—and therefore to value—the work that data science rests upon. We can also look to the data themselves in order to honor the range of forms of invisible labor involved in data science. Who is credited on each project? Whose work has been “screened out”? While one strategy is to show the work behind making data products themselves, another strategy for honoring work of all forms is to use data science to show the work of people (mostly women) who labor in other sectors of the economy, those that involve emotional labor, domestic work, and care work. Designing in solidarity with domestic workers can begin to challenge the structural inequalities that relegate their work to the margins in the first place. This point brings us back to ideas about power that we will read about throughout this semester. Power imbalances are everywhere in data science: in our datasets, in our data products, and in the environments that enable our data work. Showing the work is crucial to ensure that undervalued and invisible labor receives the credit it deserves, as well as to understand the true cost and planetary consequences of data work. 13.8 References Ahmed, S. (2013, September 11). Making feminist points. Feministkilljoys [blog]. https://feministkilljoys.com/2013/09/11/making-feminist-points/ Amazon Mechanical Turk. (n.d.). https://www.mturk.com/ (accessed April 22, 2019). Baron, E. (2018, October 1) GitHub paid executive less because she’s Asian and female, fired her for complaining: Lawsuit. Mercury News. https://www.mercurynews.com/2018/10/01/github-paid-executive-less-because-shes-asian-and-female-fired-her-for-complaining-lawsuit/ Brophy, E., &amp; Woodcock, J. (2019, February 14). The call centre seen from below: Issue 4.3 editorial. Notes from Below. https://notesfrombelow.org/article/call-centre-seen-below-issue-43-editorial “Collaborators’ Bill of Rights.” (2011). In Off the tracks: Laying new lines for digital humanities scholars. MediaCommons. Davis, A. (1983). Women, race, &amp; class. Penguin Random House. Di Pressi, H., Gorman, S., Posner, M., Sasayma, R., &amp; Schmitt (with contributions from R. Crooks, M. Driscoll, A. Earhart, S. Keralis, T. Naiman, &amp; T. Presner). (2018, June 8). A student collaborators’ bill of rights. https://humtech.ucla.edu/news/a-student-collaborators-bill-of-rights/ Dryden, A. (2014, April 29). Programming diversity. Talk at Mix-IT, Lyon, France. https://www.ashedryden.com/mixit-programming-diversity Ehmke, C. A. (2017, July 5). Antisocial coding: My year at GitHub. Coraline Ada Ehmke [blog]. https://where.coraline.codes/blog/my-year-at-github/ Federici, S. (1975). Wages against housework. Power of Women Collective and Falling Water Press. Ford, H., &amp; Wajcman, J. (2017). “Anyone can edit”, not everyone does: Wikipedia’s infrastructure and the gender gap. Social Studies of Science, 47(4), 511-527. Gibson-Graham, J. K., &amp; the Community Economies Collective. (2017, February 27). Cultivating Community Economies. Next System Project. https://thenextsystem.org/cultivating-community-economies Hill, B. M., &amp; Shaw, A. (2013). The Wikipedia gender gap revisited: Characterizing survey response bias with propensity score estimation. PLOS One, 8(6), 1-5. Hitlin, P. (2016, July 11). Research in the crowdsourcing age, a case study. Pew Research Center: Internet &amp; Technology. https://www.pewresearch.org/internet/2016/07/11/research-in-the-crowdsourcing-age-a-case-study/ Hochschild, A. R. (2012). The managed heart: Commercialization of human feeling (3rd ed.). University of California Press. Howe, J. (2006, January 6). The rise of crowdsourcing. WIRED. https://www.wired.com/2006/06/crowds/ Martin, J. L. (2010). Life’s a beach but you’re an ant, and other unwelcome news for the sociology of culture. Poetics, 38(2), 229-244. Miller, C. C. (2014, April 21). Github founder resigns after investigation. New York Times. https://archive.nytimes.com/bits.blogs.nytimes.com/2014/04/21/github-founder-resigns-after-investigation/ Miranda, V. (2011). Cooking, caring and volunteering: Unpaid work around the world. OECD Social, Employment and Migration Papers, 116. https://www.oecd.org/berlin/47258230.pdf Posner, M. (2018, April). See no evil. Logic Magazine. https://logicmag.io/scale/see-no-evil/ Ross, J., Irani, L., Silberman, M. S., Zaldivar, A., &amp; Tomlinson, B. Who are the crowdworkers? Shifting demographics in Mechanical Turk. In CHI 2010: Imagine All the People (pp. 2863-2872). Schmidt, B. M. (2017, May 16). A brief visual history of MARC cataloging at the Library of Congress. Sapping Attention [blog]. http://sappingattention.blogspot.com/2017/05/a-brief-visual-history-of-marc.html Terranova, T. (2000). Free labor: Producing culture for the digital economy. Social Text, 18(2), 33-58. Terrell, J., Kofink, A., Middleton, J., Rainear, C., Murphy-Hill, E., Parnin, C., &amp; Stallings, J. (2017). Gender differences and bias in open source: Pull request acceptance of women versus men. PeerJ Computer Science, 3(e111). Whose Knowledge. (n.d.). #VisibleWikiWomen 2019. https://whoseknowledge.org/initiatives/visiblewikiwomen-2019/ Wikimedia. (n.d.). Editor Survey 2011/Executive Summary. https://meta.wikimedia.org/wiki/Editor_Survey_2011/Executive_Summary "],["m5u-numbers-dont-speak-for-themselves.html", "14 M5U: Numbers Don’t Speak for Themselves 14.1 Introduction 14.2 Bigger is not Always Better 14.3 Theory 14.4 The Importance of Theory 14.5 Research isn’t Just Empirical 14.6 The Importance of Context 14.7 Conclusion 14.8 References", " 14 M5U: Numbers Don’t Speak for Themselves This chapter draws on material from 6. The Numbers Don’t Speak for Themselves by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0. Changes to the source material include light editing, adding new material, deleting original material, rearranging material, changing the citation style, adding links, replacing images, changing original authors’ voice to third person, and adding first-person language from current author. The resulting content is licensed under CC BY 4.0. 14.1 Introduction In April 2014, 276 young women were kidnapped from their high school in the town of Chibok in northern Nigeria. Boko Haram, a militant terrorist group, claimed responsibility for the attacks. The press coverage, both in Nigeria and around the world, was fast and furious. SaharaReporters.com challenged the government’s ability to keep its students safe. CNN covered parents’ anguish. The Japan Times connected the kidnappings to the increasing unrest in Nigeria’s northern states. And the BBC told the story of a girl who had managed to evade the kidnappers. Several weeks after this initial reporting, the popular blog FiveThirtyEight published its own data-driven story about the event, titled “Kidnapping of Girls in Nigeria Is Part of a Worsening Problem” (Chalabi, 2014). The story reported skyrocketing rates of kidnappings. It asserted that in 2013 alone there had been more than 3,608 kidnappings of young women. Charts and maps accompanied the story to visually make the case that abduction was at an all-time high—here’s a link to one prominent one. Shortly thereafter, the news website had to issue an apologetic retraction because its numbers were just plain wrong. The outlet had used the Global Database of Events, Language and Tone (GDELT) as its data source. GDELT is a big data project led by computational social scientist Kalev Leetaru. It collects news reports about events around the world and parses the news reports for actors, events, and geography with the aim of providing a comprehensive set of data for researchers, governments, and civil society. GDELT tries to focus on conflict—for example, whether conflict is likely between two countries or whether unrest is sparking a civil war—by analyzing media reports. However, as political scientist Erin Simpson pointed out to FiveThirtyEight in a widely cited Twitter thread (archived here, GDELT’s primary data source is media reports. The project is not at a stage at which its data can be used to make reliable claims about independent cases of kidnapping. The kidnapping of schoolgirls in Nigeria was a single event. There were thousands of global media stories about it. Although GDELT de-duplicated some of those stories to a single event, it still logged, erroneously, that hundreds of kidnapping events had happened that day. The FiveThirtyEight report had counted each of those GDELT pseudoevents as a separate kidnapping incident. The error was embarrassing for FiveThirtyEight, not to mention for the reporter, but it also helps to illustrate some of the larger problems related to data found “in the wild.” First, the hype around “big data” leads to projects like GDELT wildly overstating the completeness and accuracy of its data and algorithms. On the website and in publications, the project leads have stated that GDELT is “an initiative to construct a catalog of human societal-scale behavior and beliefs across all countries of the world, connecting every person, organization, location, count, theme, news source, and event across the planet into a single massive network that captures what’s happening around the world, what its context is and who’s involved, and how the world is feeling about it, every single day” (Leetaru, n.d.). 14.2 Bigger is not Always Better This boasting by GDELT is typical of projects that ignore context, fetishize size, and inflate their technical and scientific capabilities. In doing so, they tap into patriarchial, cis-masculinist, totalizing fantasies of world domination as enacted through data capture and analysis. In fact, D’Ignazio and Klein (the original authors of this material) make a pointed comparison between masculinity and data science in terms of obsession with… size… that I have edited out here. You can read their comparison at the link in the attributions above—or review Gieseking (2018), which uses similar wordplay to make an important critique of big data. In GDELT’s case, the question is whether we should take its claims of big data at face value or whether the emphasis on size is trying to trick funding organizations into giving the project massive amounts of research funding. (This trick has worked many times before.) The GDELT technical documentation does not provide any more clarity as to whether it is counting media reports (as Simpson asserts) or single events. The database FiveThirtyEight used is called the GDELT Event Database, which certainly makes it sound like it’s counting events. The GDELT documentation states that “if an event has been seen before it will not be included again,” which also makes it sound like it’s counting events. And a 2013 research paper related to the project confirms that GDELT is indeed counting events, but only events that are unique to specific publications. So it’s counting events, but with an asterisk. Compounding the matter, the documentation offers no guidance as to what kinds of research questions are appropriate to ask the database or what the limitations might be. People like Simpson who are familiar with the area of research known as event detection may know to not believe (1) the title of the database, (2) the documentation, and (3) the marketing hype. But how would outsiders, let alone newcomers to the platform, ever know that? Here, we’re focusing on GDELT, but the truth is that it’s not very different from any number of other data repositories out there on the web. There are a proliferating number of portals, observatories, and websites that make it possible to download all manner of government, corporate, and scientific data. Consider application programming interfaces (APIs): programs that serve as go-betweens allowing individual users to access data stored by digital platforms. There are APIs that make it possible to write little programs to query massive datasets (like, for instance, all of Twitter) and download them in a structured way. There are test datasets for network analysis, machine learning, social media, and image recognition. There are fun datasets, curious datasets, and newsletters that inform readers of datasets to explore for journalism or analysis: You may enjoy exploring datasets about dogs in Zurich, UFO sightings, or a list of abandoned shopping carts in Bristol rivers. My favorite is perhaps this dataset of all the murals in Brussels inspired by the proud Franco-Belgian comics tradition; in fact, this is such a perfect combination of my personal interests in French, comics, and data that I feel obligated to specify that the original authors of this material included this reference—I didn’t add it after the fact. In our current moment, we tend to think of this unfettered access to information as an inherent good. And in many ways, it is kind of amazing that one can just google and download data on, for instance, pigeon racing, the length of guinea pig teeth, or every single person accused of witchcraft in Scotland between 1562 and 1736. Personally, I’ve benefited tremendously from this! I’m not exaggerating when I say I owe my career in great part to the ability to download truckloads and truckloads of tweets. (Though under current Twitter leadership, this ability has disappeared for me and many other researchers.) However, it’s also easy to let this convenient access to data distract us from other important commitments in data science. 14.3 Theory The word theory gets tossed around a lot when discussing science and scientific research, but sometimes people attribute different meanings or values to it. You may have heard someone disparage some scientific conclusion as “just a theory,” suggesting that the word has a flavor of “less than fact.” That’s not entirely wrong, but it’s easy to see how the word theory can—and has—been abused by over-emphasizing that flavor. In response, some scientists and science educators will emphasize that a theory is actually the result of tremendous amounts of scientific inquiry, and theories related to evolution and climate change are much more trustworthy than their detractors would have you believe. This is an appropriate and accurate response, but all of this talk about the truth values associated with theory (as compared to fact) actually sidesteps an important component of what theory actually is—and that component is key to our understanding and use of theory as it pertains to data science. Let’s take a look at a definition from one of my old textbooks to bring our attention to the elements of theory that we care about. Maxwell (2013) suggests that we can understand theory as: a conception or model of what is out there that you plan to study, and of what is going on with these things and why (p. 39) That is, if a phenomenon is a thing we want to study (likely in relationship to other phenomena—that is, other things we want to study), a theory is a description of the relationship between all of those different things. For example, one of my preferred theoretical perspectives is that of literacies (as those of you who have taken my LIS 618 class will remember). In short, this theoretical perspective suggests that people in different contexts use different technologies (or the same technologies in different ways) to express and understand meaning. Thus, when I began my dissertation research and started working with data from 60ish different Twitter hashtags used by teachers in different U.S. states and Canadian provinces or territories, my literacies perspective suggested that there ought to be differences in the activity in those hashtags based on contextual differences. By trying to describe the differences between hashtags, I could get a glimpse at how teachers in different contexts valued different features of Twitter. This example is a good start, but you might notice that this study was pretty descriptive and exploratory—it wasn’t interested in determining a clear cause-effect relationship. In positivist sciences, like data science, a theory isn’t just a relationship between phenomena, it’s a causal relationship between phenomena. More X causes more Y. More X causes more Y, but only if Z remains steady. More X causes more Y, and a combination of more Y and more Z causes more A. Remember that positivist sciences are concerned largely with predictions—theories give us the means by which to make and test predictions. In the context of research, Maxwell (2013) writes that The function of this theory is to inform the rest of your design—to help you to assess and refine your goals, develop realistic and relevant research questions, select appropriate methods, and identify potential validity threats to your conclusions. It also helps you justify your research… (pp. 39-40) In short, if a given theory suggests that more X causes more Y, then as a researcher, your job is to figure out how to measure X, how to measure Y, and what statistical tests can help you determine if more X does indeed cause more Y in the context that you’re doing research in. Your results will then add to our collective understanding of that theory. After all, it is true that a theory isn’t quite the same as fact—especially in the social sciences, theories are always being poked, prodded, tested, challenged, and refined. Theories are necessarily abstract and necessarily make particular assumptions—it’s worth asking (always in good faith) if something important has been abstracted out, or if a given assumption actually holds true. That said, if there’s been a lot of research about a particular theory, then professionals may find it trustworthy enough to make practical decisions based on that theory. If researchers have compellingly demonstrated that more X causes more Y, and if more Y is desired in a particular workplace, then the obvious solution is more X. 14.4 The Importance of Theory To oversimplify things a bit (okay, a lot), the traditional (positivist) research process involves two main phases. In the first phase, researchers collect some data, explore that data, and then use that data to generate a theory. In the second phase, other researchers ask a question, identify a theory that ought to predict the answer to that question, collect data to test that prediction, and publish their results, offering new insights into the theory. In the traditional research process, data is assumed to be difficult to come by, so—as we read earlier—researchers focus their efforts on collecting only that data that the theory directs them to. However, as we’ve discussed in previous weeks, we live in a world in which data is not sparse. In a 2008 Wired article, “The End of Theory,” editor Chris Anderson made the now-infamous claim that “the numbers speak for themselves” (Anderson, 2008). His main assertion was that the advent of big data would soon allow data scientists to conduct analyses at the scale of the entire human population, without needing to restrict their analysis to a smaller sample. To understand his claim, you need to understand one of the basic premises of statistics, which we’ll revisit (though not uncritically) later in the semester. Statistical inference is based on the idea of sampling: that you can infer things about a large-scale phenomenon by studying a random and/or representative sample of people or things making up that phenomenon and then mapping those findings back on the phenomenon as a whole. Say that you want to know who all of the voters in the US will vote for in a coming presidential election. You couldn’t contact all of them, of course, but you could call three thousand of them on the phone and then use those results to predict how the rest of the people would likely vote. There would also need to be some statistical modeling and theory involved, because how do you know that those three thousand people are an accurate representation of the whole population? This is where Anderson made his intervention: at the point at which we have data collected on the entire population, we no longer need modeling or any other “theory” to first test and then prove. We can look directly at the data themselves. Now, you can’t write an article claiming that the basic structure of scientific inquiry is obsolete and not expect some pushback. Anderson wrote the piece to be provocative, and sure enough, it prompted numerous responses and debates, including those that challenge the idea that this argument is a “new” way of thinking in the first place (for example, in the early seventeenth century, Francis Bacon argued for a form of inductive reasoning, in which the scientist gathers data, analyzes them, and only thereafter forms a hypothesis; see Mazzocchi, 2015). Lack of novelty was not the only critique, though. One of Anderson’s major examples is Google Search. Google’s search algorithms don’t need to have a hypothesis about why some websites have more incoming links—other pages that link to the site—than others; they just need a way to determine the number of links so they can use that number to determine the popularity and relevance of the site in search results. We no longer need causation, Anderson (2008) insists: “Correlation is enough.” But what happens when the number of links is also highly correlated with sexist, racist, and pornographic results? The influence of racism, sexism, and colonialism is precisely what we see described in Algorithms of Oppression, information studies scholar Safiya Umoja Noble’s study of the harmful stereotypes about Black and Latinx women perpetuated by search algorithms such as Google’s. Noble demonstrates that Google Search results do not simply correlate with our racist, sexist, and colonialist society; that society causes the racist and sexist results. More than that, Google Search reinforces these oppressive views by ranking results according to how many other sites link to them. The rank order, in turn, encourages users to continue to click on those same sites. Here, correlation without context is clearly not enough because it recirculates racism and sexism and perpetuates inequality. In fact, not only does correlation fail to capture the whole picture, but we can actually use theory to account for the racism, sexism, and colonialism that are associated with Google results! A theoretical perspective that sees technological platforms (like Google) as having specific values (rather than being neutral tools; see van Dijck, 2013) gives us the conceptual tools we need to ask ourselves what the Google software values (say, popularity in the form of hyperlinks) and what it doesn’t actively value (say, anti-racism and feminism). Taking the data at face value—like Anderson would have us do—misses a huge part of what’s going on here. 14.5 Research isn’t Just Empirical Let’s continue along these lines. One of the things that sets scientific research apart is its empiricism. That is, research is empirical—it’s based on data. However, it’s easy to forget that research isn’t just empirical. Scientific research represents a particular branch of philosophy, so to be done properly, we need to have our philosophical caps on, not just our data ones. Berry (2011) argues that the sources of big data we now have access to: provide destabilising amounts of knowledge and information that lack the regulating force of philosophy (p. 8) When we use theory as a guiding force—instead of just relying on data—this helps us consider research from a philosophical point of view. It forces us to state our assumptions out loud and gives us additional means of evaluating research. To give a specific example, let’s consider a Sheriff’s Office in the Tampa, Florida area that received criticism for collecting student information from schools, matching it up with other data, and using data science techniques to predict which students “could ‘fall into a life of crime’” (Bedi &amp; McGrory, 2020). I admittedly don’t know all the details about what this Sheriff’s Office is doing, and I have a number of objections to this project based on what I do know, but let’s use this story as a starting point for a hypothetical situation. You’ll notice that some of the details I include in this hypothetical example are similar to those I raised when discussing a hypothetical example of an algorithm that makes sentencing recommendations for criminals. This isn’t to be redundant but rather to emphasize how widespread these issues are. More data is generally more helpful for making predictions, so if I had the job of predicting future criminals (and I didn’t immediately resign from the job), I would use all the school, county, and other data I could find to start making these predictions. One of the first things I’d do is look at current criminals, collect all the data I could about them, and use that data to determine what to look for in students that might serve as a red flag. Now, let’s imagine that people of color are disproportionately represented among the criminal population of the county I’m in. This could be due to a number of factors, including a relationship between race and socioeconomic status (that is, race predicts socioeconomic status because of centuries of white people enriching themselves at the expense of people of color, and socioeconomic status predicts criminal activity). Alternatively, the local police could more likely to arrest a person of color for something that they would give a white person a pass for. In short, it would be irresponsible and disgustingly racist to argue that there is something inherent to race that makes a person more likely to be a criminal. Let’s say that these additional factors aren’t accounted for in my data but that my data do say that there are proportionally more people of color in the criminal records that I have access to. As a result, in my number crunching, it’s possible that I come up with a statistically-defensible argument that race predicts criminal activity. If I’m committed to theory, I’m going to have to explicitly argue to my supervisors that I’m operating off of a theoretical perspective that race causes criminal activity. At that point, I would hope that my supervisors would recognize that theory as inexcusably racist, dismiss that as a valid way of predicting future criminal activity, and send me back to the drawing board. However, if I have Anderson-like faith that the data will tell me everything I need to know, that intermediary step of evaluating and justifying the theory may not be necessary. There’s no philosophical oversight of the causal and other relationships that I’m proposing, and it’s easy for terrible decisions to be rubber stamped in the name of objective data. In fact, things get worse: With advanced data science techniques like machine learning, it’s possible to develop a tool that makes predictions but is not transparent about the criteria that it uses to make those predictions and therefore does not allow its “theory” to be evaluated. It’s entirely possible that these tools are based on racist “theory” that is not (or cannot be!) properly evaluated. This ought to worry us. 14.6 The Importance of Context We should also consider the ways that the importance of context pushes back on Anderson’s argument. One of the central tenets of feminist thinking is that all knowledge is situated. A less academic way to put this is that context matters. When approaching any new source of knowledge, whether it be a dataset or dinner menu (or a dataset of dinner menus; see Muñoz &amp; Rawson, 2019), it’s essential to ask questions about the social, cultural, historical, institutional, and material conditions under which that knowledge was produced, as well as about the identities of the people who created it. Rather than seeing knowledge artifacts, like datasets, as raw input that can be simply fed into a statistical analysis or data visualization, a feminist approach insists on connecting data back to the context in which they were produced. This context allows us, as data scientists, to better understand any functional limitations of the data and any associated ethical obligations, as well as how the power and privilege that contributed to their making may be obscuring the truth. The major issue with much of the data that can be downloaded from web portals or through APIs is that they come without context or metadata. If you are lucky you might get a paragraph about where the data are from or a data dictionary that describes what each column in a particular spreadsheet means, but more often than not, you don’t even get that. Let’s consider an example of data about government procurement. The data not look very technically complicated, but you still have to figure out how the business process behind them works. How does the government run the bidding process? How does it decide who gets awarded a contract? Are all the bids published here, or just the ones that were awarded contracts? What do terms like competition, cooperation agreement, and terms of collaboration mean to the data publisher? Without answers to even some of these questions—to say nothing of the local knowledge required to understand how power is operating in this particular ecosystem—it would be difficult to even begin a data exploration or analysis project, even if the data are easily accessible and seem straightforward. This scenario is not uncommon. Most data arrive on our computational doorstep context-free. And this lack of context becomes even more of a liability when accompanied by the kind of marketing hype we see in GDELT and similar projects. In fact, the 1980s version of these claims is what led Donna Haraway to propose the concept of situated knowledge in the first place (see Haraway, 1988). Subsequent feminist work has drawn on the concept of situated knowledge to elaborate ideas about ethics and responsibility in relation to knowledge-making. Along this line of thinking, it becomes the responsibility of the person evaluating that knowledge, or building upon it, to ensure that its “situatedness” is taken into account. For example, information studies scholar Christine Borgman advocates for understanding data in relation to the “knowledge infrastructure” from which they originate. As Borgman (2015) defines it, a knowledge infrastructure is “an ecology of people, practices, technologies, institutions, material objects, and relationships.” In short, it is the context that makes the data possible. There’s another reason that context is necessary for making sense of correlation, and it has to do with how racism, sexism, and other forces of oppression enter into the environments in which data are collected. The next example has to do with sexual assault and violence. If you do not want to read about these topics, you may want to wrap up your reading here. In April 1986, Jeanne Clery, a student at Lehigh University, was sexually assaulted and murdered in her dorm room. Her parents later found out that there had been thirty- eight violent crimes at Lehigh in the prior three years, but nobody had viewed that as important data that should be made available to parents or to the public. The Clerys mounted a campaign to improve data collection and communication efforts related to crimes on college campuses, and it was successful: the Jeanne Clery Act was passed in 1990, requiring all US colleges and universities to make on-campus crime statistics available to the public. So we have an ostensibly comprehensive national dataset about an important public topic. In 2016, three students in Catherine’s D’Ignazio data journalism class at Emerson College —Patrick Torphy, Michaela Halnon, and Jillian Meehan—downloaded the Clery Act data and began to explore it, hoping to better understand the rape culture that has become pervasive on college campuses across the United States. The term rape culture was coined by second-wave feminists in the 1970s to denote a society in which male sexual violence is normalized and pervasive, victims are blamed, and the media exacerbates the problem. Rape culture includes jokes, music, advertising, laws, words, and images that normalize sexual violence. In 2017, following the election of a US president who joked about sexual assault on the campaign trail and the exposé of Harvey Weinstein’s predatory behavior in Hollywood, high-profile women began speaking out against rape culture with the #MeToo hashtag. #MeToo, a movement started over a decade ago by activist Tarana Burke, encourages survivors to break their silence and build solidarity to end sexual violence. These students soon became puzzled, however. Williams College, a small, wealthy liberal arts college in rural Massachusetts, seemed to have an epidemic of sexual assault, whereas Boston University (BU), a large research institution in the center of the city, seemed to have strikingly few cases relative to its size and population (not to mention that several high-profile sexual assault cases at BU had made the news in recent years). The students were suspicious of these numbers, and investigated further. After comparing the Clery Act data with anonymous campus climate surveys, consulting with experts, and interviewing survivors, they discovered, paradoxically, that the truth was closer to the reverse of the picture that the Clery Act data suggest. The students were suspicious of these numbers, and investigated further. After comparing the Clery Act data with anonymous campus climate surveys, consulting with experts, and interviewing survivors, they discovered, paradoxically, that the truth was closer to the reverse of the picture that the Clery Act data suggest. For example, many of the colleges with higher reported rates of sexual assault were actually places where more institutional resources were being devoted to support for survivors. This is important context, and clearly adds considerations that the original data did not cover. Note, though that we still need “the regulating force of philosophy” (Berry, 2011, p. 8) to help us puzzle this out. Even with additional context, this data could be interpreted in a number of ways, and philosophy and theory are key for helping us figure out what’s going on here. For example, a bad faith actor might argue that “more support for survivors leads to more sexual assault” and conclude that universities should actually provide less support for survivors. This theory strikes me as unlikely (and irresponsible)—it doesn’t stand up to philosophical inquiry. Rather, it seems much more likely to me that providing more support for survivors creates an environment where people are more comfortable reporting sexual assault. In short, in a faint echo of the GDELT controversy, we have to understand reports of sexual assault as just that—not as actual instances of sexual assault. Context, theory, and data are all critical parts of coming to the right conclusion. Indeed, the context and theory that we’ve considered also helps explain the colleges with lower numbers. The Clery Act requires colleges and universities to provide annual reports of sexual assault and other campus crimes, and there are stiff financial penalties for not reporting. But the numbers are self-reported, and there are also strong financial incentives for colleges not to report. No college wants to tell the government—let alone parents of prospective students—that it has a high rate of sexual assault on campus. This is compounded by the fact that survivors of sexual assault often do not want to come forward—because of social stigma, the trauma of reliving their experience, or the resulting lack of social and psychological support. Mainstream culture has taught survivors that their experiences will not be treated with care and that they may in fact face more harm, blame, and trauma if they do come forward. In fact, my undergraduate alma mater—a conservative, religious university—has sometimes interpreted reports of sexual assault as admissions of sexual relations outside of marriage and disciplined survivors instead of supporting them. Is it any surprise that some students might hesitate to report sexual assault? There are further power differentials reflected in the data when race and sexuality are taken into account. For example, in 2014, twenty-three students filed a complaint against Columbia University, alleging that Columbia was systematically mishandling cases of rape and sexual violence reported by LGBTQ students. Zoe Ridolfi-Starr, the lead student named in the complaint, told the Daily Beast, “We see complete lack of knowledge about the specific dynamics of sexual violence in the queer community, even from people who really should be trained in those issues” (Golden, 2014). 14.7 Conclusion Data alone clearly do not show the whole picture, and even context without theory can be misleading. We need all three, and we must not fall into the trap of believing that numbers don’t speak for themselves. 14.8 References Anderson, C. (2008). The end of theory: The data deluge makes the scientific method obsolete. Wired. https://www.wired.com/2008/06/pb-theory/ Bedi, N., &amp; McGrory, K. (2020, November 19). Pasco’s sheriff uses grades and abuse histories to label schoolchildren potential criminals. The kids and their parents don’t know. Tampa Bay Times. https://projects.tampabay.com/projects/2020/investigations/police-pasco-sheriff-targeted/school-data/ Berry, D. M. (2011). The computational turn: Thinking about the digital humanities. Culture Machine, 12(. Borgman, C. L. (2015). Big data, little data, no data: Scholarship in the networked world. MIT Press. Chalabi, M. (2014, May 8). Kidnapping of girls in Nigeria is part of a worsening problem. FiveThirtyEight. https://fivethirtyeight.com/features/nigeria-kidnapping/ Gieseking, J. J. (2018). Size matters to lesbians, too: Queer feminist interventions into the scale of big data. Professional Geographer, 70((1), 150-156. Golden, A. (2014, April 30). Is Columbia University mishandling LGBT rape cases? Daily Beast. https://www.thedailybeast.com/is-columbia-university-mishandling-lgbt-rape-cases?ref=scroll Haraway, D. (1988). Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist Studies, 14(3), 575-599. Leetaru, K. (n.d.). The GDELT Project. GDELT. https://www.gdeltproject.org/ Maxwell, J. A. (2013). Qualitative research design: An interactive approach (3rd edition). SAGE Publications, Inc. Mazzocchi, F. (2015). Could big data be the end of theory in science? EMBO Reports, 16(10), 1250-1255. Muñoz, T., &amp; Rawson, K. (2016). Data dictionary. Curating Menus. http://curatingmenus.org/data_dictionary/ van Dijck, J. (2013). The culture of connectivity: A critical history of social media. Oxford University Press. "],["m5a-are-ethics-enough-in-data-science.html", "15 M5A: Are Ethics Enough in Data Science? 15.1 Introduction 15.2 Ethics vs. Politics 15.3 Why Politics? 15.4 Conclusion 15.5 References", " 15 M5A: Are Ethics Enough in Data Science? This chapter draws on material from: Data Science as Political Action: Grounding Data Science in a Politics of Justice by Ben Green, licensed under CC BY 4.0. 2. Collect, Analyze, Imagine, Teach by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0. Changes to the source material include removal of original material, reformatting original material, addition of new material, combining of sources, and editing of original material for a different audience. The resulting content is licensed under CC BY 4.0 15.1 Introduction The field of data science has entered a period of reflection and reevaluation. Alongside its rapid growth in both size and stature in recent years, data science has become beset by controversies and scrutiny. Machine learning algorithms that guide decisions in areas such as hiring, healthcare, criminal sentencing, and welfare are often biased, inscrutable, and proprietary (Anwin et al., 2016; Buolamwini &amp; Gebru, 2018; Eubanks, 2018; Obermeyer et al., 2019; O’Neil, 2017; Wexler, 2018). Algorithms that drive social media feeds manipulate people’s emotions (Kramer et al., 2014), spread misinformation (Vosoughi et al., 2018), and amplify political extremism (Nicas, 2018). Facilitating these and other algorithms are massive datasets, often gained illicitly or without meaningful consent, that reveal sensitive and intimate information about people (de Montjoye et al., 2015; Kosinski et al., 2013; Rosenberg et al., 2018; Thompson &amp; Warzel, 2019) Many individuals and organizations responded to these controversies by advocating for a focus on ethics in training and practice (Green, 2021). Data ethics represents a growing interdisciplinary effort—both critical and computational—to ensure that the ethical issues brought about by our increasing reliance on data-driven systems are identified and addressed. Thus far, the major trend has been to emphasize the issue of “bias,” and the values of “fairness, accountability, and transparency” in mitigating its effects. The broad motivation behind these efforts is the assumption that, if only data scientists were more attuned to the ethical implications of their work, many harms associated with data science could be avoided. This is a promising development, especially for technical fields that have not historically foregrounded ethical issues, and as funding mechanisms for research on data and ethics proliferate. However, addressing bias in a dataset is a tiny technological Band-Aid for a much larger problem. Even the values of “fairness, accountability, and transparency,” which seek to address instances of bias in data-driven systems, are themselves non-neutral, as they locate the source of the bias in individual people and specific design decisions. In short, these concepts may do good work, but they ultimately keep the roots of the problem in place. In other words, they maintain the current structure of power, even if they don’t intend to, because they let structural issues off the hook They direct data scientists’ attention toward seeking technological fixes instead of social and political solutions. Sometimes those fixes are necessary and important. But as technology scholars Julia Powles and Helen Nissenbaum (2018) assert, “Bias is real, but it’s also a captivating diversion.” There is a more fundamental problem that must also be addressed: we do not all arrive in the present with equal power or privilege. Hundreds of years of history and politics and culture have brought us to the present moment. This is a reality of our lives as well as our data. 15.2 Ethics vs. Politics In practice, technology ethics suffer from four significant limitations (Green, 2021): First, technology ethics principles are abstract and lack mechanisms to ensure that engineers follow ethical principles. Second, technology ethics has a myopic focus on individual engineers and on technology design, overlooking the structural sources of technological harms. Third, technology ethics is subsumed into corporate logics and practices rather than substantively altering behavior. All told, the rise of technology ethics often reflects a practice dubbed “ethics-washing”: tech companies deploying the language of ethics to resist more structural reforms that would curb their power and profits. Thus, while ethics provides useful frameworks to help data scientists reflect on their practice and the impacts of their work, these approaches are insufficient for generating a data science that avoids social harms and that promotes social justice. The normative responsibilities of data scientists cannot be managed through a narrow professional ethics that lacks normative weight and supposes that, with some reflection and a commitment to best practices, data scientists will make the “right” decisions that lead to “good” technology. Instead of relying on vague moral principles that obscure the structural drivers of injustice, data scientists must engage in politics: the process of negotiating between competing perspectives, values, and goals. In other words, we must recognize data science as a form of political action. Data scientists must recognize themselves as political actors engaged in normative constructions of society. In turn, data scientists must evaluate their efforts according to the downstream impacts on people’s lives. In this context, politics and political do not refer to partisan or electoral debates about specific parties and candidates. Instead, these terms have a broader meaning that transcends activity directly pertaining to the government, its laws, and its representatives. Two aspects of politics are particularly important: First, politics is everywhere in the social world. As defined by politics professor Adrian Leftwich (1984), “politics is at the heart of all collective social activity, formal and informal, public and private, in all human groups, institutions, and societies”. Second, politics has a broad reach. Political scientist Harold Lasswell (1936) describes politics as “who gets what, when, how.” The “what” here could mean many things: money, goods, status, influence, respect, rights, and so on. Understood in these terms, politics comprises any activities that affect or make claims about the who, what, when, and how in social groups, both small and large. Data scientists are political actors in that they play an increasingly powerful role in determining the distribution of rights, status, and goods across many social contexts. As data scientists develop tools that inform important social and political decisions—who receives a job offer, what news people see, where police patrols—they shape social outcomes around the world. Data scientists are some of today’s most powerful (and obscured) political actors, structuring how institutions conceive of problems and make decisions. 15.3 Why Politics? This section responds to three arguments that are commonly invoked by data scientists when they are challenged to take political stances regarding their work. These arguments have been expressed in a variety of public and private settings and will be familiar to anyone who has engaged in discussions about the social responsibilities of data scientists. These are by no means the only arguments proffered in this larger debate, nor do they represent any sort of unified position among data scientists. Nonetheless, the three positions considered here are among the most common and compelling arguments made against a politically oriented data science. Any promotion of a more politically engaged data science must contend with them. 15.3.1 Argument 1: “I am Just an Engineer” This first argument represents a common attitude among engineers. In this view, although engineers develop new tools, their work does not determine how a tool will be used. Artifacts are seen as neutral objects that lack any inherent normative character and that can simply be used in good or bad ways. By this logic, engineers bear no responsibility for the impacts of their creations. It is common for data scientists to argue that the impacts of technology are unknowable. However, by articulating their limited role as neutral researchers, data scientists provide themselves with an excuse to abdicate responsibility for the social and political impacts of their work. When a paper that used neural networks to classify crimes as gang-related was challenged for its potentially harmful effects on minority communities, a senior author on the paper deflected responsibility by arguing, “It’s basic research” (Hutson, 2018). Although it is common for engineers to see themselves as separate from politics, many scholars have thoroughly articulated how technology embeds politics and shapes social outcomes. As political theorist Langdon Winner (1986) describes: “technological innovations are similar to legislative acts or political foundings that establish a framework for public order that will endure over many generations. For that reason, the same careful attention one would give to the rules, roles, and relationships of politics must also be given to such things as the building of highways, the creation of television networks, and the tailoring of seemingly insignificant features on new machines. The issues that divide or unite people in society are settled not only in the institutions and practices of politics proper, but also, and less obviously, in tangible arrangements of steel and concrete, wires and semiconductors, and nuts and bolts.” Even though technology does not conform to conventional notions of politics, it often shapes society in much the same way as laws, elections, and judicial opinions. In this sense, “the scientific workplace functions as a key site for the production of social and political order” (Jasanoff, 2003). Thus, as with many other types of scientists, data scientists possess “a source of fresh power that escapes the routine and easy definition of a stated political power” (Latour, 1983). There are many examples of engineers developing and deploying technologies that, by structuring behavior and shifting power, shape aspects of society. As one example, when automobiles were introduced onto city streets in the 1920s, they created chaos and conflict in the existing social order (Norton, 2011). Many cities turned to traffic engineers as “disinterested experts” whose scientific methods could provide a neutral and optimal solution. But the engineers’ solution contained unexamined assumptions and values, namely, that “traffic efficiency worked for the benefit of all”. As traffic engineers changed the timings of traffic signals to enable cars to flow freely, their so-called solution “helped to redefine streets as motor thoroughfares where pedestrians did not belong”. These actions by traffic engineers helped shape the next several decades of automobile-focused urban development in US cities. Although these particular outcomes could be chalked up to unthoughtful design, any decisions that the traffic engineers made would have had some such impact: determining how to time streetlights requires judgments about what outcomes and whose interests to prioritize. Whatever they and the public may have believed, traffic engineers were never “just” engineers optimizing society “for the benefit of all”. Instead, they were engaged in the process—via formulas and signal timings—of defining which street uses should be supported and which should be constrained. The traffic engineers may not have decreed by law that streets were for cars, but their technological intervention assured this outcome by other means. Data scientists today risk repeating this pattern of designing tools with inherently political characters yet largely overlooking their own agency and responsibility. By imagining an artificially limited role for themselves, engineers create an environment of scientific development that requires few moral or political responsibilities. But this conception of engineering has always been a mirage. Developing any technology contributes to the particular “social contract implied by building that technological system in a particular form” (Winner, 1986). Of course, we must also resist placing too much responsibility on data scientists. The point is not that, if only they recognized their social impacts, engineers could themselves solve social issues. Technology is at best just one tool among many for addressing complex social problems (Green, 2019). Nor should we uncritically accept the social influence that data scientists have. Having unelected and unaccountable technical experts make core decisions about governance away from the public eye imperils essential notions of how a democratic society ought to function. As Science, Technology, and Society (STS) scholar Sheila Jasanoff (2006) argues, “The very meaning of democracy increasingly hinges on negotiating the limits of the expert’s power in relation to that of the publics served by technology.” Nonetheless, the design and implementation of technology does rely, at some level, on trained practitioners. This raises several questions that animate the rest of this article. What responsibilities should data scientists bear? How must data scientists reconceptualize their scientific and societal roles in light of these responsibilities? 15.3.2 Argument 2: “We Shouldn’t Take Political Stances” Data scientists adhering to this second argument likely accept the response to Argument 1 but feel stuck, unsure how to appropriately act as more than “just” an engineer. “Sure, I am developing tools that impact people’s lives”, they may acknowledge, before asking, “But is not the best thing to just be as neutral as possible?” Although it is understandable how data scientists come to this position, their desire for neutrality suffers from two important failings. First, neutrality is an unachievable goal, as it is impossible to engage in science or politics without being influenced by one’s background, values, and interests. Second, striving to be neutral is not itself a politically neutral position. Instead, it is a fundamentally conservative one—not in a partisan sense, but in the sense that it is committed to maintaining the status quo. An ethos of objectivity has long been prevalent among scientists. Since the nineteenth century, objectivity has evolved into a set of widespread ethical and normative scientific practices. Conducting good science—and being a good scientist—meant suppressing one’s own perspective so that it would not contaminate the interpretations of observations (Daston &amp; Galison, 2007). Yet this conception of science was always rife with contradictions and oversights. Knowledge is shaped and bounded by the social contexts that generated it. This insight forms the backbone of standpoint theory, which articulates that “nothing in science can be protected from cultural influence—not its methods, its research technologies, its conceptions of nature’s fundamental ordering principles, its other concepts, metaphors, models, narrative structures, or even formal languages” (Harding, 1998). Although scientific standards of objectivity account for certain kinds of individual subjectivity, they are too narrowly construed: “methods for maximizing objectivism have no way of detecting values, interests, discursive resources, and ways of organizing the production of knowledge that first constitute scientific problems, and then select central concepts, hypotheses to be tested, and research designs” (Harding, 1998). Every aspect of science is imbued with the characteristics and interests of those who produce it. This does not invalidate every scientific finding as arbitrary, but it points to science’s contingency and reliance on its practitioners. All research and engineering are developed within particular institutions and cultures and with particular problems and purposes in mind. Just as it is impossible to conduct science in any truly neutral way, there is no such thing as a neutral (or apolitical) approach to politics. As philosopher Roberto Unger (1987) writes, political neutrality is an “illusory and ultimately idolatrous goal” because “no set of practices and institutions can be neutral among conceptions of the good.” Instead of being neutral and apolitical, attempts to be neutral and apolitical embody an implicitly conservative (that is, status quo-preserving) politics. Because neutrality does not mean value-free—it means acquiescence to dominant social and political values, freezing the status quo in place. Neutrality may appear to be apolitical, but that is only because the status quo is taken as a neutral default. Anything that challenges the status quo—which efforts to promote social justice must do by definition—will therefore be seen as political. But efforts for reform are no more political than efforts to resist reform or even the choice simply to not act, both of which preserve existing systems. Although surely not the intent of every scientist and engineer who strives for neutrality, broad cultural conceptions of science as neutral entrench the perspectives of dominant social groups, who are the only ones entitled to legitimate claims of neutrality. For example, many scholars have noted that neutrality is defined by a masculine perspective, making it impossible for women to be seen as objective or for neutral positions to consider female standpoints (Harding, 1998; Lloyd, 1993; Keller, 1985; MacKinnon, 1982). The voices of Black women are particularly subjugated as partisan and anecdotal (Collins, 2000). Because of these perceptions, when people from marginalized groups critique scientific findings, they are cast off as irrational, political, and representing a particular perspective (Haraway, 1988). In contrast, the practices of science and the perspectives of the dominant groups that uphold it are rarely considered to suffer from the same maladies. Data science exists on this political landscape. Whether articulated by their developers or not, machine learning systems already embed political stances. Overlooking this reality merely allows these political judgments to pass without scrutiny, in turn granting data science systems with more credence and legitimacy than they deserve. Predictive policing algorithms offer a particularly pointed example of how striving to remain neutral entrenches and legitimize existing political conditions. The issue is not simply that the training data behind predictive policing algorithms are biased due to a history of overenforcement in minority neighborhoods. In addition, our very definitions of crime and how to address it are the product of racist and classist historical processes. Dating back to the eras of slavery and reconstruction, cultural associations of Black men with criminality have justified extensive police forces with broad powers (Butler, 2017). The War on Drugs, often identified as a significant cause of mass incarceration, emerged out of an explicit agenda by the Nixon administration to target people of color (Alexander, 2012; Baum, 2016). Meanwhile, crimes like wage theft (when employers deny employees wages or benefits to which they are legally entitled) are systemically underenforced by police and do not even register as relevant to conversations about predictive policing—even though wage theft steals more value than other kinds of theft combined (Meixell &amp; Eisenbrey, 2014). Moreover, predictive policing rests on a model of policing that is itself unjust. Predictive policing software could exist only in a society that deploys vast punitive resources to prevent social disorder, following “broken windows” tactics. Policing has always been far from neutral: “the basic nature of the law and the police, since its earliest origins, is to be a tool for managing inequality and maintaining the status quo” (Vitale, 2017). The issues with policing are not flaws of training or methods or “bad apple” officers, but are endemic to policing itself (Butler, 2017; Vitale, 2017). Against this backdrop, choosing to develop predictive policing algorithms is not neutral. Accepting common definitions of crime and how to address it may seem to allow data scientists to remove themselves from politics, but instead upholds historical politics of social hierarchy. Although predictive policing represents a notably salient example of how data science cannot be neutral, the same could be said of all applied data science. Biased data are certainly one piece of the story, but so are existing social and political conditions, definitions and classifications of social problems, and the set of institutions that respond to those problems. None of these factors are neutral and removed from politics. And while data scientists are of course not responsible for creating these aspects of society, they are responsible for choosing how to interact with them. Neutrality in the face of injustice only reinforces that injustice. When engaging with aspects of the world steeped in history and politics, in other words, it is impossible for data scientists to not take political stances. I do not mean to suggest that every data scientist should share a singular political vision—that would be wildly unrealistic. It is precisely because the field (and world) hosts a diversity of normative perspectives that we must surface political debates and recognize the role they play in shaping data science practice. Nor is my argument meant to suggest that articulating one’s political commitments is a simple task. Normative ideals can be complex and conflicting, and one’s own principles can evolve over time. Data scientists need not have precise answers about every political question. However, they must act in light of articulated principles and grapple with the uncertainty that surrounds these ideals. 15.3.3 Argument 3: “Don’t Let the Perfect Be the Enemy of the Good” Following the responses to Arguments 1 and 2, data scientists asserting this third argument likely acknowledge that their creations will unavoidably have social impacts and that neutrality is not possible. Yet still holding out against a thorough political engagement, they fall back on a seemingly pragmatic position: because data science tools can improve society in incremental but important ways, we should support their development rather than argue about what a perfect solution might be. Despite being the most sophisticated of the three arguments, this position suffers from several underdeveloped principles. First, data science lacks robust theories regarding what “perfect” and “good” actually entail. As a result, the field typically adopts a superficial approach to reform that involves making vague (almost tautological) claims about what social conditions are desirable. Second, this argument fails to articulate how to evaluate or navigate the relationship between the perfect and the good. Efforts to promote social good thus tend to take for granted that technology-centric incremental reform is an appropriate strategy for social progress. Yet, considered from a perspective of substantive equality and anti-oppression, many data science efforts to do good are not, in fact, consistently doing good. 15.3.3.1 Data Science Lacks a Thorough Definition of “Good” Across the broad world of data science, from academic institutes to conferences to companies to volunteer organizations, “social good” (or just “good”) has become a popular term. While this is both commendable and exciting, the field has not developed (nor even much debated) any working definitions of the term “social good” to guide its efforts. Instead, the field seems to operate on a “know it when you see it” approach, relying on rough proxies such as crime = bad, poverty = bad, and so on. There is, of course, extensive literature (spanning philosophy, STS, and other fields) that considers what is socially desirable, yet data science efforts to promote “social good” rarely reference this literature. This lack of definition leads to “data science for social good” projects that span a wide range of conflicting political orientations. For example, some work under the “social good” umbrella is explicitly developed to enhance police accountability and promote non-punitive alternatives to incarceration (Bauman et al., 2018; Carton et al., 2016). In contrast, other work under the “social good” label aims to enhance police operations. One such paper aimed to classify gang crimes in Los Angeles (Hutson, 2018; Seo et al., 2018) This project involved taking for granted the legitimacy of the Los Angeles Police Department’s gang data—a notoriously biased type of data (Felton, 2018) from a police department that has a long history of abusing minorities in the name of gang suppression (Vitale, 2017). That such politically disparate and conflicting work could be similarly characterized as “social good” should prompt a reconsideration of the core terms and principles. When the term encompasses everything, it means nothing. The point is not that there exists a single optimal definition of “social good”, nor that every data scientist should agree on one set of principles. Instead, there is a multiplicity of perspectives that must be openly acknowledged to surface debates about what “good” actually entails. Currently, however, the field lacks the language and perspective to sufficiently evaluate and debate differing visions of what is “good”. By framing their notions of “good” in such vague and undefined terms, data scientists get to have their cake and eat it too: They can receive praise and publications based on broad claims about solving social challenges while avoiding substantive engagement with social and political impacts. Most dangerously, data science’s vague framing of social good allows those already in power to present their normative judgments about what is “good” as neutral facts that are difficult to challenge. As discussed earlier, neutrality is an impossible goal and attempts to be neutral tend to reinforce the status quo. If the field does not openly debate definitions of “perfect” and “good”, the assumptions and values of dominant groups will tend to win out. Projects that purport to enhance social good but fail to reflexively engage with the political context are likely to reproduce the exact forms of social oppression that many working towards “social good” seek to dismantle. 15.3.3.2 Pursuing an Incremental “Good” Can Reinforce Oppression Even if data scientists acknowledge that “social good” is often poorly defined, they may still adhere to the argument that “we should not let the perfect be the enemy of the good”. “After all”, they might say, “is not some solution, however imperfect, better than nothing?” As one paper asserts, “we should not delay solutions over concerns of optimal” outcomes (Sylvester &amp; Raff, 2018). At this point the second failure of Argument 3 becomes clear: It tells us nothing about the relationship between the perfect and the good. Although data scientists generally acknowledge that data science cannot provide perfect solutions to social problems, the field typically takes for granted that incremental reforms using data science contribute to the “social good”. On this logic, we should applaud any attempts to alleviate issues such as crime, poverty, and discrimination. Meanwhile, because “the perfect” represents an unrealizable utopia we should not waste time and energy debating the ideal solution. Although efforts to promote “social good” using data science can be productive, pursuing such applications without a rigorous theory of social change can lead to harmful consequences. A reform that seems desirable from a narrow perspective focused on immediate improvements can be undesirable from a broader perspective focused on long-term, structural reforms. Understood in these terms, the dichotomy between the idealized “perfect” and the incremental “good” is a false one: articulating visions of an ideal society is an essential step for developing and evaluating incremental reforms. In order to rigorously conceive of and compare potential incremental reforms, we must first debate and refine our conceptions of the society we want to create; following those ideals, we can then evaluate whether potential incremental reforms push society in the desired direction. Because there is a multiplicity of imagined “perfects”, which in turn suggest an even larger multiplicity of incremental “goods,” reforms must be evaluated based on what type of society they promote in both the short and long term. In other words, rather than treating any incremental reform as desirable, data scientists must recognize that different incremental reforms can push society down drastically different paths. When attempting to achieve reform, an essential task is to evaluate the relationship between incremental changes and long-term agendas for a more just society. As social philosopher André Gorz (1961) proposes, we must distinguish between “reformist reforms” and “non- reformist reforms.” Gorz explains, “A reformist reform is one which subordinates its objectives to the criteria of rationality and practicability of a given system and policy.” In contrast, a non-reformist reform “is conceived not in terms of what is possible within the framework of a given system and administration, but in view of what should be made possible in terms of human needs and demands.” Reformist and non-reformist reforms are both categories of incremental reform, but they are conceived through distinct processes. Reformist reformers start within existing systems, looking for ways to improve them. In contrast, non-reformist reformers start beyond existing systems, looking for ways to achieve emancipatory social conditions. Because of the distinct ways that these two types of reforms are conceived, the pursuit of one versus the other can lead to widely divergent social and political outcomes. The solutions proposed by data scientists are almost entirely reformist reforms. The standard logic of data science—grounded in accuracy and efficiency—tends toward accepting and working within the parameters of existing systems. Data science interventions are therefore typically proposed to improve the performance of a system rather than to substantively alter it. And while these types of reforms have value under certain conditions, such an ethos of reformist reforms is unequipped to identify and pursue the larger changes that are necessary across many institutions. This approach may even serve to entrench and legitimize the status quo. From the standpoint of existing systems, it is impossible to imagine alternative ways of structuring society—when reform is conceived in this way, “only the most narrow parameters of change are possible and allowable” (Lorde, 1984). In this sense, data science’s dominant strategy of pursuing a reformist, incremental good resembles a algorithm pursuing the strategy of making immediate improvements in the local vicinity of the status quo. Although this strategy can be useful for simple problems, it is unreliable in complex search spaces: We may quickly find a local maximum but will never reach a further-afield terrain of far better solutions. Moves that are immediately beneficial can be counterproductive for finding the global optimum. Similarly, although reformist reforms can lead to certain improvements, a strategy limited to reformist reforms cannot guide robust responses to complex political problems. Reforms that appear desirable within the narrow scope of a reformist strategy can be counterproductive for achieving structural reforms. Even though the optimal political solution is rarely achievable (and is often subject to significant debate), it is necessary to fully characterize the space of possible reforms and to evaluate how reliably different approaches can generate more egalitarian outcomes. The point is not that data science is incapable of improving society. However, data science interventions must be evaluated against alternative reforms as just one of many options, rather than compared merely against the status quo as the only possible reform. There should not a default presumption that machine learning provides an appropriate reform for every problem. The point is not that data science is incapable of improving society. However, data science interventions must be evaluated against alternative reforms as just one of many options, rather than compared merely against the status quo as the only possible reform. There should not a default presumption that machine learning provides an appropriate reform for every problem. 15.4 Conclusion In sum, attempts by data scientists to avoid politics overlook technology’s social impacts, privilege the status quo, and narrow the range of possible reforms. The field of data science will be unable to meaningfully advance social justice without accepting itself as political. 15.5 References Alexander, M. (2012). The new Jim Crow: Mass incarceration in the age of colorblindness. The New Press. Anwin, J., Larson, J., Mattu, S., &amp; Kirchner, L. (2016). Machine bias. ProPublica. https://www.propublica.org/article/machine-bias- risk-assessments-in-criminal-sentencing Baum, D. (2016). Legalize it all. Harper’s Magazine. https://harpers.org/archive/2016/04/legalize-it-all/ Bauman, M. J., Boxer, K. S., Lin, T.-Y., Salmon, E., Naveed, H., Haynes, L., Walsh, J., Helsby, J., Yoder, S., Sullivan, R., et al. (2018). Reducing incarceration through prioritized interventions. Proceedings of the 1st ACM SIGCAS conference on computing and sustainable societies. Association for Computing Machinery. Buolamwini, J., &amp; Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. Proceedings of the 1st Conference on Fairness, Accountability and Transparency, 81, 77-91. Butler, P. (2017). Chokehold: Policing Black men. The New Press. Carton, S., Helsby, J., Joseph, K., Mahmud, A., Park, Y., Walsh, J., Cody, C., Patterson, E., Haynes, L., &amp; Ghani, R. (2016). Identifying police officers at risk of adverse events. Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. Association for Computing Machinery. Collins, P. H. (2000). Black feminist thought: Knowledge, consciouness, and the politics of empowerment. Routledge. Daston, L., &amp; Galison, P. (2007). Objectivity. Zone Books. de Montjoye, Y.-A., Radaelli, L., Singh, V. K., &amp; Pentland, A. S. (2015). Unique in the shopping mall: On the reidentifiability of credit card metadata. Science, 347(6221), 536-539. Felton, E. (2018, March 15). Gang databases are a life sentence for Black and Latino communities. Pacific Standard. https://psmag.com/social-justice/gang-databases-life-sentence-for-black-and-latino-communities Gorz, A. (1967). Strategy for labor. Beacon Press. Green, B. (2021). The contestation of tech ethics: A sociotechnical approach to technology ethics in practice. Journal of Social Computing, 2(3), 209-225 https://doi.org/10.23919/JSC.2021.0018. Green, B. (2020). The false promise of risk assessments: Epistemic reform and the limits of fairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency (pp. 594-606). Green, B. (2019). The smart enough city: Putting technology in its place to reclaim our urban future. MIT Press. Eubanks, V. (2018). Automating inequality: How high-tech tools profile, polics, and punish the poor. St. Martin’s Press. Haraway, D. (1988). Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist Studies, 14(3), 575-599. Harding, S. (1998). Is science multicultural? Postcolonialisms, feminisms, and epistemologies. Indiana University Press. Hutson, M. (2018, February 28). Artificial intelligence could identify gang crimes—and ignite an ethical firestorm. Science. https://edtechbooks.org/-uYJu Jasanoff, S. (2003). In a constitutional moment: Science and social order at the millennium. In B. Joerges &amp; H. Nowotny (Eds.), Social studies of science and technology: Looking back, ahead (p. 155-180). Springer. Jasanoff, S. (2006). Technology as a site and object of politics. In R. E. Goodin &amp; C. Tilly (Eds.), The Oxford handbook of contextual political analysis (pp. 745-763). Oxford University Press. Karakatsanis, A. (2019). The punishment bureaucracy: How to think about “criminal justice reform.” The Yale Law Journal Forum, 128, 848-935. Keller, E. F. (1985). Reflections on gender and science. Yale University Press. Kosinski, M., Stillwell, D., &amp; Graepel, T. (2013). Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences of the United States of America, 110(15), 5802-5805. Kramer, A. D. I., Guillory, J. E., &amp; Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788-8790. Lasswell, H. D. (1936). Politics: Who gets what, when, how. Whittlesey House. Latour, B. (1983). Give me a laboratory and I will raise the world. In K. Knorr-Cetina &amp; M. J. Mulkay (Eds.), Science Observed: Perspectives on the social study of science (pp 141-170). Sage. Leftwich, A. (1984). Politics: people, resources, and power. In A. Leftwich (Ed.), What is politics? The activity and its study (pp. 62-84). Basil Blackwell. Lloyd, G. (1993). Maleness, metaphor, and the “crisis” of reason. In L. M. Antony &amp; C. E. Witt (Eds.), A mind of one’s own: Feminist essays on reason and objectivity. Westview Press. Lorde, A. (1984). Sister outsider: Essays &amp; speeches. Crossing Press. MacKinnon, C. A. (1982). Feminism, Marxism, method, and the state: An agenda for theory. Signs: Journal of Women in Culture and Society, 7(3), 515-544. McLeod, A. M. (2013). Confronting criminal law’s violence: The possibilities of unfinished alternatives. Unbound: Harvard Journal of the Legal Left, 8, 109-132. Meixell, B., &amp; Eisenbrey, R. (2014, September 11). An epidemic of wage theft is costing workers hundreds of millions of dollars a year. Economic Policy Institute. https://www.epi.org/publication/epidemic-wage-theft-costing-workers-hundreds/ Nicas, J. (2018, February 7). How YouTube drives people to the internet’s darkest corners. Wall Street Journal. https://www.wsj.com/articles/how-youtube-drives-viewers-to-the-internets-darkest-corners-1518020478 Norton, P. D. (2011). Fighting traffic: The dawn of the motor age in the American city. MIT Press. Obermeyer, Z., Powers, B., Vogeli, C., &amp; Mullainathan, S. (2019). Dissecting racial bias in an algorithm used to manage the health of populations. Science, 366(6464), 447-453. O’Neil, C. (2017) Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway Books. Powles, J. (2018, December 7). The seductive diversion of ‘solving’ bias in artifical intelligence. Medium. https://medium.com/s/story/the-seductive-diversion-of-solving-bias-in-artificial-intelligence-890df5e5ef53 Rosenberg, M., Confessore, N., &amp; Cadwalladr, C. (2018, March 17). How Trump consultants exploited the Facebook data of millions. New York Times. https://edtechbooks.org/-RJPW Seo, S., Chan, H., Brantingham, P. J., Leap, J., Vayanos, P., Tambe, M., &amp; Liu, Y. (2018). Partially generative neural networks for gang crime classification with partial information. Proceedings of the 2018 AAAI/ACM conference on AI, ethics and society (AIES). Association for the Advancement of Artificial Intelligence. Sylvester &amp; Raff (2018). What about applied fairness? Paper presented at The 35th International Conference on Machine Learning. Stockholm, Sweden. Thompson, S. A., &amp; Warzel, C. (2019, December 20). How to track President Trump. New York Times. https://www.nytimes.com/interactive/2019/12/20/opinion/location-data-national-security.html Unger, R. M. (1987). False necessity: Anti-necessitarian social theory in the service of radical democracy. Cambridge University Press. Vitale, A. S. (2017). The end of policing. Verso Books. Vosoughi, S., Roy, D., &amp; Aral, S. (2018). The spread of true and false news online. Science, 359(6380), 1146-1151. Wexler, R. (2018). Life, liberty, and trade secrets: Intellectual property in the criminal justice system. Stanford Law Review, 70(5), 1343-1429. Winner, L. (1986). The whale and the reactor: A search for limits in an age of high technology. University of Chicago Press. "],["m5c-reflect-on-theoretical-and-philosophical-constraints.html", "16 M5C: Reflect on Theoretical and Philosophical Constraints", " 16 M5C: Reflect on Theoretical and Philosophical Constraints You will complete the Module 5 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt: Earlier in this module, we established that theory and philosophy can play a helpfully constraining role in data science. To be clear, data is tremendously helpful for re-evaluating theory and philosophy, and if the data are telling us something different than our theory and philosophy, that’s something worth paying attention to. Much of the history of science is letting data tell us when and where our philosophical commitments are wrong—the data show that the earth revolves around the sun no matter how committed our philosophy is to keeping the earth at the center of the universe. However, we’ve also established that numbers don’t speak for themselves, and we must also use theory and philosophy to ground and question our findings. For example, even if we found that installing government-run cameras inside citizens’ homes was very helpful for catching criminals, our philosophical commitments to human privacy and dignity ought to tell us that this isn’t an acceptable policy solution. Knowing when to let data override theory and philosophy and when to let theory and philosophy override what the data seem to be telling us is a tricky balance—but we have to be committed to both. In a world where data is increasingly seen as having all the answers, it is especially important that we use theory and philosophy to keep data in check. Think about a professional context—a current job, past job, or hoped-for future job—where you could potentially apply your data science or other research skills. What are the theoretical or philosophical commitments that are widely held in that context? Then think about the ways that those commitments could constrain what data you collect, how you interpret data analysis, and how you would act on data. Are some “data-driven decisions” out of the picture? Is that a good thing? A bad thing? Both? After thinking about these questions, write a short response to this discussion post that addresses some of the answers that you came up with. Focus in particular on how theory and philosophy might provide a helpful constraint when deciding how to use or act on data. "],["m6u-unicorns-janitors-and-rock-stars.html", "17 M6U: Unicorns, Janitors, and Rock Stars 17.1 Introduction 17.2 Intentionally Challenging Data Visualization 17.3 Sexy Scientists and… Drab Janitors? 17.4 Strangers in the Dataset 17.5 Sharing Work and Sharing Credit 17.6 Conclusion 17.7 References", " 17 M6U: Unicorns, Janitors, and Rock Stars This chapter draws on material from: 5. Unicorns, Janitors, Ninjas, Wizards, and Rock Stars by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0. Machine learning’s crumbling foundations by Cory Doctorow, licensed under CC BY 4.0. Changes to the source material include light editing, adding new material, deleting original material, combining matieral, rearranging material, changing the citation style, adding links, replacing images, changing original authors’ voice to third person, and adding first-person language from current author. The resulting content is licensed under CC BY 4.0. 17.1 Introduction In Spring 2017, Bloomberg News ran an article with the provocative title “America’s Rich Get Richer and the Poor Get Replaced by Robots” (Giudice &amp; Lu, 2017). Using census data, the authors reported that income inequality is widening across the nation. San Francisco is leading the pack, with an income gap of almost half a million dollars between the richest and the poorest twenty percent of residents. As in other places, the wealth gap has race and gender dimensions. In the Bay Area, people of color earn sixty-eight cents for every dollar earned by white people, and 59 percent of single mothers live in poverty (National Equity Atlas, 2018). San Francisco also has the lowest proportion of children to adults in any major US city, and—since 2003—an escalating rate of evictions. Although the San Francisco Rent Board collects data on these evictions, it does not track where people go after they are evicted, how many of those people end up homeless, or which landlords are responsible for systematically evicting major blocks of the city. In 2013, the Anti-Eviction Mapping Project (AEMP) stepped in. The initiative is a self-described collective of “housing justice activists, researchers, data nerds, artists, and oral historians.” It is a multiracial group with significant, though not exclusive, project leadership by women. The AEMP is mapping eviction, and doing so through a collaborative, multimodal, and—yes—quite messy process. If you visit antievictionmap.com, you won’t actually find a single map. There are seventy-eight distinct maps linked from the homepage: maps of displaced residents, of evictions, of tech buses, of property owners, of the Filipino diaspora, of the declining numbers of Black residents in the city, and more. The AEMP has a distinct way of working that is grounded in its stated commitment to antiracist, feminist, and decolonial methodologies (Anti-Eviction Mapping Project, 2016). Most of the projects happen in collaboration with nonprofits and community-based organizations. Additional projects originate from within the collective. For example, the group recently published an atlas of the Bay Area called Counterpoints: A San Fancisco Bay Area Atlas of Displacement &amp; Resistance (Anti-Eviction Mapping Project, 2021), which covers topics such as migration and relocation, gentrification and the prison pipeline, Indigenous and colonial histories of the region, and speculation about the future. One of the AEMP’s longest-standing collaborations is with the Eviction Defense Collaborative (EDC), a nonprofit that provides court representation for people who have been evicted. Although the city does not collect data on the race or income of evictees, the EDC does collect those demographics, and it works with 90 percent of the tenants whose eviction cases end up in San Francisco courts (Eviction Defense Collaborative, 2016). In 2014, the EDC approached the AEMP to help produce its annual report and in return offered to share its demographic data with the organization. Since then, the two groups have continued to work together on EDC annual reports, as well as additional analyses of evictions with a focus on race. The AEMP has also gone on to produce reports with tenants’ rights organizations, timelines of gentrification with Indigenous students, oral histories with grants from anthropology departments, and murals with arts organizations, as well as maps and more maps. Some of the AEMP’s maps are designed to leverage the ability of data visualization to make patterns visible at a glance. For example, the Tech Bus Stop Eviction Map produced in 2014 plots the locations of three years of Ellis Act evictions (Anti-Eviction Mapping Project, n.d.-a). This is a form of “no-fault” eviction in which landlords can claim that they are going out of the rental business. In many cases, this is so that they can convert the building to a condominium and sell the units at significant profit. San Francisco has seen almost five thousand invocations of the Ellis Act since 1994. On the map, the AEMP plotted Ellis Act evictions in relationship to the location of technology company bus stops. Starting in the 2000s, tech companies with campuses in Silicon Valley began offering private luxury buses as a perk to attract employees who wanted to live in downtown San Francisco but didn’t want the hassle of commuting. Colloquially known as “the Google buses” because Google was the most visible company to implement the practice, these vehicles use public bus stops—illegally at first—to shuttle employees to their offices in comfort (and also away from the public transportation system, which would otherwise reap the benefits of their fares; see Solnit, 2013). Because a new, wealthy clientele began to seek condos near the bus stops, property values soared—and so did the rate of evictions of long-time neighborhood residents. The AEMP analysis showed that, between 2011 and 2013, 69 percent of no-fault evictions occurred within four blocks of a tech bus stop. The map makes that finding plain. 17.2 Intentionally Challenging Data Visualization But other AEMP maps are intentionally designed not to depict a clear correlation between evictions and place. In Narratives of Displacement and Resistance (Anti-Eviction Mapping Project, n.d.-a), five thousand evictions are each represented as a differently sized red bubble, so the base map of San Francisco is barely visible underneath. On top of this red sea, sky-blue bubbles dot the locations where the AEMP has conducted interviews with displaced residents, as well as activists, mediamakers, and local historians. Clicking on a blue bubble sets one of the dozens of oral histories in motion—for instance, the story of Phyllis Bowie, a resident facing eviction from her one-bedroom apartment. “I was born and raised in San Francisco proudly,” she begins. Bowie goes on to recall how she returned from the Air Force and worked like crazy for two years at her own small business, building up an income record that would make her eligible for a lease-to-own apartment in Midtown, the historically Black neighborhood where she had grown up. In 2015, however, the city broke the master lease and took away the rent control on her building. Now tenants like Bowie, who moved there with the promise of homeownership, are facing skyrocketing rents that none of them can afford. Bowie is leading rent strikes and organizing the building’s tenants, but their future is uncertain. This uncertainty is carried over into the design of the map, which uses a phenomenon that is often discouraged in data visualization—occlusion—to drive home its point. Occlusion typically refers to the “problem” that occurs when some marks (like the eviction dots) obscure other important features (like the whole geography of the city). But here it underscores the point that there are very few patterns to detect when the entire city is covered in big red eviction bubbles and abundant blue story dots. Put another way, the whole city is a pattern, and that pattern is the problem—much more than a problem of information design. In this way, the Narratives map enacts dissent from San Francisco city policies in the same way that it enacts dissent from the conventions of information design. It refuses both the clarity and cleanliness associated with the best practices of data visualization and the homogenizing and “cleanliness” associated with the forces of gentrification that lead to evictions in the first place (see Naegler, 2012). The visual point of the map is simple and exhortative: there are too many evictions. And there are too many eviction stories. The map does not efficiently reveal how evictions data may be correlated with Bay Area Rapid Transit (BART) stops, income, Google bus stops, or any other potential dimensions of the data. Even finding the Narratives map is difficult, given the sheer number of maps and visualizations on the AEMP website. There is no “master dashboard” that integrates all the information that the AEMP has collected into a single interface. But all these design choices reinforce the main purpose of the AEMP: to document the effects of displacement and to resist it through critical and creative means. The AEMP thus offers a rich set of examples of feminist counterdata collection and countervisualization strategies. Individually and together, the maps illustrate how Katie Lloyd Thomas, founding member of the feminist art architecture collective taking place, envisions “partiality and contestation” taking place in graphical design. “Rather than tell one ‘true’ story of consensus, [the drawing/graphic] might remember and acknowledge multiple, even contradictory versions of reality,” she explains (Thomas, 2016). The Narratives map populates its landscape with these contradictory realities. It demonstrates that behind each eviction is a person—a person like Bowie, with a unique voice and a unique story. The voices we hear are diverse, multiple, specific, divergent— and deliberately so. In so doing, the Narratives map, and the AEMP more generally, exemplify another feminist principle of data science: embrace pluralism. Embracing pluralism in data science means valuing many perspectives and voices and doing so at all stages of the process—from collection to cleaning to analysis to communication. It also means attending to the ways in which data science methods can inadvertently work to suppress those voices in the service of clarity, cleanliness, and control. Many of our received ideas about data science work against pluralistic meaning-making processes, and one goal of data feminism is to change that. 17.3 Sexy Scientists and… Drab Janitors? Data cleaning holds a special kind of mythos in data science. “It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data,” writes Hadley Wickham (2014) in the first sentence of the abstract of “Tidy Data,” his widely cited paper. Wickham is also the author of the tidyr package for R, which we’ll work with later this week. Wickham’s package shares the sentiment expressed in his paper: that data are inherently messy and need to be tidied and tamed. As we’ve already learned, Wickham went on to create the “tidyverse,” an expanded set of packages that formalize a clear workflow for processing data, which have been developed by a team of equally enthusiastic contributors. Articles in the popular and business press corroborate this insistence on tidiness, as well as its pressing need. In Harvard Business Review, the work of the data scientist is glorified in terms of this tidying function: “At ease in the digital realm, they are able to bring structure to large quantities of formless data and make analysis possible” (Davenport &amp; Patil, 2012). Here, the intrepid analyst wrangles orderly tables from unstructured chaos. According to the article, it’s “the sexiest job of the 21st century.” Along these same lines, people who work with data are often called unicorns (because they are rare and have special skills), wizards (because they can do magic), ninjas (because they execute complicated, expert moves), or rock stars (because they outperform others). However, the data analyst’s work is sometimes described as far less attractive. A 2014 article equated the task of cleaning data to the low-wage maintenance work of “janitors” (Lohr, 2014). This kind of classist comparison overlooks how undervalued the latter kind of work is (whether it’s janitorial or data cleaning). Obtaining good data and/or cleaning up bad data is tedious, repetitive grunt-work. It’s unglamorous, time-consuming, and low-waged. Cleaning data is the equivalent of sterilizing surgical implements–vital, high-skilled, and invisible unless someone fails to do it. It’s work performed by anonymous, low-waged adjuncts to the surgeon, who is the star of the show and who gets credit for the success of the operation. The title of a Google Research team (Sambasivan et al., 2021) paper beautifully summarizes how this is playing out in machine learning, one major application of data science: “Everyone wants to do the model work, not the data work: Data Cascades in High-Stakes AI.” The paper analyzes machine learning failures from a cross-section of high-stakes projects (health diagnostics, anti-poaching, etc) in East Africa, West Africa and India. They trace the failures of these projects to data-quality, and drill into the factors that caused the data problems. The failures stem from a variety of causes. First, data-gathering and cleaning are low-waged, invisible, and thankless work. Front-line workers who produce the data —like medical professionals who have to do extra data-entry—are not compensated for extra work. Often, no one even bothers to explain what the work is for. Some of the data-cleaning workers are atomized pieceworkers, such as those who work for Amazon’s Mechanical Turk, who lack both the context in which the data was gathered and the context for how it will be used. This data is passed to model-builders, who lack related domain expertise. The hastily labeled X-ray of a broken bone, annotated by an unregarded and overworked radiologist, is passed onto a data scientist who knows nothing about broken bones and can’t assess the labels. This is an age-old problem in automation, pre-dating computer science and even computers. The “scientific management” craze that started in the 1880s saw technicians observing skilled workers with stopwatches and clipboards, then restructuring the workers’ jobs by fiat. Rather than engaging in the anthropological, interpretive work that Clifford Geertz called “thick description,” the management “scientists” discarded workers’ qualitative experience, then treated their own positivist assessments as quantitative and thus authoritative. Bad data make bad models. Bad models instruct people to make ineffective or harmful interventions. Those bad interventions produce more bad data, which is fed into more bad models—it’s a “data-cascade.” GIGO (Garbage In, Garbage Out) was already a bedrock of statistical practice before the term was coined in 1957. Statistical analysis and inference cannot proceed from bad data. Producing good data and validating datasets are the kind of unsexy, undercompensated maintenance work that all infrastructure requires—and, as with other kinds of infrastructure, it is undervalued by journals, academic departments, funders, corporations, and governments. But all technological debts accrue punitive interest. The decision to operate on bad data because good data is in short supply isn’t like looking for your car-keys under the lamp-post—it’s like driving with untrustworthy brakes and a dirty windscreen. 17.4 Strangers in the Dataset It’s pretty clear that we ought to value data cleaning more (both in terms of the language we use and the money we pay). Data cleaners must be able to tame the chaos of information overload. They must “scrub” and “cleanse” dirty data. And they must undertake deliberate action—either extraordinary or mundane—to put data back in their proper place. However, it’s also worth being more critical about assumptions and anxieties about the need for tidiness, cleanliness, and order. But what might be lost in the process of dominating and disciplining data? Whose perspectives might be lost in that process? And, conversely, whose perspectives might be additionally imposed? The ideas expressed by Wickham, and by the press, carry the assumption that all data scientists, in all contexts, value cleanliness and control over messiness and complexity. But as the example of the AEMP demonstrates, these are not the requirements, nor the goals, of all data projects. In fact, Rawson and Muñoz (2019) caution that cleaning can function as a “diversity-hiding trick” (p. 290). In the perceived “messiness” of data, there is actually rich information about the circumstances under which it was collected. Data studies scholar Yanni Loukissas (2019) concurs. Rather than talking about datasets, he advocates that we talk about data settings—his term to describe both the technical and the human processes that affect what information is captured in the data collection process and how the data are then structured. As an example of how the data setting matters, Loukissas tells the story of being at a hackathon in Cambridge, Massachusetts, where he began to explore a dataset from the Clemson University Library, located in South Carolina. He stumbled across a puzzling record in which the librarian had noted the location of the item as “upstate.” Such a designation is, of course, relational to the place of collection. For South Carolinians, upstate is a very legible term that refers to the westernmost region of the state, where Clemson is located. But it does not hold the same meaning for a person from New York, where upstate refers to its own northern region, nor does it hold the same meaning for a person sitting at a hackathon in Massachusetts, which does not have an upstate part of the state. Had someone at the hackathon written the entry from where they sat, they might have chosen to list the ten or so counties that South Carolinians recognize as upstate, so as to be more clearly legible to a wider geographic audience. But there is meaning conveyed by the term that would not be conveyed by other, more general ways of indicating the same region. Only somebody already located in South Carolina would have referred to that region in that way. From that usage of the term, we can reason that the data were originally collected in South Carolina. This information is not included elsewhere in the library record. It is because of records like this one that the process of cleaning and tidying data can be so complicated and, at times, can be a destructive rather than constructive act. One way to think of it is like chopping off the roots of a tree that connects it to the ground from which it grew. It’s an act that irreversibly separates the data from their context. We might relate the growth of tools like tidyr that help to trim and tidy data to another human intervention into the environment: the proliferation of street names and signs that transformed the landscape of the nineteenth-century United States. Geographer Reuben Rose-Redwood (2008) describes how, for example, prior to the Revolutionary War, very few streets in Manhattan had signs posted at intersections. Street names, such as they existed, were vernacular and related to the particularity of a spot—for example, “Take a right at the red house.” But with the increased mobility of people and things—think of the postal system, the railroads, or the telegraph—street names needed to become systematized. Rose-Redwood calls this the production of “legible urban spaces.” Then, as now, there is high economic value to legible urban spaces, particularly for large corporations (we’re looking at you, Amazon) to deliver boxes of anything and everything directly to people’s front doors. This is a point that Rose-Redwood makes with respect to the nineteenth century as well. It wasn’t just the mail carrier who needed these signs. Rather, the street names were for the “stranger, merchant, or businessman”—this from an 1838 Philadelphia business directory—who came from elsewhere to operationalize a new landscape in the service of their own profit. The point here is that one does not need street names for navigation until one has strangers in the landscape. Likewise, many data do not need cleaning until there are strangers in the dataset. The Clemson University Library dataset was perfectly clear to Clemson’s own librarians. But once hackers in Cambridge get their hands on it, upstate started to make a lot less sense, and it was not at all helpful in producing, for instance, a map of all the library records in the United States. Put more generally, once the data scientists involved in a project are not from within the community, once the place of analysis changes, once the scale of the project shifts, or once a single dataset needs to be combined with others—then we have strangers in the dataset. These rock stars and ninjas are strangers in the dataset because, like the hackers in Cambridge, they often sit at one, two, or many levels removed from the collection and maintenance process of the data that they work with. This is a negative externality—an inadvertent third-party consequence—that arises when working with open data, application programming interfaces (APIs), and the vast stores of training data available online. These data appear available and ready to mobilize, but what they represent is not always well-documented or easily understood by outsiders. Being a stranger in the dataset is not an inherently bad thing, but it carries significant risk of what renowned postcolonial scholar Gayatri Spivak (2010) calls epistemic violence—the harm that dominant groups like colonial powers wreak by privileging their paradigms and ways of knowing over local and Indigenous ways. 17.5 Sharing Work and Sharing Credit This problem is compounded by the belief that data work is a solitary undertaking. This is reflected in the fact that unicorns are unique by definition, and wizards, ninjas, and rock stars are also all people who seem to work alone. This is a fallacy, of course; every rock star requires a backing band, and if we’ve learned anything from Harry Potter, it’s that any particular tap of the wand is the culmination of years of education, training, and support. Wizards, ninjas, rock stars, and janitors each have something else in common: they are assumed to be men. If you doubt this assertion, try doing a Google image search and count how many male-presenting wizards and janitors you see before you get to a single female-presenting one. Or consider why news articles about “data janitors” don’t describe them as doing “cleaning lady” work? Like janitorial work, which is disproportionately undertaken by working-class people of color, the idea of the data ninja also carries racist connotations (Data USA, n.d.; Ti, 2016). And there’s even more: shared among the first four terms—unicorns, wizards, ninjas, rock stars—is a focus on the individual’s extraordinary technical expertise and their ability to prevail when others cannot. We might have more accurately said “his ability to prevail” because these ideas about individual mastery and prevailing against steep odds are, of course, also associated with men. There is a “genius” in the world of eviction data—it is Matthew Desmond, designated as such by the MacArthur Foundation for his work on poverty and eviction in the United States. He is a professor and director of the Eviction Lab at Princeton University, which has been working for several years to compile a national database of evictions and make it available to the general public. Although the federal government collects national data on foreclosures, there is no national database of evictions, something that is desperately needed for the many communities where housing is in crisis. Initially, the Eviction Lab had approached community organizations like the AEMP to request their data. The AEMP wanted to know more—about privacy protections and how the Eviction Lab would keep the data from falling into landlord hands. Instead of continuing the conversation, the Eviction Lab turned to a real estate data broker and purchased data of lower quality. But in an article written by people affiliated with the AEMP and other housing justice organizations, the authors state, “AEMP and Tenants Together have found three-times the amount of evictions in California as Desmond’s Eviction Lab show (Aiello et al., 2018). As Desmond tells it in an interview with Catherine D’Ignazio, this decision was due to a change in the lab’s data collection strategy. “We’re a research lab, so one thing that’s important to us is the data cleaning process. If you want to know does Chicago evict more people than Boston, you’ve got to compare apples to apples.” In Desmond’s view, it is more methodologically sound to compare datasets that have been already aggregated and standardized. And yet Desmond acknowledges that Eviction Lab’s data for the state of California is undercounting evictions; there is even a message on the site that makes that explicit. So here’s an ethical quandary: Does one choose cleaner data at a larger scale that is relatively easy and quick to purchase? Or more accurate data at a local scale for which one has to engage and build trust with community groups? In this case, the priority was placed on speed at the expense of establishing trusted relationships with actors on the ground, and on broad national coverage at the expense of local accuracy. Though the Eviction Lab is doing important work, continued decisions that prioritize speed and comprehensiveness can’t help but maintain the cultural status of the solitary “genius,” effectively downplaying the work of coalitions, communities, and movements that are—not coincidentally—often led primarily by women and people of color. 17.6 Conclusion What might be gained if we not only recognized but also valued the fact that data work involves multiple voices and multiple types of expertise? What if producing new social relationships—increasing community solidarity and enhancing social cohesion—was valued (and funded) as much as acquiring data? This could lead to a multiplication of projects like the AEMP: projects that do demonstrable good with data and do so together with the communities they seek to support. 17.7 References Aiello, D., Bates, L., Graziani, T., Herring, C., Maharawal, M. McElory, E. Phan, P., &amp; Purser, G. (2018, August 22). Eviction lab misses the mark. ShelterForce. https://shelterforce.org/2018/08/22/eviction-lab-misses-the-mark/ Anti-Eviction Mapping Project (2016). About/Acerca De Nosotros. https://www.antievictionmap.com/about Anti-Eviction Mapping Project (2021). Counterpoints: A San Francisco Bay Area atlas of displacement &amp; resistance. PM Press. Anti-Eviction Mapping Project (n.d.-a). Narratives of displacement and resistance. http://www.antievictionmappingproject.net/narratives.html Anti-Eviction Mapping Project (n.d.-b). Tech bus stops and no-fault evictions. https://www.antievictionmappingproject.net/techbusevictions.html Data USA. (n.d.). Janitors &amp; building cleaners: Race &amp; ethnicity. https://datausa.io/profile/soc/janitors-building-cleaners Davenport, T. H., &amp; Patil, D. J. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century Eviction Defense Collaborative. (2016). City of change: Fighting for San Francisco’s vanishing communities. http://www.antievictionmappingproject.net/EDC_2016.pdf Giudice, V. D., &amp; Lu, W. (2017, April 26). America’s rich get richer and the poor get replaced by robots. Bloomberg.com. https://www.bloomberg.com/news/articles/2017-04-26/america-s-rich-poor-divide-keeps-ballooning-as-robots-take-jobs Lohr, S. (2014, August 17). For big-data scientists, “janitor work” is key hurdle to insights. New York Times. https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html Loukissas, Y. A. (2019). All data are local: Thinking critically in a data-driven society. MIT Press. Naegler, L. (2012). Gentrification and resistance: cultural criminology, control, and the commodification of urban protest in Hamburg. LIT Verlag. National Equity Atlas. (2018). Wages: Median (old) San Francisco-Oakland-Fremont, CA Area. http://nationalequityatlas.org/indicators/Wages:_Median_(old)/Over_time:7616/San_Francisco-Oakland-Fremont,_CA_Metro_Area/false/ Rawson, K., &amp; Muñoz, T. (2019). Against cleaning. In M. K. Gold &amp; Lauren F. Klein (Eds.), Debates in the digital humanities 2019. University of Minnesota Press. Rose-Redwood, R. S. (2008). Indexing the great ledger of the community: Urban house numbering, city directories, and the production of spatial legibility. Journal of Historical Geography, 34(2), 286-310. Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., &amp; Aroyo, L. (2021). “Everyone wants to do the model work, not the data work”: Data cascades in high-stakes AI. In CHI conference on human factors in computing systems (CHI ’21). ACM. Solnit, R. (2013). Diary. London Review of Books, 35(3), 34-35. https://www.lrb.co.uk/v35/n03/rebecca-solnit/diary Spivak, G. C. (2010). Can the subaltern speak? Reflection on the history of an idea. Columbia University Press. Thomas, K. L. (2016). Lines in practice: Thinking architectural representation through feminist critiques of geometry. Geography Research Forum, 21, 57-76. Ti, A. (2016, September 21). Team, we have to give up on ninja. Angry Asian Man [blog]. http://blog.angryasianman.com/2016/09/team-we-have-to-give-up-on-ninja.html United Way. (2017). Snapshot of poverty: San Francisco County. https://uwba.org/wp-content/uploads/2017/10/SanFrancisco-Snapshot.pdf Wickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1-23. "],["m6a-wrangling-and-tidying-data.html", "18 M6A: Wrangling and Tidying Data 18.1 Introduction 18.2 The pipe operator: %&gt;% 18.3 filter rows 18.4 summarize variables 18.5 group_by rows 18.6 mutate existing variables 18.7 arrange and sort rows 18.8 join data frames 18.9 Other verbs 18.10 Conclusion 18.11 Tidying Data 18.12 Importing data 18.13 “Tidy” data 18.14 Case study: Democracy in Guatemala 18.15 tidyverse package 18.16 Conclusion", " 18 M6A: Wrangling and Tidying Data This content draws on material from: Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim, licensed under CC BY-NC-SA 4.0 Changes to the source material include addition of new material; light editing; rearranging, removing, and combining original material; adding and changing links; and adding first-person language from current author. The resulting content is licensed under CC BY-NC-SA 4.0. 18.1 Introduction Two weeks ago, we learned about importing a dataset into R. However, In this chapter, we’ll introduce a series of functions from the dplyr package for data wrangling that will allow you to take a data frame and “wrangle” it (transform it) to suit your needs. Such functions include: filter() a data frame’s existing rows to only pick out a subset of them. For example, the alaska_flights data frame. summarize() one or more of its columns/variables with a summary statistic. Examples of summary statistics include the median and interquartile range of temperatures as we saw in Section ?? on boxplots. group_by() its rows. In other words, assign different rows to be part of the same group. We can then combine group_by() with summarize() to report summary statistics for each group separately. For example, say you don’t want a single overall average departure delay dep_delay for all three origin airports combined, but rather three separate average departure delays, one computed for each of the three origin airports. mutate() its existing columns/variables to create new ones. For example, convert hourly temperature recordings from degrees Fahrenheit to degrees Celsius. arrange() its rows. For example, sort the rows of weather in ascending or descending order of temp. join() it with another data frame by matching along a “key” variable. In other words, merge these two data frames together. Notice how we used computer_code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling has intuitively verb-named functions that are easy to remember. There is a further benefit to learning to use the dplyr package for data wrangling: its similarity to the database querying language SQL (pronounced “sequel” or spelled out as “S”, “Q”, “L”). SQL (which stands for “Structured Query Language”) is used to manage large databases quickly and efficiently and is widely used by many institutions with a lot of data. While SQL is a topic left for a book or a course on database management, keep in mind that once you learn dplyr, you can learn SQL easily. We’ll talk more about their similarities in Subsection 18.8.4. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 6.4 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(nycflights13) 18.2 The pipe operator: %&gt;% Before we start data wrangling, let’s first introduce a nifty tool that gets loaded with the dplyr package: the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Let’s start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) This code isn’t so hard to read since we are applying only three functions: f(), then g(), then h() and each of the functions is short in its name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then.” For example, you can obtain the same output as the hypothetical sequence of functions as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example, the flights data frame we explored in Section 6.5. The sequence of functions, here f(), g(), and h(), will mostly be a sequence of any number of the six data wrangling verb-named functions we listed in the introduction to this chapter. For example, the filter(carrier == \"AS\") function and argument specified we previewed earlier. The result will be the transformed/modified data frame that you want. In our example, we’ll save the result in a new data frame by using the &lt;- assignment operator with the name alaska_flights via alaska_flights &lt;-. alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) Much like when adding layers to a ggplot() using the + sign, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence using the pipe operator %&gt;%. Furthermore, much like how the + sign has to come at the end of lines when constructing plots, the pipe operator %&gt;% has to come at the end of lines as well. Keep in mind, there are many more advanced data wrangling functions than just the six listed in the introduction to this chapter; you’ll see some examples of these in Section 18.9. However, just with these six verb-named functions you’ll be able to perform a broad array of data wrangling tasks for the rest of this book. 18.3 filter rows Figure 18.1: Diagram of filter() rows operation. The filter() function here works much like the “Filter” option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria. We begin by focusing only on flights from New York City to Portland, Oregon. The dest destination code (or airport code) for Portland, Oregon is \"PDX\". Run the following and look at the results in RStudio’s spreadsheet viewer to ensure that only flights heading to Portland are chosen here: portland_flights &lt;- flights %&gt;% filter(dest == &quot;PDX&quot;) View(portland_flights) Note the order of the code. First, take the flights data frame flights then filter() the data frame so that only those where the dest equals \"PDX\" are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(dest = \"PDX\") will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other operators beyond just the == operator that tests for equality: &gt; corresponds to “greater than” &lt; corresponds to “less than” &gt;= corresponds to “greater than or equal to” &lt;= corresponds to “less than or equal to” != corresponds to “not equal to.” The ! is used in many programming languages to indicate “not.” Furthermore, you can combine multiple criteria using operators that make comparisons: | corresponds to “or” &amp; corresponds to “and” To see many of these in action, let’s filter flights for all rows that departed from JFK and were heading to Burlington, Vermont (\"BTV\") or Seattle, Washington (\"SEA\") and departed in the months of October, November, or December. Run the following: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot; &amp; (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) &amp; month &gt;= 10) View(btv_sea_flights_fall) Note that even though colloquially speaking one might say “all flights leaving Burlington, Vermont and Seattle, Washington,” in terms of computer operations, we really mean “all flights leaving Burlington, Vermont or leaving Seattle, Washington.” For a given row in the data, dest can be \"BTV\", or \"SEA\", or something else, but not both \"BTV\" and \"SEA\" at the same time. Furthermore, note the careful use of parentheses around dest == \"BTV\" | dest == \"SEA\". We can often skip the use of &amp; and just separate our conditions with a comma. The previous code will return the identical output btv_sea_flights_fall as the following code: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot;, (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;), month &gt;= 10) View(btv_sea_flights_fall) Let’s present another example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to flights that didn’t go to Burlington, VT or Seattle, WA. not_BTV_SEA &lt;- flights %&gt;% filter(!(dest == &quot;BTV&quot; | dest == &quot;SEA&quot;)) View(not_BTV_SEA) Again, note the careful use of parentheses around the (dest == \"BTV\" | dest == \"SEA\"). If we didn’t use parentheses as follows: flights %&gt;% filter(!dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) We would be returning all flights not headed to \"BTV\" or those headed to \"SEA\", which is an entirely different resulting data frame. Now say we have a larger number of airports we want to filter for, say \"SEA\", \"SFO\", \"PDX\", \"BTV\", and \"BDL\". We could continue to use the | (or) operator: many_airports &lt;- flights %&gt;% filter(dest == &quot;SEA&quot; | dest == &quot;SFO&quot; | dest == &quot;PDX&quot; | dest == &quot;BTV&quot; | dest == &quot;BDL&quot;) but as we progressively include more airports, this will get unwieldy to write. A slightly shorter approach uses the %in% operator along with the c() function. Recall from Subsection 6.3.1 that the c() function “combines” or “concatenates” values into a single vector of values. many_airports &lt;- flights %&gt;% filter(dest %in% c(&quot;SEA&quot;, &quot;SFO&quot;, &quot;PDX&quot;, &quot;BTV&quot;, &quot;BDL&quot;)) View(many_airports) What this code is doing is filtering flights for all flights where dest is in the vector of airports c(\"BTV\", \"SEA\", \"PDX\", \"SFO\", \"BDL\"). Both outputs of many_airports are the same, but as you can see the latter takes much less energy to code. The %in% operator is useful for looking for matches commonly in one vector/variable compared to another. As a final note, we recommend that filter() should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations you care about. 18.4 summarize variables The next common task when working with data frames is to compute summary statistics. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (also called the average) and the median (the middle value). Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value also called the minimum, the largest value also called the maximum, and the standard deviation. See Appendix ?? for a glossary of such summary statistics. Let’s calculate two summary statistics of the temp temperature variable in the weather data frame: the mean and standard deviation (recall from Section 6.5 that the weather data frame is included in the nycflights13 package). To compute these summary statistics, we need the mean() and sd() summary functions in R. Summary functions in R take in many values and return a single value, as illustrated in Figure 18.2. Figure 18.2: Diagram illustrating a summary function in R. More precisely, we’ll use the mean() and sd() summary functions within the summarize() function from the dplyr package. Note you can also use the British English spelling of summarise(). As shown in Figure 18.3, the summarize() function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics. Figure 18.3: Diagram of summarize() rows. We’ll save the results in a new data frame called summary_temp that will have two columns/variables: the mean and the std_dev: summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp), std_dev = sd(temp)) summary_temp # A tibble: 1 × 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 NA NA Why are the values returned NA? As we saw in Subsection ?? when creating the scatterplot of departure and arrival delays for alaska_flights, NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it? Perhaps the data was not collected at all because it was too difficult to do so? Perhaps there was an erroneous value that someone entered that has been corrected to read as missing? You’ll often encounter issues with missing values when working with real data. Going back to our summary_temp output, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, NA is returned. To work around this fact, you can set the na.rm argument to TRUE, where rm is short for “remove”; this will ignore any NA missing values and only return the summary value for all non-missing values. The code that follows computes the mean and standard deviation of all non-missing values of temp: summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_temp # A tibble: 1 × 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 55.3 17.8 Notice how the na.rm = TRUE are used as arguments to the mean() and sd() summary functions individually, and not to the summarize() function. However, one needs to be cautious whenever ignoring missing values as we’ve just done. In the upcoming Learning checks questions, we’ll consider the possible ramifications of blindly sweeping rows with missing values “under the rug.” This is in fact why the na.rm argument to any summary statistic function in R is set to FALSE by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis. What are other summary functions we can use inside the summarize() verb to compute summary statistics? As seen in the diagram in Figure 18.2, you can use any function in R that takes many values and returns just one. Here are just a few: mean(): the average sd(): the standard deviation, which is a measure of spread min() and max(): the minimum and maximum values, respectively IQR(): interquartile range sum(): the total amount when adding multiple numbers n(): a count of the number of rows in each group. This particular summary function will make more sense when group_by() is covered in Section 18.5. 18.5 group_by rows Figure 18.4: Diagram of group_by() and summarize(). Say instead of a single mean temperature for the whole year, you would like 12 mean temperatures, one for each of the 12 months separately. In other words, we would like to compute the mean temperature split by month. We can do this by “grouping” temperature observations by the values of another variable, in this case by the 12 values of the variable month. Run the following code: summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_monthly_temp # A tibble: 12 × 3 month mean std_dev &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 35.6 10.2 2 2 34.3 6.98 3 3 39.9 6.25 4 4 51.7 8.79 5 5 61.8 9.68 6 6 72.2 7.55 7 7 80.1 7.12 8 8 74.5 5.19 9 9 67.4 8.47 10 10 60.1 8.85 11 11 45.0 10.4 12 12 38.4 9.98 This code is identical to the previous code that created summary_temp, but with an extra group_by(month) added before the summarize(). Grouping the weather dataset by month and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the 12 months of the year. It is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the data frame changes. For example, let’s consider the diamonds data frame included in the ggplot2 package. Run this code: diamonds # A tibble: 53,940 × 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ℹ 53,930 more rows Observe that the first line of the output reads # A tibble: 53,940 x 10. This is an example of meta-data, in this case the number of observations/rows and variables/columns in diamonds. The actual data itself are the subsequent table of values. Now let’s pipe the diamonds data frame into group_by(cut): diamonds %&gt;% group_by(cut) # A tibble: 53,940 × 10 # Groups: cut [5] carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ℹ 53,930 more rows Observe that now there is additional meta-data: # Groups: cut [5] indicating that the grouping structure meta-data has been set based on the 5 possible levels of the categorical variable cut: \"Fair\", \"Good\", \"Very Good\", \"Premium\", and \"Ideal\". On the other hand, observe that the data has not changed: it is still a table of 53,940 \\(\\times\\) 10 values. Only by combining a group_by() with another data wrangling operation, in this case summarize(), will the data actually be transformed. diamonds %&gt;% group_by(cut) %&gt;% summarize(avg_price = mean(price)) # A tibble: 5 × 2 cut avg_price &lt;ord&gt; &lt;dbl&gt; 1 Fair 4359. 2 Good 3929. 3 Very Good 3982. 4 Premium 4584. 5 Ideal 3458. If you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the ungroup() function: diamonds %&gt;% group_by(cut) %&gt;% ungroup() # A tibble: 53,940 × 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ℹ 53,930 more rows Observe how the # Groups: cut [5] meta-data is no longer present. Let’s now revisit the n() counting summary function we briefly introduced previously. Recall that the n() function counts rows. This is opposed to the sum() summary function that returns the sum of a numerical variable. For example, suppose we’d like to count how many flights departed each of the three airports in New York City: by_origin &lt;- flights %&gt;% group_by(origin) %&gt;% summarize(count = n()) by_origin # A tibble: 3 × 2 origin count &lt;chr&gt; &lt;int&gt; 1 EWR 120835 2 JFK 111279 3 LGA 104662 We see that Newark (\"EWR\") had the most flights departing in 2013 followed by \"JFK\" and lastly by LaGuardia (\"LGA\"). Note there is a subtle but important difference between sum() and n(); while sum() returns the sum of a numerical variable, n() returns a count of the number of rows/observations. 18.5.1 Grouping by more than one variable You are not limited to grouping by one variable. Say you want to know the number of flights leaving each of the three New York City airports for each month. We can also group by a second variable month using group_by(origin, month): by_origin_monthly &lt;- flights %&gt;% group_by(origin, month) %&gt;% summarize(count = n()) by_origin_monthly # A tibble: 36 × 3 # Groups: origin [3] origin month count &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 EWR 1 9893 2 EWR 2 9107 3 EWR 3 10420 4 EWR 4 10531 5 EWR 5 10592 6 EWR 6 10175 7 EWR 7 10475 8 EWR 8 10359 9 EWR 9 9550 10 EWR 10 10104 # ℹ 26 more rows Observe that there are 36 rows to by_origin_monthly because there are 12 months for 3 airports (EWR, JFK, and LGA). Why do we group_by(origin, month) and not group_by(origin) and then group_by(month)? Let’s investigate: by_origin_monthly_incorrect &lt;- flights %&gt;% group_by(origin) %&gt;% group_by(month) %&gt;% summarize(count = n()) by_origin_monthly_incorrect # A tibble: 12 × 2 month count &lt;int&gt; &lt;int&gt; 1 1 27004 2 2 24951 3 3 28834 4 4 28330 5 5 28796 6 6 28243 7 7 29425 8 8 29327 9 9 27574 10 10 28889 11 11 27268 12 12 28135 What happened here is that the second group_by(month) overwrote the grouping structure meta-data of the earlier group_by(origin), so that in the end we are only grouping by month. The lesson here is if you want to group_by() two or more variables, you should include all the variables at the same time in the same group_by() adding a comma between the variable names. 18.6 mutate existing variables Figure 18.5: Diagram of mutate() columns. Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius (°C) instead of degrees Fahrenheit (°F). The formula to convert temperatures from °F to °C is \\[ \\text{temp in C} = \\frac{\\text{temp in F} - 32}{1.8} \\] We can apply this formula to the temp variable using the mutate() function from the dplyr package, which takes existing variables and mutates them to create new ones. weather &lt;- weather %&gt;% mutate(temp_in_C = (temp - 32) / 1.8) In this code, we mutate() the weather data frame by creating a new variable temp_in_C = (temp - 32) / 1.8 and then overwrite the original weather data frame. Why did we overwrite the data frame weather, instead of assigning the result to a new data frame like weather_new? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable temp, but instead created a new variable called temp_in_C? Because if we did this, we would have erased the original information contained in temp of temperatures in Fahrenheit that may still be valuable to us. Let’s now compute monthly average temperatures in both °F and °C using the group_by() and summarize() code we saw in Section 18.5: summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean_temp_in_F = mean(temp, na.rm = TRUE), mean_temp_in_C = mean(temp_in_C, na.rm = TRUE)) summary_monthly_temp # A tibble: 12 × 3 month mean_temp_in_F mean_temp_in_C &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 35.6 2.02 2 2 34.3 1.26 3 3 39.9 4.38 4 4 51.7 11.0 5 5 61.8 16.6 6 6 72.2 22.3 7 7 80.1 26.7 8 8 74.5 23.6 9 9 67.4 19.7 10 10 60.1 15.6 11 11 45.0 7.22 12 12 38.4 3.58 Let’s consider another example. Passengers are often frustrated when their flight departs late, but aren’t as annoyed if, in the end, pilots can make up some time during the flight. This is known in the airline industry as gain, and we will create this variable using the mutate() function: flights &lt;- flights %&gt;% mutate(gain = dep_delay - arr_delay) Let’s take a look at only the dep_delay, arr_delay, and the resulting gain variables for the first 5 rows in our updated flights data frame in Table 18.1. Table 18.1: First five rows of departure/arrival delay and gain variables dep_delay arr_delay gain 2 11 -9 4 20 -16 2 33 -31 -1 -18 17 -6 -25 19 The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its “gained time in the air” is a loss of 9 minutes, hence its gain is 2 - 11 = -9. On the other hand, the flight in the fourth row departed a minute early (dep_delay of -1) but arrived 18 minutes early (arr_delay of -18), so its “gained time in the air” is \\(-1 - (-18) = -1 + 18 = 17\\) minutes, hence its gain is +17. Let’s look at some summary statistics of the gain variable by considering multiple summary functions at once in the same summarize() code: gain_summary &lt;- flights %&gt;% summarize( min = min(gain, na.rm = TRUE), q1 = quantile(gain, 0.25, na.rm = TRUE), median = quantile(gain, 0.5, na.rm = TRUE), q3 = quantile(gain, 0.75, na.rm = TRUE), max = max(gain, na.rm = TRUE), mean = mean(gain, na.rm = TRUE), sd = sd(gain, na.rm = TRUE), missing = sum(is.na(gain)) ) gain_summary # A tibble: 1 × 8 min q1 median q3 max mean sd missing &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 -196 -3 7 17 109 5.66 18.0 9430 We see for example that the average gain is +5 minutes, while the largest is +109 minutes! However, this code would take some time to type out in practice. We’ll see later on in Subsection ?? that there is a much more succinct way to compute a variety of common summary statistics: using the skim() function from the skimr package. Recall from Section ?? that since gain is a numerical variable, we can visualize its distribution using a histogram. ggplot(data = flights, mapping = aes(x = gain)) + geom_histogram(color = &quot;white&quot;, bins = 20) Figure 18.6: Histogram of gain variable. The resulting histogram in Figure 18.6 provides a different perspective on the gain variable than the summary statistics we computed earlier. For example, note that most values of gain are right around 0. To close out our discussion on the mutate() function to create new variables, note that we can create multiple new variables at once in the same mutate() code. Furthermore, within the same mutate() code we can refer to new variables we just created. As an example, consider the mutate() code Hadley Wickham and Garrett Grolemund show in Chapter 5 of R for Data Science [@rds2016]: flights &lt;- flights %&gt;% mutate( gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours ) 18.7 arrange and sort rows One of the most commonly performed data wrangling tasks is to sort a data frame’s rows in the alphanumeric order of one of the variables. The dplyr package’s arrange() function allows us to sort/reorder a data frame’s rows according to the values of the specified variable. Suppose we are interested in determining the most frequent destination airports for all domestic flights departing from New York City in 2013: freq_dest &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) freq_dest # A tibble: 105 × 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ABQ 254 2 ACK 265 3 ALB 439 4 ANC 8 5 ATL 17215 6 AUS 2439 7 AVL 275 8 BDL 443 9 BGR 375 10 BHM 297 # ℹ 95 more rows Observe that by default the rows of the resulting freq_dest data frame are sorted in alphabetical order of destination. Say instead we would like to see the same data, but sorted from the most to the least number of flights (num_flights) instead: freq_dest %&gt;% arrange(num_flights) # A tibble: 105 × 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 LEX 1 2 LGA 1 3 ANC 8 4 SBN 10 5 HDN 15 6 MTJ 15 7 EYW 17 8 PSP 19 9 JAC 25 10 BZN 36 # ℹ 95 more rows This is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because arrange() always returns rows sorted in ascending order by default. To switch the ordering to be in “descending” order instead, we use the desc() function as so: freq_dest %&gt;% arrange(desc(num_flights)) # A tibble: 105 × 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ORD 17283 2 ATL 17215 3 LAX 16174 4 BOS 15508 5 MCO 14082 6 CLT 14064 7 SFO 13331 8 FLL 12055 9 MIA 11728 10 DCA 9705 # ℹ 95 more rows 18.8 join data frames Another common data transformation task is “joining” or “merging” two different datasets. For example, in the flights data frame, the variable carrier lists the carrier code for the different flights. While the corresponding airline names for \"UA\" and \"AA\" might be somewhat easy to guess (United and American Airlines), what airlines have codes \"VX\", \"HA\", and \"B6\"? This information is provided in a separate data frame airlines. View(airlines) We see that in airlines, carrier is the carrier code, while name is the full name of the airline company. Using this table, we can see that \"VX\", \"HA\", and \"B6\" correspond to Virgin America, Hawaiian Airlines, and JetBlue, respectively. However, wouldn’t it be nice to have all this information in a single data frame instead of two separate data frames? We can do this by “joining” the flights and airlines data frames. Note that the values in the variable carrier in the flights data frame match the values in the variable carrier in the airlines data frame. In this case, we can use the variable carrier as a key variable to match the rows of the two data frames. Key variables are almost always identification variables that uniquely identify the observational units as we saw in Subsection 6.5.4. This ensures that rows in both data frames are appropriately matched during the join. Hadley and Garrett [@rds2016] created the diagram shown in Figure 18.7 to help us understand how the different data frames in the nycflights13 package are linked by various key variables: Figure 18.7: Data relationships in nycflights13 from R for Data Science. 18.8.1 Matching “key” variable names In both the flights and airlines data frames, the key variable we want to join/merge/match the rows by has the same name: carrier. Let’s use the inner_join() function to join the two data frames, where the rows will be matched by the variable carrier, and then compare the resulting data frames: flights_joined &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(flights) View(flights_joined) Observe that the flights and flights_joined data frames are identical except that flights_joined has an additional variable name. The values of name correspond to the airline companies’ names as indicated in the airlines data frame. A visual representation of the inner_join() is shown in Figure 18.8 [@rds2016]. There are other types of joins available (such as left_join(), right_join(), outer_join(), and anti_join()), but the inner_join() will solve nearly all of the problems you’ll encounter in this book. Figure 18.8: Diagram of inner join from R for Data Science. 18.8.2 Different “key” variable names Say instead you are interested in the destinations of all domestic flights departing NYC in 2013, and you ask yourself questions like: “What cities are these airports in?”, or “Is \"ORD\" Orlando?”, or “Where is \"FLL\"?”. The airports data frame contains the airport codes for each airport: View(airports) However, if you look at both the airports and flights data frames, you’ll find that the airport codes are in variables that have different names. In airports the airport code is in faa, whereas in flights the airport codes are in origin and dest. This fact is further highlighted in the visual representation of the relationships between these data frames in Figure 18.7. In order to join these two data frames by airport code, our inner_join() operation will use the by = c(\"dest\" = \"faa\") argument with modified code syntax allowing us to join two data frames where the key variable has a different name: flights_with_airport_names &lt;- flights %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) View(flights_with_airport_names) Let’s construct the chain of pipe operators %&gt;% that computes the number of flights from NYC to each destination, but also includes information about each destination airport: named_dests &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) %&gt;% arrange(desc(num_flights)) %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% rename(airport_name = name) named_dests # A tibble: 101 × 9 dest num_flights airport_name lat lon alt tz dst tzone &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 ORD 17283 Chicago Ohare Intl 42.0 -87.9 668 -6 A Amer… 2 ATL 17215 Hartsfield Jackson At… 33.6 -84.4 1026 -5 A Amer… 3 LAX 16174 Los Angeles Intl 33.9 -118. 126 -8 A Amer… 4 BOS 15508 General Edward Lawren… 42.4 -71.0 19 -5 A Amer… 5 MCO 14082 Orlando Intl 28.4 -81.3 96 -5 A Amer… 6 CLT 14064 Charlotte Douglas Intl 35.2 -80.9 748 -5 A Amer… 7 SFO 13331 San Francisco Intl 37.6 -122. 13 -8 A Amer… 8 FLL 12055 Fort Lauderdale Holly… 26.1 -80.2 9 -5 A Amer… 9 MIA 11728 Miami Intl 25.8 -80.3 8 -5 A Amer… 10 DCA 9705 Ronald Reagan Washing… 38.9 -77.0 15 -5 A Amer… # ℹ 91 more rows In case you didn’t know, \"ORD\" is the airport code of Chicago O’Hare airport and \"FLL\" is the main airport in Fort Lauderdale, Florida, which can be seen in the airport_name variable. 18.8.3 Multiple “key” variables Say instead we want to join two data frames by multiple key variables. For example, in Figure 18.7, we see that in order to join the flights and weather data frames, we need more than one key variable: year, month, day, hour, and origin. This is because the combination of these 5 variables act to uniquely identify each observational unit in the weather data frame: hourly weather recordings at each of the 3 NYC airports. We achieve this by specifying a vector of key variables to join by using the c() function. Recall from Subsection 6.3.1 that c() is short for “combine” or “concatenate.” flights_weather_joined &lt;- flights %&gt;% inner_join(weather, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;origin&quot;)) View(flights_weather_joined) 18.8.4 Normal forms The data frames included in the nycflights13 package are in a form that minimizes redundancy of data. For example, the flights data frame only saves the carrier code of the airline company; it does not include the actual name of the airline. For example, the first row of flights has carrier equal to UA, but it does not include the airline name of “United Air Lines Inc.” The names of the airline companies are included in the name variable of the airlines data frame. In order to have the airline company name included in flights, we could join these two data frames as follows: joined_flights &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(joined_flights) We are capable of performing this join because each of the data frames have keys in common to relate one to another: the carrier variable in both the flights and airlines data frames. The key variable(s) that we base our joins on are often identification variables as we mentioned previously. This is an important property of what’s known as normal forms of data. The process of decomposing data frames into less redundant tables without losing information is called normalization. More information is available on Wikipedia. Both dplyr and SQL we mentioned in the introduction of this chapter use such normal forms. Given that they share such commonalities, once you learn either of these two tools, you can learn the other very easily. 18.9 Other verbs Here are some other useful data wrangling verbs: select() only a subset of variables/columns. rename() variables/columns to have new names. Return only the top_n() values of a variable. 18.9.1 select variables Figure 18.9: Diagram of select() columns. We’ve seen that the flights data frame in the nycflights13 package contains 19 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package: glimpse(flights) However, say you only need two of these 19 variables, say carrier and flight. You can select() these two variables: flights %&gt;% select(carrier, flight) This function makes it easier to explore large datasets since it allows us to limit the scope to only those variables we care most about. For example, if we select() only a smaller number of variables as is shown in Figure 18.9, it will make viewing the dataset in RStudio’s spreadsheet viewer more digestible. Let’s say instead you want to drop, or de-select, certain variables. For example, consider the variable year in the flights data frame. This variable isn’t quite a “variable” because it is always 2013 and hence doesn’t change. Say you want to remove this variable from the data frame. We can deselect year by using the - sign: flights_no_year &lt;- flights %&gt;% select(-year) Another way of selecting columns/variables is by specifying a range of columns: flight_arr_times &lt;- flights %&gt;% select(month:day, arr_time:sched_arr_time) flight_arr_times This will select() all columns between month and day, as well as between arr_time and sched_arr_time, and drop the rest. The select() function can also be used to reorder columns when used with the everything() helper function. For example, suppose we want the hour, minute, and time_hour variables to appear immediately after the year, month, and day variables, while not discarding the rest of the variables. In the following code, everything() will pick up all remaining variables: flights_reorder &lt;- flights %&gt;% select(year, month, day, hour, minute, time_hour, everything()) glimpse(flights_reorder) Lastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/columns that match those conditions. As examples, flights %&gt;% select(starts_with(&quot;a&quot;)) flights %&gt;% select(ends_with(&quot;delay&quot;)) flights %&gt;% select(contains(&quot;time&quot;)) 18.9.2 rename variables Another useful function is rename(), which as you may have guessed changes the name of variables. Suppose we want to only focus on dep_time and arr_time and change dep_time and arr_time to be departure_time and arrival_time instead in the flights_time data frame: flights_time_new &lt;- flights %&gt;% select(dep_time, arr_time) %&gt;% rename(departure_time = dep_time, arrival_time = arr_time) glimpse(flights_time_new) Note that in this case we used a single = sign within the rename(). For example, departure_time = dep_time renames the dep_time variable to have the new name departure_time. This is because we are not testing for equality like we would using ==. Instead we want to assign a new variable departure_time to have the same values as dep_time and then delete the variable dep_time. Note that new dplyr users often forget that the new variable name comes before the equal sign. 18.9.3 top_n values of a variable We can also return the top n values of a variable using the top_n() function. For example, we can return a data frame of the top 10 destination airports using the example from Subsection 18.8.2. Observe that we set the number of values to return to n = 10 and wt = num_flights to indicate that we want the rows corresponding to the top 10 values of num_flights. See the help file for top_n() by running ?top_n for more information. named_dests %&gt;% top_n(n = 10, wt = num_flights) Let’s further arrange() these results in descending order of num_flights: named_dests %&gt;% top_n(n = 10, wt = num_flights) %&gt;% arrange(desc(num_flights)) 18.10 Conclusion 18.10.1 Summary table Let’s recap our data wrangling verbs in Table 18.2. Using these verbs and the pipe %&gt;% operator from Section 18.2, you’ll be able to write easily legible code to perform almost all the data wrangling and data transformation necessary for the rest of this book. Table 18.2: Summary of data wrangling verbs Verb Data wrangling operation filter() Pick out a subset of rows summarize() Summarize many values to one using a summary statistic function like mean(), median(), etc. group_by() Add grouping structure to rows in data frame. Note this does not change values in data frame, rather only the meta-data mutate() Create new variables by mutating existing ones arrange() Arrange rows of a data variable in ascending (default) or descending order inner_join() Join/merge two data frames, matching rows by a key variable 18.10.2 What’s to come? So far in this book, we’ve explored, visualized, and wrangled data saved in data frames. These data frames were saved in a spreadsheet-like format: in a rectangular shape with a certain number of rows corresponding to observations and a certain number of columns corresponding to variables describing these observations. We’ll see in the upcoming Chapter ?? that there are actually two ways to represent data in spreadsheet-type rectangular format: (1) “wide” format and (2) “tall/narrow” format. The tall/narrow format is also known as “tidy” format in R user circles. While the distinction between “tidy” and non-“tidy” formatted data is subtle, it has immense implications for our data science work. This is because almost all the packages used in this book, including the ggplot2 package for data visualization and the dplyr package for data wrangling, all assume that all data frames are in “tidy” format. Furthermore, up until now we’ve only explored, visualized, and wrangled data saved within R packages. But what if you want to analyze data that you have saved in a Microsoft Excel, a Google Sheets, or a “Comma-Separated Values” (CSV) file? In Section 18.12, we’ll show you how to import this data into R using the readr package. 18.11 Tidying Data In Subsection 6.3.1, we introduced the concept of a data frame in R: a rectangular spreadsheet-like representation of data where the rows correspond to observations and the columns correspond to variables describing each observation. In Section 6.5, we started exploring our first data frame: the flights data frame included in the nycflights13 package. In Chapter ??, we created visualizations based on the data included in flights and other data frames such as weather. In Chapter ??, we learned how to take existing data frames and transform/modify them to suit our ends. In this final chapter of the “Data Science with tidyverse” portion of the book, we extend some of these ideas by discussing a type of data formatting called “tidy” data. You will see that having data stored in “tidy” format is about more than just what the everyday definition of the term “tidy” might suggest: having your data “neatly organized.” Instead, we define the term “tidy” as it’s used by data scientists who use R, outlining a set of rules by which data is saved. Knowledge of this type of data formatting was not necessary for our treatment of data visualization in Chapter ?? and data wrangling in Chapter ??. This is because all the data used were already in “tidy” format. In this chapter, we’ll now see that this format is essential to using the tools we covered up until now. Furthermore, it will also be useful for all subsequent chapters in this book when we cover regression and statistical inference. First, however, we’ll show you how to import spreadsheet data in R. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 6.4 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(readr) library(tidyr) library(nycflights13) library(fivethirtyeight) 18.12 Importing data Up to this point, we’ve almost entirely used data stored inside of an R package. Say instead you have your own data saved on your computer or somewhere online. How can you analyze this data in R? Spreadsheet data is often saved in one of the following three formats: First, a Comma Separated Values .csv file. You can think of a .csv file as a bare-bones spreadsheet where: Each line in the file corresponds to one row of data/one observation. Values for each line are separated with commas. In other words, the values of different variables are separated by commas in each row. The first line is often, but not always, a header row indicating the names of the columns/variables. Second, an Excel .xlsx spreadsheet file. This format is based on Microsoft’s proprietary Excel software. As opposed to bare-bones .csv files, .xlsx Excel files contain a lot of meta-data (data about data). Recall we saw a previous example of meta-data in Section 18.5 when adding “group structure” meta-data to a data frame by using the group_by() verb. Some examples of Excel spreadsheet meta-data include the use of bold and italic fonts, colored cells, different column widths, and formula macros. Third, a Google Sheets file, which is a “cloud” or online-based way to work with a spreadsheet. Google Sheets allows you to download your data in both comma separated values .csv and Excel .xlsx formats. One way to import Google Sheets data in R is to go to the Google Sheets menu bar -&gt; File -&gt; Download as -&gt; Select “Microsoft Excel” or “Comma-separated values” and then load that data into R. A more advanced way to import Google Sheets data in R is by using the googlesheets4 package, a method we leave to a more advanced data science book. We’ll cover two methods for importing .csv and .xlsx spreadsheet data in R: one using the console and the other using RStudio’s graphical user interface, abbreviated as “GUI.” 18.12.1 Using the console First, let’s import a Comma Separated Values .csv file that exists on the internet. The .csv file dem_score.csv contains ratings of the level of democracy in different countries spanning 1952 to 1992 and is accessible at https://moderndive.com/data/dem_score.csv. Let’s use the read_csv() function from the readr [@R-readr] package to read it off the web, import it into R, and save it in a data frame called dem_score. library(readr) dem_score &lt;- read_csv(&quot;https://moderndive.com/data/dem_score.csv&quot;) dem_score # A tibble: 96 × 10 country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Albania -9 -9 -9 -9 -9 -9 -9 -9 5 2 Argentina -9 -1 -1 -9 -9 -9 -8 8 7 3 Armenia -9 -7 -7 -7 -7 -7 -7 -7 7 4 Australia 10 10 10 10 10 10 10 10 10 5 Austria 10 10 10 10 10 10 10 10 10 6 Azerbaijan -9 -7 -7 -7 -7 -7 -7 -7 1 7 Belarus -9 -7 -7 -7 -7 -7 -7 -7 7 8 Belgium 10 10 10 10 10 10 10 10 10 9 Bhutan -10 -10 -10 -10 -10 -10 -10 -10 -10 10 Bolivia -4 -3 -3 -4 -7 -7 8 9 9 # ℹ 86 more rows In this dem_score data frame, the minimum value of -10 corresponds to a highly autocratic nation, whereas a value of 10 corresponds to a highly democratic nation. Note also that backticks surround the different variable names. Variable names in R by default are not allowed to start with a number nor include spaces, but we can get around this fact by surrounding the column name with backticks. We’ll revisit the dem_score data frame in a case study in the upcoming Section 18.14. Note that the read_csv() function included in the readr package is different than the read.csv() function that comes installed with R. While the difference in the names might seem trivial (an _ instead of a .), the read_csv() function is, in our opinion, easier to use since it can more easily read data off the web and generally imports data at a much faster speed. Furthermore, the read_csv() function included in the readr saves data frames as tibbles by default. 18.12.2 Using RStudio’s interface Let’s read in the exact same data, but this time from an Excel file saved on your computer. Furthermore, we’ll do this using RStudio’s graphical interface instead of running read_csv() in the console. First, download the Excel file dem_score.xlsx by going to https://moderndive.com/data/dem_score.xlsx, then Go to the Files pane of RStudio. Navigate to the directory (i.e., folder on your computer) where the downloaded dem_score.xlsx Excel file is saved. For example, this might be in your Downloads folder. Click on dem_score.xlsx. Click “Import Dataset…” At this point, you should see a screen pop-up like in Figure 18.10. After clicking on the “Import” button on the bottom right of Figure 18.10, RStudio will save this spreadsheet’s data in a data frame called dem_score and display its contents in the spreadsheet viewer. Figure 18.10: Importing an Excel file to R. Furthermore, note the “Code Preview” block in the bottom right of Figure 18.10. You can copy and paste this code to reload your data again later programmatically, instead of repeating this manual point-and-click process. 18.13 “Tidy” data Let’s now switch gears and learn about the concept of “tidy” data format with a motivating example from the fivethirtyeight package. The fivethirtyeight package [@R-fivethirtyeight] provides access to the datasets used in many articles published by the data journalism website, FiveThirtyEight.com. For a complete list of all 129 datasets included in the fivethirtyeight package, check out the package webpage by going to: https://fivethirtyeight-r.netlify.app/articles/fivethirtyeight.html. Let’s focus our attention on the drinks data frame and look at its first 5 rows: # A tibble: 5 × 5 country beer_servings spirit_servings wine_servings total_litres_of_pure…¹ &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan 0 0 0 0 2 Albania 89 132 54 4.9 3 Algeria 25 0 14 0.7 4 Andorra 245 138 312 12.4 5 Angola 217 57 45 5.9 # ℹ abbreviated name: ¹​total_litres_of_pure_alcohol After reading the help file by running ?drinks, you’ll see that drinks is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed in 193 countries. This data was originally reported on FiveThirtyEight.com in Mona Chalabi’s article: “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”. Let’s apply some of the data wrangling verbs we learned in Chapter ?? on the drinks data frame: filter() the drinks data frame to only consider 4 countries: the United States, China, Italy, and Saudi Arabia, then select() all columns except total_litres_of_pure_alcohol by using the - sign, then rename() the variables beer_servings, spirit_servings, and wine_servings to beer, spirit, and wine, respectively. and save the resulting data frame in drinks_smaller: drinks_smaller &lt;- drinks %&gt;% filter(country %in% c(&quot;USA&quot;, &quot;China&quot;, &quot;Italy&quot;, &quot;Saudi Arabia&quot;)) %&gt;% select(-total_litres_of_pure_alcohol) %&gt;% rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings) drinks_smaller # A tibble: 4 × 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 Let’s now ask ourselves a question: “Using the drinks_smaller data frame, how would we create the side-by-side barplot in Figure 18.11?”. Recall we saw barplots displaying two categorical variables in Subsection ??. Figure 18.11: Comparing alcohol consumption in 4 countries. Let’s break down the grammar of graphics we introduced in Section ??: The categorical variable country with four levels (China, Italy, Saudi Arabia, USA) would have to be mapped to the x-position of the bars. The numerical variable servings would have to be mapped to the y-position of the bars (the height of the bars). The categorical variable type with three levels (beer, spirit, wine) would have to be mapped to the fill color of the bars. Observe that drinks_smaller has three separate variables beer, spirit, and wine. In order to use the ggplot() function to recreate the barplot in Figure 18.11 however, we need a single variable type with three possible values: beer, spirit, and wine. We could then map this type variable to the fill aesthetic of our plot. In other words, to recreate the barplot in Figure 18.11, our data frame would have to look like this: drinks_smaller_tidy # A tibble: 12 × 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 Italy beer 85 3 Saudi Arabia beer 0 4 USA beer 249 5 China spirit 192 6 Italy spirit 42 7 Saudi Arabia spirit 5 8 USA spirit 158 9 China wine 8 10 Italy wine 237 11 Saudi Arabia wine 0 12 USA wine 84 Observe that while drinks_smaller and drinks_smaller_tidy are both rectangular in shape and contain the same 12 numerical values (3 alcohol types by 4 countries), they are formatted differently. drinks_smaller is formatted in what’s known as “wide” format, whereas drinks_smaller_tidy is formatted in what’s known as “long/narrow” format. In the context of doing data science in R, long/narrow format is also known as “tidy” format. In order to use the ggplot2 and dplyr packages for data visualization and data wrangling, your input data frames must be in “tidy” format. Thus, all non-“tidy” data must be converted to “tidy” format first. Before we convert non-“tidy” data frames like drinks_smaller to “tidy” data frames like drinks_smaller_tidy, let’s define “tidy” data. 18.13.1 Definition of “tidy” data You have surely heard the word “tidy” in your life: “Tidy up your room!” “Write your homework in a tidy way so it is easier to provide feedback.” Marie Kondo’s best-selling book, The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing, and Netflix TV series Tidying Up with Marie Kondo. “I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant What does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized,” the word “tidy” in data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of “tidy” data [@tidy] shown also in Figure 18.12: A dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes. “Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Figure 18.12: Tidy data graphic from R for Data Science. For example, say you have the following table of stock prices in Table 18.3: Table 18.3: Stock prices (non-tidy format) Date Boeing stock price Amazon stock price Google stock price 2009-01-01 $173.55 $174.90 $174.34 2009-01-02 $172.61 $171.42 $170.04 Although the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in “tidy” format. While there are three variables corresponding to three unique pieces of information (date, stock name, and stock price), there are not three columns. In “tidy” data format, each variable should be its own column, as shown in Table 18.4. Notice that both tables present the same information, but in different formats. Table 18.4: Stock prices (tidy format) Date Stock Name Stock Price 2009-01-01 Boeing $173.55 2009-01-01 Amazon $174.90 2009-01-01 Google $174.34 2009-01-02 Boeing $172.61 2009-01-02 Amazon $171.42 2009-01-02 Google $170.04 Now we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider the data in Table 18.5. Table 18.5: Example of tidy data Date Boeing Price Weather 2009-01-01 $173.55 Sunny 2009-01-02 $172.61 Overcast In this case, even though the variable “Boeing Price” occurs just like in our non-“tidy” data in Table 18.3, the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Boeing price, and the Weather that particular day. 18.13.2 Converting to “tidy” data In this book so far, you’ve only seen data frames that were already in “tidy” format. Furthermore, for the rest of this book, you’ll mostly only see data frames that are already in “tidy” format as well. This is not always the case however with all datasets in the world. If your original data frame is in wide (non-“tidy”) format and you would like to use the ggplot2 or dplyr packages, you will first have to convert it to “tidy” format. To do so, we recommend using the pivot_longer() function in the tidyr package [@R-tidyr]. Going back to our drinks_smaller data frame from earlier: drinks_smaller # A tibble: 4 × 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 We convert it to “tidy” format by using the pivot_longer() function from the tidyr package as follows: drinks_smaller_tidy &lt;- drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = -country) drinks_smaller_tidy # A tibble: 12 × 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 China spirit 192 3 China wine 8 4 Italy beer 85 5 Italy spirit 42 6 Italy wine 237 7 Saudi Arabia beer 0 8 Saudi Arabia spirit 5 9 Saudi Arabia wine 0 10 USA beer 249 11 USA spirit 158 12 USA wine 84 We set the arguments to pivot_longer() as follows: names_to here corresponds to the name of the variable in the new “tidy”/long data frame that will contain the column names of the original data. Observe how we set names_to = \"type\". In the resulting drinks_smaller_tidy, the column type contains the three types of alcohol beer, spirit, and wine. Since type is a variable name that doesn’t appear in drinks_smaller, we use quotation marks around it. You’ll receive an error if you just use names_to = type here. values_to here is the name of the variable in the new “tidy” data frame that will contain the values of the original data. Observe how we set values_to = \"servings\" since each of the numeric values in each of the beer, wine, and spirit columns of the drinks_smaller data corresponds to a value of servings. In the resulting drinks_smaller_tidy, the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values. Note again that servings doesn’t appear as a variable in drinks_smaller so it again needs quotation marks around it for the values_to argument. The third argument cols is the columns in the drinks_smaller data frame you either want to or don’t want to “tidy.” Observe how we set this to -country indicating that we don’t want to “tidy” the country variable in drinks_smaller and rather only beer, spirit, and wine. Since country is a column that appears in drinks_smaller we don’t put quotation marks around it. The third argument here of cols is a little nuanced, so let’s consider code that’s written slightly differently but that produces the same output: drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = c(beer, spirit, wine)) Note that the third argument now specifies which columns we want to “tidy” with c(beer, spirit, wine), instead of the columns we don’t want to “tidy” using -country. We use the c() function to create a vector of the columns in drinks_smaller that we’d like to “tidy.” Note that since these three columns appear one after another in the drinks_smaller data frame, we could also do the following for the cols argument: drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = beer:wine) With our drinks_smaller_tidy “tidy” formatted data frame, we can now produce the barplot you saw in Figure 18.11 using geom_col(). This is done in Figure 18.13. Recall from Section ?? on barplots that we use geom_col() and not geom_bar(), since we would like to map the “pre-counted” servings variable to the y-aesthetic of the bars. ggplot(drinks_smaller_tidy, aes(x = country, y = servings, fill = type)) + geom_col(position = &quot;dodge&quot;) Figure 18.13: Comparing alcohol consumption in 4 countries using geom_col(). Converting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the pivot_longer() function is with practice, practice, and more practice using different datasets. For example, run ?pivot_longer and look at the examples in the bottom of the help file. We’ll show another example of using pivot_longer() to convert a “wide” formatted data frame to “tidy” format in Section 18.14. If however you want to convert a “tidy” data frame to “wide” format, you will need to use the pivot_wider() function instead. Run ?pivot_wider and look at the examples in the bottom of the help file for examples. You can also view examples of both pivot_longer() and pivot_wider() on the tidyverse.org webpage. There’s a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a dataset in the weekly #TidyTuesday event that might serve as a nice place for you to find other data to explore and transform. After reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airline companies’ safety records. This data was originally reported on the data journalism website, FiveThirtyEight.com, in Nate Silver’s article, “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s only consider the variables airlines and those relating to fatalities for simplicity: airline_safety_smaller &lt;- airline_safety %&gt;% select(airline, starts_with(&quot;fatalities&quot;)) airline_safety_smaller # A tibble: 56 × 3 airline fatalities_85_99 fatalities_00_14 &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 Aer Lingus 0 0 2 Aeroflot 128 88 3 Aerolineas Argentinas 0 0 4 Aeromexico 64 0 5 Air Canada 0 0 6 Air France 79 337 7 Air India 329 158 8 Air New Zealand 0 7 9 Alaska Airlines 0 88 10 Alitalia 50 0 # ℹ 46 more rows This data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable fatalities_years indicating the incident year and a variable count of the fatality counts? 18.13.3 nycflights13 package Recall the nycflights13 package we introduced in Section 6.5 with data about all domestic flights departing from New York City in 2013. Let’s revisit the flights data frame by running View(flights). We saw that flights has a rectangular shape, with each of its 336,776 rows corresponding to a flight and each of its 22 columns corresponding to different characteristics/measurements of each flight. This satisfied the first two criteria of the definition of “tidy” data from Subsection 18.13.1: that “Each variable forms a column” and “Each observation forms a row.” But what about the third property of “tidy” data that “Each type of observational unit forms a table”? Recall that we saw in Subsection 6.5.3 that the observational unit for the flights data frame is an individual flight. In other words, the rows of the flights data frame refer to characteristics/measurements of individual flights. Also included in the nycflights13 package are other data frames with their rows representing different observational units [@R-nycflights13]: airlines: translation between two letter IATA carrier codes and airline company names (16 in total). The observational unit is an airline company. planes: aircraft information about each of 3,322 planes used, i.e., the observational unit is an aircraft. weather: hourly meteorological data (about 8,705 observations) for each of the three NYC airports, i.e., the observational unit is an hourly measurement of weather at one of the three airports. airports: airport names and locations. The observational unit is an airport. The organization of the information into these five data frames follows the third “tidy” data property: observations corresponding to the same observational unit should be saved in the same table, i.e., data frame. You could think of this property as the old English expression: “birds of a feather flock together.” 18.14 Case study: Democracy in Guatemala In this section, we’ll show you another example of how to convert a data frame that isn’t in “tidy” format (“wide” format) to a data frame that is in “tidy” format (“long/narrow” format). We’ll do this using the pivot_longer() function from the tidyr package again. Furthermore, we’ll make use of functions from the ggplot2 and dplyr packages to produce a time-series plot showing how the democracy scores have changed over the 40 years from 1952 to 1992 for Guatemala. Recall that we saw time-series plots in Section ?? on creating linegraphs using geom_line(). Let’s use the dem_score data frame we imported in Section 18.12, but focus on only data corresponding to Guatemala. guat_dem &lt;- dem_score %&gt;% filter(country == &quot;Guatemala&quot;) guat_dem # A tibble: 1 × 10 country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Guatemala 2 -6 -5 3 1 -3 -7 3 3 Let’s lay out the grammar of graphics we saw in Section ??. First we know we need to set data = guat_dem and use a geom_line() layer, but what is the aesthetic mapping of variables? We’d like to see how the democracy score has changed over the years, so we need to map: year to the x-position aesthetic and democracy_score to the y-position aesthetic Now we are stuck in a predicament, much like with our drinks_smaller example in Section 18.13. We see that we have a variable named country, but its only value is \"Guatemala\". We have other variables denoted by different year values. Unfortunately, the guat_dem data frame is not “tidy” and hence is not in the appropriate format to apply the grammar of graphics, and thus we cannot use the ggplot2 package just yet. We need to take the values of the columns corresponding to years in guat_dem and convert them into a new “names” variable called year. Furthermore, we need to take the democracy score values in the inside of the data frame and turn them into a new “values” variable called democracy_score. Our resulting data frame will have three columns: country, year, and democracy_score. Recall that the pivot_longer() function in the tidyr package does this for us: guat_dem_tidy &lt;- guat_dem %&gt;% pivot_longer(names_to = &quot;year&quot;, values_to = &quot;democracy_score&quot;, cols = -country, names_transform = list(year = as.integer)) guat_dem_tidy # A tibble: 9 × 3 country year democracy_score &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 Guatemala 1952 2 2 Guatemala 1957 -6 3 Guatemala 1962 -5 4 Guatemala 1967 3 5 Guatemala 1972 1 6 Guatemala 1977 -3 7 Guatemala 1982 -7 8 Guatemala 1987 3 9 Guatemala 1992 3 (Note this code differs slightly from our print edition due to an update of the tidyr package to version 1.1.0.) We set the arguments to pivot_longer() as follows: names_to is the name of the variable in the new “tidy” data frame that will contain the column names of the original data. Observe how we set names_to = \"year\". In the resulting guat_dem_tidy, the column year contains the years where Guatemala’s democracy scores were measured. values_to is the name of the variable in the new “tidy” data frame that will contain the values of the original data. Observe how we set values_to = \"democracy_score\". In the resulting guat_dem_tidy the column democracy_score contains the 1 \\(\\times\\) 9 = 9 democracy scores as numeric values. The third argument is the columns you either want to or don’t want to “tidy.” Observe how we set this to cols = -country indicating that we don’t want to “tidy” the country variable in guat_dem and rather only variables 1952 through 1992. The last argument of names_transform tells R what type of variable year should be set to. Without specifying that it is an integer as we’ve done here, pivot_longer() will set it to be a character value by default. We can now create the time-series plot in Figure 18.14 to visualize how democracy scores in Guatemala have changed from 1952 to 1992 using a geom_line(). Furthermore, we’ll use the labs() function in the ggplot2 package to add informative labels to all the aes()thetic attributes of our plot, in this case the x and y positions. ggplot(guat_dem_tidy, aes(x = year, y = democracy_score)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Democracy Score&quot;) Figure 18.14: Democracy scores in Guatemala 1952-1992. Note that if we forgot to include the names_transform argument specifying that year was not of character format, we would have gotten an error here since geom_line() wouldn’t have known how to sort the character values in year in the right order. 18.15 tidyverse package Notice at the beginning of the chapter we loaded the following four packages, which are among four of the most frequently used R packages for data science: library(ggplot2) library(dplyr) library(readr) library(tidyr) Recall that ggplot2 is for data visualization, dplyr is for data wrangling, readr is for importing spreadsheet data into R, and tidyr is for converting data to “tidy” format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you. After installing the tidyverse package as you would a normal package as seen in Section 6.4, running: library(tidyverse) would be the same as running: library(ggplot2) library(dplyr) library(readr) library(tidyr) library(purrr) library(tibble) library(stringr) library(forcats) The purrr, tibble, stringr, and forcats are left for a more advanced book; check out R for Data Science to learn about these packages. For the remainder of this book, we’ll start every chapter by running library(tidyverse), instead of loading the various component packages individually. The tidyverse “umbrella” package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames are in “tidy” format. This standardization of input and output data frames makes transitions between different functions in the different packages as seamless as possible. For more information, check out the tidyverse.org webpage for the package. 18.16 Conclusion If you want to learn more about using the readr and tidyr package, we suggest that you check out RStudio’s “Data Import Cheat Sheet.” In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Browse Cheatsheets” -&gt; Scroll down the page to the “Data Import Cheat Sheet.” The first page of this cheatsheet has information on using the readr package to import data, while the second page has information on using the tidyr package to “tidy” data. You can see a preview of both cheatsheets in the figures below. Figure 18.15: Data Import cheatsheet (first page): readr package. Figure 18.16: Data Import cheatsheet (second page): tidyr package. 18.16.1 What’s to come? Congratulations! You’ve completed the “Data Science with tidyverse” portion of this book. We’ll now move to the “Data modeling with moderndive” portion of this book in Chapters ?? and ??, where you’ll leverage your data visualization and wrangling skills to model relationships between different variables in data frames. However, we’re going to leave Chapter ?? on “Inference for Regression” until after we’ve covered statistical inference in Chapters ??, ??, and ??. Onwards and upwards into Data Modeling as shown in Figure 18.17! Figure 18.17: ModernDive flowchart - on to Part II! "],["module-6-connection.html", "19 Module 6 Connection", " 19 Module 6 Connection "],["module-7-understanding.html", "20 Module 7 Understanding", " 20 Module 7 Understanding "],["module-7-application.html", "21 Module 7 Application", " 21 Module 7 Application "],["module-7-connection.html", "22 Module 7 Connection", " 22 Module 7 Connection "],["module-8-understanding.html", "23 Module 8 Understanding", " 23 Module 8 Understanding "],["module-8-application.html", "24 Module 8 Application", " 24 Module 8 Application "],["module-8-connection.html", "25 Module 8 Connection", " 25 Module 8 Connection "],["module-9-understanding.html", "26 Module 9 Understanding", " 26 Module 9 Understanding "],["module-9-application.html", "27 Module 9 Application", " 27 Module 9 Application "],["module-9-connection.html", "28 Module 9 Connection", " 28 Module 9 Connection "],["module-10-understanding.html", "29 Module 10 Understanding", " 29 Module 10 Understanding "],["module-10-application.html", "30 Module 10 Application", " 30 Module 10 Application "],["module-10-connection.html", "31 Module 10 Connection", " 31 Module 10 Connection "],["module-11-understanding.html", "32 Module 11 Understanding", " 32 Module 11 Understanding "],["module-11-application.html", "33 Module 11 Application", " 33 Module 11 Application "],["module-11-connection.html", "34 Module 11 Connection", " 34 Module 11 Connection "],["module-12-understanding.html", "35 Module 12 Understanding", " 35 Module 12 Understanding "],["module-12-application.html", "36 Module 12 Application", " 36 Module 12 Application "],["module-12-connection.html", "37 Module 12 Connection", " 37 Module 12 Connection "],["module-13-understanding.html", "38 Module 13 Understanding", " 38 Module 13 Understanding "],["module-13-application.html", "39 Module 13 Application", " 39 Module 13 Application "],["module-13-connection.html", "40 Module 13 Connection", " 40 Module 13 Connection "],["module-14-understanding.html", "41 Module 14 Understanding", " 41 Module 14 Understanding "],["module-14-application.html", "42 Module 14 Application", " 42 Module 14 Application "],["module-14-connection.html", "43 Module 14 Connection", " 43 Module 14 Connection "],["module-15-understanding.html", "44 Module 15 Understanding", " 44 Module 15 Understanding "],["module-15-application.html", "45 Module 15 Application", " 45 Module 15 Application "],["module-15-connection.html", "46 Module 15 Connection", " 46 Module 15 Connection "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
