[["index.html", "Introduction to Data Science A Remixed Textbook for ICT/LIS 661 at the Uniersity of Kentucky [Fall 2023 Edition] 1 Introduction", " Introduction to Data Science A Remixed Textbook for ICT/LIS 661 at the Uniersity of Kentucky [Fall 2023 Edition] Spencer P. Greenhalgh, PhD 1 Introduction "],["module-1-understanding.html", "2 Module 1 Understanding", " 2 Module 1 Understanding "],["m1a-install-r-and-rstudio.html", "3 M1A: Install R and RStudio 3.1 Introduction 3.2 R 3.3 RStudio 3.4 References", " 3 M1A: Install R and RStudio This content draws on material from STAT 545 by Jenny Bryan, licensed under CC BY-SA 4.0 Changes to the source material include addition of new material; light editing; rearranging, removing, and combining original material; adding and changing links; and adding first-person language from current author. The resulting content is licensed under CC BY-SA 4.0. 3.1 Introduction My formal training is in education, so I have some strong opinions about what learning looks like and what good teaching ought to look like. In particular, I hold to a sociocultural view of learning that assumes that: knowledge is distributed in the world among individuals, the tools, artifacts, and books that they use, and the communities and practices in which they participate (Greeno et al., 1996, p. 20) In other words, I can’t teach you data science by merely rattling off a list of facts for you to memorize and then repeat at the appropriate time. Rather, if I’m going to effectively teach you data science, I need to introduce you to data science communities, have you use the tools that data scientists use, and have you act in the way that data scientists act. In relation to this second point, R and RStudio are software that are widely used in the world of data science, so becoming familiar with them is part of learning data science. Some data scientists prefer other software, and that’s fine, but this is what we’ve decided on teaching here in UK’s School of Information Science (and it’s what I personally use, so I’m better suited to teaching it anyway). This activity is about introducing you to this software and helping you set it up. Even if you have a pre-existing installation of R or RStudio, I highly recommend that you re-install both and get as current as possible. It can be considerably harder to run old software than new. 3.2 R R is an open source programming language designed for statistics. Two things are important about that initial description: First, R is open source, meaning that is freely available and that other programmers may add to it or modify it to their heart’s content. This is good news—it means that in addition to the basic features of R, it is possible (and relatively easy) to add new features by installing and loading packages. We’ll be doing plenty of that this semester. Second, R is designed for statistics. That doesn’t mean it can’t be used for other things: In my research, I regularly use R, but I rarely use it for traditional statistics. Nonetheless, R is built with statistical needs and tasks in mind. You can (and should now) install R from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system (as opposed to the source code). Follow the link for your operating system at the CRAN link in this paragraph. (You’ll probably notice that version names for R are… eccentric!). 3.3 RStudio Programming in R can be done in a number of ways, but in this class, we’ll be using an IDE (integrated development environment) called RStudio (developed by an organization called Posit). To download RStudio Desktop, navigate to this this link. It will provide you with a link for downloading R; since you’ve already done that, you can ignore it. What you shouldn’t ignore, though, is the link it will provide to download RStudio for your computer’s operatins system. It’s important to understand that RStudio is one of many interfaces available for working with the R programming language. Another interface (simply called R) will also be installed on your computer as a result of installing the R language, and it will be possible to open R code in either the R interface or the RStudio interface. Make sure that you always open code in RStudio—not only do I find it cleaner and easier to work with code there, but RStudio also has some extra features that we’ll be using throughout the semester. 3.4 References Greeno, J., Collins, A., &amp; Resnick, L. (1996). Cognition and learning. In D. Berliner &amp; R. Calfee (eds.), Handbook of educational psychology (pp. 15-46). Macmillan. "],["m1c-introduce-yourself-to-the-class.html", "4 M1C: Introduce Yourself to the Class", " 4 M1C: Introduce Yourself to the Class You will complete the Module 1 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt: One of the hardest parts about teaching online classes is getting to know my students! For this week’s connection activity, please introduce yourself to me and to your classmates. You might include details like your pronouns, what program you’re in, your current (or expected future) job, and what you like to do for fun. I’d also be interested to hear why you’re taking this class, what you’re hoping to learn, and any big questions or concerns that you have. As with all discussion posts in this course, I strongly encourage you to read over and respond to what your classmates have posted. However, as important as meaningful interaction is, I’m skeptical that it can be structured or required, so I will never require you to do so. That said, I do expect that you will make an effort to contribute to meaningful interaction within the class. To help with this, I’m experimenting with the “like” feature in Canvas discussions (and with sorting posts by the number of “likes” they receive, though you won’t be able to see others’ posts before adding your own). Feel free to use “liking” to add to this interaction! "],["m2u-the-new-and-shiny-science-of-data.html", "5 M2U: The New(?) and Shiny(?) Science of Data 5.1 Data, Data Science, and Big Data 5.2 The Many Skills of Data Science 5.3 References", " 5 M2U: The New(?) and Shiny(?) Science of Data This chapter draws on material from: Introduction: Why Data Science Needs Feminism by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0 1: The Power Chapter by Catherine D’Ignazio and Lauren Klein, licensed under CC BY 4.0 Version 3: An Introduction to Data Science by Jeffrey Stanton, licensed under CC BY-NC-SA 3.0 Changes to the source material include adding new material; editing, reformatting, and rearranging of original material; adding links; adding or replacing images; changing the citation style; changing original authors’ voice to third person; and adding first-person language from current author. The resulting content is licensed under CC BY-NC-SA 3.0. 5.1 Data, Data Science, and Big Data Data science refers to an emerging area of work concerned with the collection, preparation, analysis, visualization, management, and preservation of large collections of information, sometimes referred to as big data. 5.1.1 Data To introduce data science, it makes sense that we ought to talk about data first. The word data is the plural of the the Latin word datum. One quick word before we continue: Because the word data is the plural of datum, I (and many people) prefer data as a plural noun—hence “What are Data?” for the section title. (In fact, I think it’s funny to define data science as “the science of datums,” but that’s a terrible joke and I promise I won’t do it again in this book). However, it’s quite common in American English to treat data as a singular word—so common in fact, that you might notice me trip up and write “What is Data?” at some point. My opinion here is strong enough that I won’t mind if you point out when I’m inconsistent but not so strong that I’m going to get picky about how you treat the word—go with whatever comes more naturally to you. Even though we rarely use the singular datum, it’s worth briefly exploring its etymology. The word means “a given”—that is, something taken for granted. That’s important: The word data was introduced in the mid-seventeenth century to supplement existing terms such as evidence and fact. Identifying information as data, rather than as either of those other two terms, served a rhetorical purpose (Poovey, 1998; Posner &amp; Klein, 2017; Rosenberg, 2013). It converted otherwise debatable information into the solid basis for subsequent claims. Modern usage of the word data started in the 1940s and 1950s as practical electronic computers began to input, process, and output data. When computers work with data, all of that data has to be broken down to individual bits as the “atoms” that make up data. A bit is a binary unit of data, meaning that it is only capable of representing one of two values: 0 and 1. That doesn’t carry a lot of information by itself (at best, “yes” vs. “no” or TRUE vs. FALSE). However, by combining bits, we can increase the amount of information that we transmit. For example, even a combination of just two bits can express four different values: 00, 01, 10 and 11. Every time you add a new bit you double the number of possible messages you can send. So three bits would give eight options and four bits would give 16 options. When we get up to eight bits—which provides 256 different combinations—we finally have something of a reasonably useful size to work with. Eight bits is commonly referred to as a byte—this term probably started out as a play on words with the word bit (and four bits is sometimes referred to as a nibble or a nybble, because nerds like jokes). A byte offers enough different combinations to encode all of the letters of the (English) alphabet, including capital and small letters. There is an old rulebook called ASCII—the American Standard Code for Information Interchange—which matches up patterns of eight bits with the letters of the alphabet, punctuation, and a few other odds and ends. For example the bit pattern 0100 0001 represents the capital letter A and the next higher pattern 0100 0010 represents capital B. This is more background than anything else—most of the time (but not all of the time!) you don’t need to know the details of what’s going on here to carry out data science. However, it is important to have a foundational understanding that when we’re working with data in this class, the computer is ultimately dealing with everything as bits and translating combinations of bits into words, pictures, numbers, and other formats that makes sense for humans. This background is also helpful for pointing out that just like the word data has connotations related to trustworthiness, it also has connotations of things that are digital and quantitative. While all of these connotations are reasonable, it’s important that we understand their limits. For example, while many people think of data as numbers alone, data can also consist of words or stories, colors or sounds, or any type of information that is systematically collected, organized, and analyzed. Some folks might resist that broad definition of data because “words or stories” told by a person don’t feel as trustworthy or objective as numbers stored in a computer. However, one of the recurring themes of this course is to emphasize that data and data systems are not objective—even when they’re digital and quantitative. When I was introducing ASCII a few paragraphs ago, there were two details in there that might have passed you by but that actually have pretty important consequences. First, I noted that ASCII can “encode all the letters of the (English) alphabet)”; second, I mentioned that the “A” in ASCII stood for “American.” Early computer systems in the United States were built around American English assumptions for what counts as a letter. This makes sense… but it has had consequences! While most modern computer systems have moved on to more advanced character encoding systems (ones that include Latin letters, Chinese characters, Arabic script, and emoji, for example), there are still some really important computer systems that use limited encoding schemes like ASCII. In 2015, Tovin Lapin wrote a newspaper article about this, noting that: Every year in California thousands of parents choose names such as José, André, and Sofía for their children, often honoring the memory of a deceased grandmother, aunt or sibling. On the state-issued birth certificates, though, those names will be spelled incorrectly. California, like several other states, prohibits the use of diacritical marks or accents on official documents. That means no tilde (~), no accent grave (`), no umlaut (¨) and certainly no cedilla (¸). Although more than a third of the state population is Hispanic, and accents are used in the names of state parks and landmarks, the state bars their use on birth records. There were attempts in 2014 to change this, but when lawmakers realized it would cost $10 million to update computer systems, things stalled. Moral of the story: even though ASCII is a straightforward technical system built on digital data with no real wiggle room for what means what, it’s still subjective and biased. How we organize data and data systems matters! So, even digital and quantitative data (systems) can be biased, which means that we ought to push lightly back against the rhetorical connotations of data as trustworthy. I’m not suggesting we throw data, science, and data science out the window and go with our gut and our opinions, but we shouldn’t take for granted that a given dataset doesn’t have its own subjectivity. Likewise, we ought to ask ourselves what information needs to become data before it can be trusted—or, more precisely, whose information needs to become data before it can be considered as fact and acted upon (Lanius, 2015; Porter, 1996). 5.1.2 Data Science If you think about it, data science is somewhat of an unintuitive name. As we noted earlier, data are more than numbers: they can be any type of information that is systematically collected, organized, and analyzed. Likewise, science simply implies a commitment to systematic methods of observation and experiment. Given these definitions, data science clearly means something more limited than science that uses data. History, for example, is a commitment to systematic methods of observation and relies on information that is systematically collected, organized, and analyzed, but we (generally) shouldn’t expect a historian to describe themselves (or be accepted) as a data scientist. Also relevant here is a critique by data and statistics expert Nate Silver, who once quipped that: I think data-scientist is a sexed up term for a statistician (Stats &amp; Data Science Views, 2013) There’s a certain amount of hype, prestige, and even sexiness associated with the term data science, and this ought to encourage us to be critical about how the term is used. Is data science just statistics gussied up to sound cooler? If all scientists use data, who is allowed to have access to the hype, prestige, and sexiness associated with data science? It is also important to acknowledge the elephant in the server room: the demographics of data science (and related occupations like software engineering and artificial intelligence research) do not represent the population as a whole. According to 2018 data from the US Bureau of Labor Statistics, released in 2018, only 26 percent of those in “computer and mathematical occupations” are women (US Bureau of Labor Statistics, 2019). Across all of those women, only 12 percent are Black or Latinx women, even though Black and Latinx women make up 22.5 percent of the US population. (Women of Color in Computing Collaborative, 2018). A report by the research group AI Now about the diversity crisis in artificial intelligence notes that women comprise only 15 percent of AI research staff at Facebook and 10 percent at Google (Myers, Whittaker, &amp; Crawford, 2019). These numbers are probably not a surprise. The more surprising thing is that those numbers are getting worse, not better. According to a research report published by the American Association of University Women in 2015, women computer science graduates in the United States peaked in the mid-1980s at 37 percent, and we have seen a steady decline in the years since then to 26 percent today (Corbett &amp; Hill, 2015). As “data analysts” (low-status number crunchers) have become rebranded as “data scientists” (high status researchers), women are being pushed out in order to make room for more highly valued and more highly compensated men (Fouad, 2014). Disparities in the higher education pipeline aren’t only along gender lines. The same report noted specific underrepresentation for Native American women, multiracial women, white women, and all Black and Latinx people. So is it really a surprise that each day brings a new example of data science being used to disempower and oppress minoritized groups? In 2018, it was revealed that Amazon had been developing an algorithm to screen its first-round job applicants. But because the model had been trained on the resumes of prior applicants, who were predominantly male, it developed an even stronger preference for male applicants. It downgraded resumes with the word women and graduates of women’s colleges. Ultimately, Amazon had to cancel the project (Gershgorn, 2018; Kraus, 2018). This example reinforces the work of Safiya Umoja Noble (2018), whose book, Algorithms of Oppression, has shown how both gender and racial biases are encoded into some of the most pervasive data-driven systems—including Google search, which boasts over five billion unique web searches per day. Noble describes how, as recently as 2016, comparable searches for “three Black teenagers” and “three white teenagers” turned up wildly different representations of those teens. The former returned mugshots, while the latter returned wholesome stock photography. Unequal representation among data scientists is all the worse for the way it can lead to biases in data science projects. 5.1.3 Big Data One possible distinction between data science and statistics is the amount of data we’re working with. Technology coverage in the 2010s (and continuing to the present) made it hard to resist the idea that big data represents some kind of revolution that has turned the whole world of information and technology topsy-turvy. But is this really true? Does big data change everything? Business analyst Doug Laney suggested that three characteristics make big data different from what came before: volume, velocity, and variety. Volume refers to the sheer amount of data. Velocity focuses on how quickly data arrives as well as how quickly those data become “stale.” Finally, variety reflects the fact that there may be many different kinds of data. Together, these three characteristics are often referred to as the “three Vs” model of big data. Note, however, that even before the dawn of the computer age we’ve had a variety of data, some of which arrives quite quickly, and that can add up to quite a lot of total storage over time. Think, for example, of the large variety and volume of data that has arrived annually at Library of Congress since the 1800s! So, it is difficult to tell that big data is fundamentally a brand new thing. Furthermore, there are some concerns that we should exercise when it comes to big data. For example, when a data set gets to a certain size (into the range of thousands of rows), conventional tests of statistical significance are meaningless, because even the most tiny and trivial results are statistically significant. We’ll talk more about statistical significance later in the semester; for the time being, though, it suffices to say that statistical significance is how researchers have traditionally determined whether their results are important or not. If big data makes statistical significance more likely, then researchers who have access to more data will get more important results, whether or not that’s actually true in practical terms! Besides that, the quality and suitability of the data matters a lot: More data does not always mean better data. 5.2 The Many Skills of Data Science Data science owes a lot to statistics and mathematics, so you’d be forgiven for thinking of a data scientist as a statistician in a white lab coat staring fixedly at blinking computer screens filled with scrolling numbers. This isn’t quite the case, though: There is much to be accomplished in the world of data science for those of us who are more comfortable working with words, lists, photographs, sounds, and other kinds of information. In addition, data science is much more than simply analyzing data. There are many people who enjoy analyzing data and who could happily spend all day looking at histograms and averages, but for those who prefer other activities, data science offers a range of roles and requires a range of skills. Here are some skills that are particularly useful: Learning the application domain: A data scientist must quickly learn how the data will be used in a particular context. Communicating with data users: A data scientist must possess strong skills for learning the needs and preferences of users. Translating back and forth between the technical terms of computing and statistics and the vocabulary of the application domain is a critical skill. Seeing the big picture of a complex system: After developing an understanding of the application domain, a data scientist must imagine how data will move around among all of the relevant systems and people. Knowing how data can be represented: A data scientist must have a clear understanding about how data can be stored and linked, as well as about “metadata” (data that describes how other data are arranged). Data transformation and analysis: When data become available for the use of decision makers, a data scientist must know how to transform, summarize, and make inferences from the data. As noted above, being able to communicate the results of analyses to users is also a critical skill here. Visualization and presentation: Although numbers often have the edge in precision and detail, a good data display (e.g., a bar chart) can often be a more effective means of communicating re- sults to data users. Attention to quality: No matter how good a set of data may be, there is no such thing as perfect data. A data scientist must know the limitations of the data they work with, know how to quantify its accuracy, and be able to make suggestions for improving the quality of the data in the future. Ethical reasoning: If data are important enough to collect, they are often important enough to affect people’s lives. A data scientist must understand important ethical issues such as privacy and must be able to communicate the limitations of data to try to prevent misuse of data or analytical results. While a keen understanding of numbers and mathematics is important, particularly for data analysis, a data scientist also needs to have excellent communication skills, be a great systems thinker, have a good eye for visual displays, and be highly capable of thinking critically about how data will be used to make decisions and affect people’s lives. Of course there are very few people who are good at all of these things, so some of the people interested in data will specialize in one area, while others will become experts in another area. Of course, this also highlights the importance of teamwork. We can’t possibly cover all these skills in depth this semester, and even these skills are just the tip of the iceberg, which just emphasizes what a wide range is represented here. I hope their importance is clear, though—for example, which of these skills could have anticipated and responded to the problems involved with María showing up as Maria on a California birth certificate? 5.3 References Corbett, C., &amp; Hill, C. (2015). Solving the equation: The variables for women’s success in engineering and computing. American Association of University Women. Fouad, N. A. (2014, August). Learning in, but getting pushed back (and out). Paper presented at the American Psychological Association. https://www.apa.org/news/press/releases/2014/08/pushed-back.pdf Gershgorn, D. (2018, October 22). Companies are on the hook if their hiring algorithms are biased. Quartz. https://qz.com/1427621/companies-are-on-the-hook-if-their-hiring-algorithms-are-biased Kraus, R. (2018, October 10). Amazon used AI to promote diversity. Too bad it’s plagued with gender bias. Mashable. https://mashable.com/article/amazon-sexist-recruiting-algorithm-gender-bias-ai#VSsbMcGmvqqa Lanius, C. (2015, January 12). Fact check: Your demand for statistical proof is racist. Cyborgology. https://thesocietypages.org/cyborgology/2015/01/12/fact-check-your-demand-for-statistical-proof-is-racist/ Lapan, T. (2015, April 11). California birth certificates and accents: O’Connor alright, Ramón and José is not. The Guardian. https://www.theguardian.com/us-news/2015/apr/11/california-birth-certifcates-accents-marks Myers, S. W., Whittaker, M., &amp; Carwford, K. (2019). Discriminating systems: Gender, race and power in AI. AI Now Institute. https://ainowinstitute.org/publication/discriminating-systems-gender-race-and-power-in-ai-2 Noble, S. U. (2018). Algorithms of oppression: How search engines reinforce racism. NYU Press. Poovey, M. (1998). A history of the modern fact: Problems of knowledge in the sciences of wealth and society. University of Chicago Press. Porter, T. M. (1996). Trust in numbers: The pursuit of objectivity in science and public life. Princeton University Press. Posner, M., &amp; Klein, L. F. (2017). Editor’s introduction: Data as media. Feminist Media Histories, 3(3), 1-8. Rosenberg, D. (2013). Data before the fact. In L. Gitelman (Ed.), “Raw” data is an oxymoron. MIT Press. Stats &amp; Data Science Views. (2013, August 23). Nate Silver: What I need from statisticians [blog post]. https://www.statisticsviews.com/article/nate-silver-what-i-need-from-statisticians/ US Bureau of Labor Statistics (2019). BLS Data Viewer. https://beta.bls.gov/dataViewer/view/timeseries/LNU02070002Q Women of Color in Computing Collaborative. (2018). Data brief: Women and girls of color in computing. https://www.wocincomputing.org/wp-content/uploads/2018/08/WOCinComputingDataBrief.pdf "],["getting-started.html", "6 M2A: Getting Started with Data in R 6.1 Introduction 6.2 What are R and RStudio? 6.3 How do I code in R? 6.4 What are R packages? 6.5 Explore your first datasets 6.6 Conclusion", " 6 M2A: Getting Started with Data in R This content draws on material from Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim, licensed under CC BY-NC-SA 4.0 Changes to the source material include light editing of original material, removing original material, changing citation style, adding new material, and replacing original authors’ “we” with an “I” for the current author. The resulting content is licensed under CC BY-NC-SA 4.0. 6.1 Introduction Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? If you are already somewhat familiar with these concepts, feel free to skip to Section 6.5 where we’ll introduce our first dataset: all domestic flights departing one of the three main New York City (NYC) airports in 2013. This is a dataset we will explore in depth for much of the rest of this book. 6.2 What are R and RStudio? Throughout this book, I will expect that you are using R via RStudio. First time users often confuse the two. At its simplest, R is like a car’s engine while RStudio is like a car’s dashboard as illustrated in Figure 6.1. Figure 6.1: Analogy of difference between R and RStudio. More precisely, R is a programming language that does the actual work of computing and calculating, while RStudio is (as we covered last week) an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well. 6.2.1 Using R via RStudio Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we will use RStudio’s interface instead of interacting with R directly. When you installed R and RStudio on your computer last week, you added two new programs (also called applications) you can open. We’ll always work in RStudio and not in the R application. Figure 6.2 shows what icon you should be clicking on your computer. Figure 6.2: Icons of R versus RStudio on your computer. After you open RStudio, you should see something similar to Figure 6.3. (Note that there may be slight differences, since the interface is updated over time.) Figure 6.3: RStudio interface to R. Note the three panes (that is, the three panels dividing the screen): the console pane, the files pane, and the environment pane. Over the course of this chapter, you’ll learn what purpose each of these panes serves. 6.3 How do I code in R? Now that you’re set up with R and RStudio, you are probably asking yourself, “OK, so how do I actually use R?”. The first thing to note is that while other statistical software programs like Excel, SPSS, or Minitab provide point-and-click interfaces, R is an interpreted language. This means you have to type in commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book. It is tempting to think that point-and-click software is superior to software that requires you to write your own code. We’re used to thinking of point-and-click software as newer and more modern than writing text commands! However, writing your own code typically gives you more control and power over what your software is doing; plus, by asking you to be more involved in the statistical analysis, it actually encourages you to learn more along the way. Speaking personally, I found that I learned a lot more about statistical analysis once I started using R than I had when I was using point-and-click interfaces. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that new R users need to understand. Consequently, while this course is not a course on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 6.3.1 Basic programming concepts and terminology In my opinion and experience, completing actual projects in R does much more to help you learn the software than a written description. That is, rather than ask you to memorize terms and concepts, I’ll ask you throughout the semester to complete walkthroughs (like this one) so that you’ll “learn by doing.” One important thing to help you with any of these walkthroughs is text formatting: I will always use a different font to distinguish regular text from computer_code. That said, it can be helpful to lay out some basic definitions and describe some basic concepts; don’t worry about committing all of this to memory, but you can come back here for clarification if you aren’t sure what a term means later on. Basics: console pane: where you enter in commands. running code: the act of telling R to perform an act by giving it commands in the console. objects: where values are saved in R. We’ll show you how to assign values to objects and how to display the contents of objects. data types: integers, doubles/numerics, logicals, and characters. : integers are values like -1, 0, 2, 4092 doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8 logicals are either TRUE or FALSE characters are text such as “muesli”, “They Might Be Giants”, “Homestar Runner is the greatest thing ever”, and “this chocolate mint tea is delicious”; note that characters are often denoted with the quotation marks around them. Vectors: a series of values. These are created using the c() function, where c() stands for “combine” or “concatenate.” For example, c(6, 11, 13, 31, 90, 92) creates a six element series of positive integer values . Factors: categorical data are commonly represented in R as factors. Categorical data can also be represented as strings. We’ll study this difference as we progress through the book. Data frames: rectangular spreadsheets. They are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. We’ll cover data frames later in Section 6.5. Conditionals: You need to test for equality in R using == (and not =, which is typically used for assignment). For example, 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). For example, 4 + 2 &gt;= 3 will return TRUE, but 3 + 5 &lt;= 1 will return FALSE. Logical operators: &amp; representing “and” as well as | representing “or.” For example, (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function’s arguments or use the function’s default values. For example, the function seq() in R generates a sequence of numbers. If you just run seq() it will return the value 1. That doesn’t seem very useful! This is because the default arguments are set as seq(from = 1, to = 1). Thus, if you don’t pass in different values for from and to to change this behavior, R just assumes all you want is the number 1. You can change the argument values by updating the values after the = sign. If we try out seq(from = 2, to = 5) we get the result 2 3 4 5 that we might expect. We’ll work with functions a lot throughout this book and you’ll get lots of practice in understanding their behaviors. To further assist you in understanding when a function is mentioned in the book, I’ll also include the () after them as I did with seq() above. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, I feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. You can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more. 6.3.2 Errors, warnings, and messages One thing that intimidates new R and RStudio users is how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and will try to explain what went wrong. Generally when there’s an error, the code will not run. For example, we’ll see in Subsection 6.4.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter ?? if you create a scatterplot based on a dataset where two of the rows of data have missing entries that would be needed to create points in the scatterplot, you will see this warning: Warning: Removed 2 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren’t there. Messages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages in the upcoming Subsection 6.4.2 or when you read data saved in spreadsheet files with the read_csv() function as you’ll see in Chapter ??. These are helpful diagnostic messages and they don’t stop your code from working. Additionally, you’ll see these messages when you install packages too using install.packages() as discussed in Subsection 6.4.1. Remember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather: If the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong! If the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise, the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine and keep on going! 6.3.3 Tips on learning to code Learning to code/program is quite similar to learning a foreign language. It can be daunting and frustrating at first. Such frustrations are common and it is normal to feel discouraged as you learn. However, just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn and improve. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone is “smart,” but really people spent a lot of time and energy designing them to appear “smart.” In reality, you have to tell a computer everything it needs to do. Furthermore, the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way. Take the “copy, paste, and tweak” approach: Especially when you learn your first programming language or you need to understand particularly complicated code, it is often much easier to take existing code that you know works and modify it to suit your ends. This is as opposed to trying to type out the code from scratch. I call this the “copy, paste, and tweak” approach. So early on, I suggest not trying to write code from memory, but rather take existing examples from the book, then copy, paste, and tweak them to suit your goals. In fact, this will be an explicit part of many activities! After you start feeling more confident, you can slowly move away from this approach and write code from scratch. Think of the “copy, paste, and tweak” approach as training wheels for a child learning to ride a bike. After getting comfortable, they won’t need them anymore. The best way to learn to code is by doing: Rather than learning to code for its own sake, I find that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in and that is important to you. Practice is key: Just as the only method to improve your foreign language skills is through lots of practice and speaking, the only method to improving your coding skills is through lots of practice. Don’t worry, however, you’ll have plenty of opportunities to do so! 6.4 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization in Chapter ??, the dplyr package for data wrangling in Chapter ??, the moderndive package that accompanies this book these walkthroughs are taken from, and the infer package for “tidy” and transparent statistical inference in Chapters ??, ??, and ??. A helpful analogy for R packages is they are like apps you can download onto a mobile phone: Figure 6.4: Analogy of R versus R packages. In this analogy, R is like a new smartphone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play. Let’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set for the time being. You might need to do this again in the future when there is an update to the app. Open the app: Installing Instagram is great, but just because you’ve installed it doesn’t mean that you can use it. To use it, you need to open it; in fact, every time you want to use it, you need to open it. Once Instagram is on your phone and opened, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version. “Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio. Let’s perform these two steps for the ggplot2 package for data visualization. 6.4.1 Package installation There are two ways to install an R package: an easy way and a more advanced way. Let’s install the ggplot2 package the easy way first as shown in Figure 6.5. In the Files pane of RStudio: Click on the “Packages” tab. Click on “Install” next to Update. Type the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2. Click “Install.” Figure 6.5: Installing packages in R the easy way. An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package. Let’s recap our analogy from earlier, because it’s important: Much like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps. Now, on your own, repeat the earlier installation steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package for data wrangling, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for generating easy-to-read tables in R. We’ll use these packages in the next section. Note that your output might be slightly different than the output displayed throughout the book, because packages get updated over time. This typically won’t be cause for concern, but if you’re worried about something, you can reach out to me and I can take a look. 6.4.2 Package loading Recall that after you’ve installed a package, you need to “load it.” In other words, you need to “open it.” We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the console pane in RStudio. What do we mean by “run the following code”? Either type or copy-and-paste the following code into the console pane and then hit the Enter key. library(ggplot2) If after running the earlier code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If, however, you get a red “error message” that reads… Error in library(ggplot2) : there is no package called ‘ggplot2’ … it means that you didn’t successfully install it. This is an example of an “error message” we discussed in Subsection 6.3.2. If you get this error message, go back to Subsection 6.4.1 on R package installation and make sure to install the ggplot2 package before proceeding. Now, “load” the dplyr, nycflights13, and knitr packages as well by repeating the earlier steps. 6.4.3 Package use One very common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to: Error: could not find function This is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been “loaded.” R doesn’t know where to find the function you are using. Almost all new users forget to do this when starting out, and it is a little annoying to get used to doing it. However, you’ll remember with practice and after some time it will become second nature for you. 6.5 Explore your first datasets Let’s put everything we’ve learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we’ll focus on datasets that are saved in “spreadsheet”-type format. This is probably the most common way data are collected and saved in many fields. Remember from Subsection 6.3.1 that these “spreadsheet”-type datasets are called data frames in R. We’ll focus on working with data saved as data frames throughout this book. Let’s first load all the packages needed for this chapter, assuming you’ve already installed them. Read Section 6.4 for information on how to install and load R packages if you haven’t already. library(nycflights13) library(dplyr) library(knitr) At the beginning of all subsequent chapters in this book, we’ll always have a list of packages that you should have installed and loaded in order to work with that chapter’s R code. 6.5.1 nycflights13 package Many of us have flown on airplanes or know someone who has. Air travel has become an ever-present aspect of many people’s lives. If you look at the Departures flight information board at an airport, you will frequently see that some flights are delayed for a variety of reasons. Are there ways that we can understand the reasons that cause flight delays? We’d all like to arrive at our destinations on time whenever possible. (Unless you secretly love hanging out at airports. If you are one of these people, pretend for a moment that you are very much anticipating being at your final destination.) Throughout this book, we’re going to analyze data related to all domestic flights departing from one of New York City’s three main airports in 2013: Newark Liberty International (EWR), John F. Kennedy International (JFK), and LaGuardia Airport (LGA). We’ll access this data using the nycflights13 R package, which contains five datasets saved in five data frames: flights: Information on all 336,776 flights. airlines: A table matching airline names and their two-letter International Air Transport Association (IATA) airline codes (also known as carrier codes) for 16 airline companies. For example, “DL” is the two-letter code for Delta. planes: Information about each of the 3,322 physical aircraft used. weather: Hourly meteorological data for each of the three NYC airports. This data frame has 26,115 rows, roughly corresponding to the \\(365 \\times 24 \\times 3 = 26,280\\) possible hourly measurements one can observe at three locations over the course of a year. airports: Names, codes, and locations of the 1,458 domestic destinations. 6.5.2 flights data frame We’ll begin by exploring the flights data frame and get an idea of its structure. Run the following code in your console, either by typing it or by copying-and-pasting it. Make sure that you only type (or copy-and-paste) the top of the two boxes below this paragraph. When walkthroughs in this book provide you with example code, there will typically be two boxes: The first will provide the code, and the second will provide the output from that code (preceded with ## before each line to show that it’s output, not code). In this case, that’s the contents of the flights data frame, which will also show in your console once you’ve run the code yourself. However, note that depending on the size of your monitor, the output may vary slightly. flights # A tibble: 336,776 × 19 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 2013 1 1 517 515 2 830 819 2 2013 1 1 533 529 4 850 830 3 2013 1 1 542 540 2 923 850 4 2013 1 1 544 545 -1 1004 1022 5 2013 1 1 554 600 -6 812 837 6 2013 1 1 554 558 -4 740 728 7 2013 1 1 555 600 -5 913 854 8 2013 1 1 557 600 -3 709 723 9 2013 1 1 557 600 -3 838 846 10 2013 1 1 558 600 -2 753 745 # ℹ 336,766 more rows # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Let’s unpack this output: A tibble: 336,776 x 19: A tibble is a specific kind of data frame in R. This particular data frame has 336,776 rows corresponding to different observations. Here, each observation is a flight. 19 columns corresponding to 19 variables describing each observation. year, month, day, dep_time, sched_dep_time, dep_delay, and arr_time are the different columns; that is, they’re the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 flights. R is only showing the first 10 rows, because if it showed all 336,776 rows, it would overwhelm your screen. 336,766 more rows indicating to us that there are 336,766 more rows of data 11 more variables: and then a list of the 11 that could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well, but it does give a nice preview. Let’s look at some different ways to explore data frames. 6.5.3 Exploring data frames There are many ways to get a feel for the data contained in a data frame such as flights. Here are three functions that take as their “argument” (their input) the data frame in question. I’ve also included a fourth method for exploring one particular column of a data frame: using the View() function, which brings up RStudio’s built-in data viewer using the glimpse() function, which is included in the dplyr package using the kable() function, which is included in the knitr package using the $ “extraction operator,” which is used to view a single variable/column in a data frame 1. View(): Run View(flights) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(flights) instead of View(flights). By running View(flights), we can explore the different variables listed in the columns. Observe that there are many different types of variables. Some of the variables like distance, day, and arr_delay are what we will call quantitative variables. These variables are numerical in nature. Other variables here are categorical. Note that if you look in the leftmost column of the View(flights) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row is representing. This will allow you to identify what object is being described in a given row by taking note of the values of the columns in that specific row. This is often called the observational unit. The observational unit in this example is an individual flight departing from New York City in 2013. You can identify the observational unit by determining what “thing” is being measured or described by each of the variables. We’ll talk more about observational units in Subsection 6.5.4 on identification and measurement variables. 2. glimpse(): The second way we’ll cover to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package by running library(dplyr). This function provides us with an alternative perspective for exploring a data frame than the View() function: glimpse(flights) Rows: 336,776 Columns: 19 $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2… $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, … $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, … $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1… $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,… $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,… $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1… $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;, &quot;… $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4… $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N394… $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LGA&quot;,… $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IAD&quot;,… $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1… $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, … $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6… $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0… $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0… Observe that glimpse() will give you the first few entries of each variable in a row after the variable name. In addition, the data type (see Subsection 6.3.1) of the variable is given immediately after each variable’s name inside &lt; &gt;. Here, int and dbl refer to “integer” and “double”, which are computer coding terminology for quantitative/numerical variables. “Doubles” take up twice the size to store on a computer compared to integers. In contrast, chr refers to “character”, which is computer terminology for text data. In most forms, text data, such as the carrier or origin of a flight, are categorical variables. The time_hour variable is another data type: dttm. These types of variables represent date and time combinations. However, we won’t work with dates and times this semester; you can always read more about this in other data science books like Introduction to Data Science by Tiffany-Anne Timbers, Melissa Lee, and Trevor Campbell or R for Data Science by Hadley Wickham and Garrett Grolemund. 3. kable(): The final way to explore the entirety of a data frame is using the kable() function from the knitr package. Let’s explore the different carrier codes for all the airlines in our dataset two ways. Run both of these lines of code in the console: airlines kable(airlines) At first glance, it may not appear that there is much difference in the outputs. However, when using tools for producing reproducible reports such as R Markdown (as we will be doing for our projects in this class), the latter code produces output that is much more legible and reader-friendly. You’ll see us use this reader-friendly style in many places in the book when we want to print a data frame as a nice table. 4. $ operator Lastly, the $ operator allows us to extract and then explore a single variable within a data frame. For example, run the following in your console airlines$name We used the $ operator to extract only the name variable and return it as a vector of length 16. We’ll only be occasionally exploring data frames using the $ operator, instead favoring the View() and glimpse() functions. 6.5.4 Identification and measurement variables There is a subtle difference between the kinds of variables that you will encounter in data frames. There are identification variables and measurement variables. For example, let’s explore the airports data frame by showing the output of glimpse(airports): glimpse(airports) Rows: 1,458 Columns: 8 $ faa &lt;chr&gt; &quot;04G&quot;, &quot;06A&quot;, &quot;06C&quot;, &quot;06N&quot;, &quot;09J&quot;, &quot;0A9&quot;, &quot;0G6&quot;, &quot;0G7&quot;, &quot;0P2&quot;, &quot;… $ name &lt;chr&gt; &quot;Lansdowne Airport&quot;, &quot;Moton Field Municipal Airport&quot;, &quot;Schaumbur… $ lat &lt;dbl&gt; 41.1, 32.5, 42.0, 41.4, 31.1, 36.4, 41.5, 42.9, 39.8, 48.1, 39.6… $ lon &lt;dbl&gt; -80.6, -85.7, -88.1, -74.4, -81.4, -82.2, -84.5, -76.8, -76.6, -… $ alt &lt;dbl&gt; 1044, 264, 801, 523, 11, 1593, 730, 492, 1000, 108, 409, 875, 10… $ tz &lt;dbl&gt; -5, -6, -6, -5, -5, -5, -5, -5, -5, -8, -5, -6, -5, -5, -5, -5, … $ dst &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;U&quot;, &quot;A&quot;, &quot;A&quot;, &quot;U&quot;, &quot;A&quot;,… $ tzone &lt;chr&gt; &quot;America/New_York&quot;, &quot;America/Chicago&quot;, &quot;America/Chicago&quot;, &quot;Ameri… The variables faa and name are what we will call identification variables, variables that uniquely identify each observational unit. In this case, the identification variables uniquely identify airports. Such variables are mainly used in practice to uniquely identify each row in a data frame. faa gives the unique code provided by the FAA for that airport, while the name variable gives the longer official name of the airport. The remaining variables (lat, lon, alt, tz, dst, tzone) are often called measurement or characteristic variables: variables that describe properties of each observational unit. For example, lat and long describe the latitude and longitude of each airport. Furthermore, sometimes a single variable might not be enough to uniquely identify each observational unit: combinations of variables might be needed. While it is not an absolute rule, for organizational purposes it is considered good practice to have your identification variables in the leftmost columns of your data frame. ### Help files Another nice feature of R are help files, which provide documentation for various functions and datasets. You can bring up help files by adding a ? before the name of a function or data frame and then run this in the console. You will then be presented with a page showing the corresponding documentation if it exists. For example, let’s look at the help file for the flights data frame. ?flights The help file should pop up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away. 6.6 Conclusion Does this chapter contain everything you need to know? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn’t be useful! Instead, we’ve covered a minimally viable set of tools to explore data in R. As I said earlier, the best way to add to your toolbox is to get into RStudio and run and write code as much as possible. If you are new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, you can check out the short book, Getting Used to R, RStudio, and R Markdown, by Chester Ismay and Patrick C. Kennedy. It includes screencast recordings that you can follow along and pause as you learn. This book also contains an introduction to R Markdown, a tool used for reproducible research in R. Figure 6.6: Preview of Getting Used to R, RStudio, and R Markdown. "],["m2c-set-up-github.html", "7 M2C: Set up GitHub 7.1 Introduction 7.2 Git—and GitHub—in Data Science 7.3 GitHub in ICT/LIS 661 7.4 Setting Up GitHub 7.5 Moving Forward", " 7 M2C: Set up GitHub This chapter draws on material from: Happy Git and GitHub for the useR by Jenny Bryan, licensed under CC BY-NC 4.0 Changes to the source material include addition of new material; light editing; rearranging and removing original material; adding and changing links; and adding first-person language from current author. The resulting content is licensed under CC BY-NC 4.0 7.1 Introduction Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files—called a repository—in a sane, highly structured way. Think of it as the more powerful cousin of the “Track Changes” features from Microsoft Word. 7.2 Git—and GitHub—in Data Science Git has been re-purposed by the data science community to manage the motley collection of files that make up typical data analytical projects, which often consist of data, figures, reports, and source code. A solo data scientist, working on a single computer, will benefit from adopting version control—but not nearly enough to justify the pain of installation and workflow upheaval. There are much easier ways to get versioned back ups of your files, if that’s all you’re worried about. This is where hosting services like GitHub, Bitbucket, and GitLab come in. They provide a home for your Git-based projects on the internet. Think of this as Dropbox or Google Drive for code. The service acts as a distribution channel or clearinghouse for your Git-managed project. It allows other people to see your stuff, sync up with you, and perhaps even make changes. These hosting providers improve upon traditional Unix Git servers with well-designed web-based interfaces. There are clear advantages to using one of these services: First, if someone needs to see your work or if you want them to try out your code, they can easily get it from GitHub. If they use Git, they can clone or fork your repository. If they don’t use Git, they can still browse your project on GitHub like a normal website and even grab everything by downloading a zip archive. Second, if you care deeply about someone else’s project, such as an R package you use heavily, you can track its development on GitHub. You can watch the repository to get notified of major activity. You can fork it to keep your own copy. You can modify your fork to add features or fix bugs and send them back to the owner as a proposed change. 7.3 GitHub in ICT/LIS 661 In this class, you will be accessing and submitting your projects for this class through GitHub. If you need feedback or help with your code, we’ll also use GitHub to send it back and forth. We won’t be using GitHub to its fullest extent, but this will at least give you some basic familiarity with the software. We are using GitHub—and not Bitbucket or GitLab—because GitHub is widely used in the R community, and because it’s what I’m most familiar with. However, it’s worth pointing out that the GitHub company has a history of controversy and that it has recently come under increased criticism from the same open source community that GitHub claims to serve. For the time being, I am continuing to use GitHub (personally and for this class), but as we’ll discuss throughout the semester, it’s important to be attentive to ethics and values in data science, and I don’t think this is any exception. 7.4 Setting Up GitHub The remainder of this walkthrough is focused on helping you get GitHub set up for this class. 7.4.1 Creating a GitHub Account First things first, head on over to github.com and create an account. If you aren’t already using GitHub and are not sure you’ll be using GitHub in the future, I recommend creating an account with your @uky.edu address, but you’re welcome to use a GitHub account you already have or to use a personal address if you think you’ll want to use this service more in the future. Here are some tips for picking a GitHub username: Incorporate your actual name! People like to know who they’re dealing with. Also makes your username easier for people to guess or remember. Reuse your username from other contexts, e.g., Twitter or Slack. But, of course, someone with no GitHub activity will probably be squatting on that. Pick a username you will be comfortable revealing to your future boss. Shorter is better than longer. Be as unique as possible in as few characters as possible. In some settings GitHub auto-completes or suggests usernames. Make it timeless. Don’t highlight your current university, employer, or place of residence. Avoid the use of upper vs. lower case to separate words. I highly recommend all lowercase. Usernames aren’t case sensitive in GitHub, but using all lowercase is kinder to people who may be working with GitHub data. A better strategy for word separation is to use a hyphen (-) or underscore (_). 7.4.2 Setting up a GitHub Repository for Class In this next section, I’ve tried to be as specific as possible in providing instructions. However, past experience has shown me that what I see on my computer while walking through these steps isn’t always exactly what other people see due to operating system or other differences. If something doesn’t match perfectly what you see, use common sense to figure out the next best option. Once you’ve created a GitHub account, please navigate to this GitHub repository, which contains some crucial files for our class. If you have your browser window open wide enough, you should see a green Use this template button to the right of the menu above the list of files and directories (or folders) Click that button and, if prompted, click on Create a new repository. The next interface has a few different sections in it. You can ignore the first few options and then go down to where it says Owner: Make sure it lists your username there! If/as you continue to work with RStudio and GitHub, you’ll get a feel for what kind of directory names are most helpful to you, but I’m mandating a consistent format for selfish reasons: So that I can easily keep track of dozens of different respositories this semester. Give this project a name that follows this format: yourlastname_yourfirstname_661 That is, my project would be named Greenhalgh_Spencer_661. You can skip the Description line, but make sure to set your repository to Private. This is important so that your homework for this class isn’t publicly accessible! Once you’ve finished all the steps on this interface, GitHub will take you to the page for your repository. Find the Settings button near the top right of the page (if you can’t see it, you may need to make your browser window larger), and click on that. Once you’re in settings, navigate to Collaborators and then Add people. Then, enter my GitHub username, greenhas. Having access to your repository will make helping you with your code much easier in the future! 7.4.3 Installing GitHub Desktop Once you’ve done all this, navigate to desktop.github.com and download the GitHub Desktop client. Open it up and make sure to associate it with your GitHub account. There are plenty of other ways to use Git and GitHub, including the command line and RStudio itself, and if you’re serious about data science, I recommend learning more about both of those options (the Happy Git and GitHub for the useR website is helpful and thorough). You’re even free to try them this semester, but remember that my instructions are always going to assume you’re using this client. That’s because I find GitHub Desktop to be the most user-friendly approach to GitHub, and since our goals for this class are just a basic introduction to the service, it’s going to do the trick. Once you’ve signed in to GitHub Desktop, navigate to File and then Clone Repository. This will bring up a list of repositories that you have on GitHub for you to choose from; in this context, “clone” simply means to create a local copy of the repository on your computer. Pay attention to the Local Path option. By default, GitHub Desktop will create the local copy of your files within: a folder that has the same name as the repository, which is placed within: either a Documents folder or the main user folder of your computer You’re free to change this, but I strongly recommend sticking with the default folder name, and unless you have strong opinions about where to keep your files, it’s probably a good idea to stick with the default filepath, too. Wherever you end up storing your repository folder, pay close attention—you’ll be navigating back to it to complete your work throughout the semester. 7.5 Moving Forward As things currently stand, your repository isn’t really doing a whole lot, but that’s okay. We’ll return to GitHub next week as we learn some more about file and project management in RStudio, and practice good GitHub habits throughout the semester. "],["m3u-research-paradigms-and-reproducibility.html", "8 M3U: Research Paradigms and Reproducibility 8.1 Introduction 8.2 Reproducibility and Paradigms 8.3 Supporting Reproducibility 8.4 Conclusion 8.5 References", " 8 M3U: Research Paradigms and Reproducibility This chapter draws on material from: Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim, licensed under CC BY-NC-SA 4.0 Open Education Science by Tim van der Zee and Justin Reich, licensed under CC BY-NC 4.0 Changes to the source material include removal of original material, the addition of new material, combining of sources, editing of original material for a different audience, and addition of first-person language from current author.. The resulting content is licensed under CC BY-NC-SA 4.0. 8.1 Introduction One goal of this reading is helping readers understand the importance of reproducible analyses. The hope is to get readers into the habit of making their data science analyses reproducible from the very beginning. Reproducibility means a lot of things in terms of different scientific fields, but they are all concerned with a common goal: Is research conducted in a way that another researcher could follow the steps and get similar results? The importance placed on reproducibility comes in part from a growing belief that a substantial portion of published research findings may actually report false positives—or, at the very least, overestimate the importance of a study’s findings (Simmons et al., 2011). Ioannidis (2005) went so far as to title an article “Why Most Published Research Findings Are False,” describing several problems in medical research that lead to false positive rates. If you can reproduce someone else’s research and get the same results, that suggests that the findings of the original study are on solid ground. However, if you try to follow their steps and get different results, there may be reason to question what the original researchers had to say. This is pretty straight forward, but reproducing someone else’s work isn’t as easy as it might seem. Most researchers in training eventually realize that short summaries of research methods in journal articles tidy over much of the complexity and messiness in research—especially research that involves humans. These summaries spare readers from trivial details, but they can also misrepresent important elements of the research process. However, if our goal is to provide practitioners, policymakers, and other researchers with data, theory, and explanations that lead to practical improvements, we need to provide them with a way to judge the quality and contextual relevance of research. That judgment depends in tern on how researchers choose to share the methods and processes behind their work. In data science, there is a particular emphasis on what is known as computational reproducibility. This refers to being able to pass all of one’s data analysis, datasets, and conclusions to someone else and have them get exactly the same results on their machine. This has obvious benefits for scientific concerns about reproducibility! In fact, one of my friends in graduate school had a job doing reproducibility for a political science journal. Anyone submitting research to be published in the journal also had to submit all of their data analysis, datasets, and conclusions. The paper to be published would be submitted to traditional peer review, but my friend would also review all of the submitted materials to make sure that there weren’t any discrepancies or errors in the data, analysis, and results described in the paper. However, there are also practical and personal benefits to computational reproducibility. In a traditional scientific workflow, you might do your statistical work and data visualization in RStudio and then write up your results in a word processor, pasting in helpful plots that you’d generated in RStudio. This isn’t a terrible demand on your time, but what if you discover a mistake partway through? Or what if someone gives you good advice on a late draft of your paper that leads you to make some changes to your data analysis? In a traditional workflow, you’d need to step through the entire process again: redo the analyses, redo the data visualization, rewrite the paper, repaste the plots. This is error prone and a frustrating use of time. In contrast, with the right software—and, more importantly, the right mindset—there are ways to include analysis and writing in the same document. Not only does this allow a colleague to easily reproduce all of the steps you’ve taken, but it might actually save you from having to redo work! This allows for data scientists to spend more time interpreting results and considering assumptions instead of spending time restarting an analysis from scratch or following a list of steps that may be different from machine to machine. Really committing to reproducibility means building new habits; it takes practice and will be difficult at times. Hopefully, though, you’ll see just why it is so important for you to keep track of your code and document it well to help yourself later and any potential collaborators as well. 8.2 Reproducibility and Paradigms I should make it clear from the beginning that I am an imperfect example of reproducibility. Part of this is because this is a difficult process—I’m still developing necessary habits and trying to get rid of bad habits. However, I also have philosophical differences with reproducibility as I’ve described it here. This isn’t to say that I disagree with the importance of reproducibility itself—I think it’s a fine idea, and I’m even going to enforce a certain level of it in this class! However, the reasons that reproducibility is important are often tied to basic assumptions that data scientists typically make about how the world works. While I draw heavily from data science techniques in my research, I also draw from research traditions that do not share the same assumptions as data science. 8.2.1 An Example Paradigm To explain more what I mean by this, let’s think about some of the assumptions that are implicitly present in my description of reproducibility in the previous section: Everyone ought to get the same results every time: As described above, reproducibility is related to a belief that different scientists should get similar results if individually performing the same analysis. If two scientists don’t come to the same answer when asking similar questions, that’s seen as a problem. People who work separately ought to come to the same conclusions. Research should be a series of nice, neat decisions: The basic idea of reproducibility is that it should be possible to provide a clear checklist that describes the steps that scientists took when carrying out an analysis. This only works if individual decisions can be clearly distinguished from each other and then communicated to another in a methodical way. Research is meant to inform practitioners and policymakers: Part of the reasoning behind making a research study reproducible is giving people confidence in the study’s findings; part of the reasoning behind giving people confidence in the study’s findings is so that other people can act on those findings. This perspective sees research as useful when it translates into specific actions that other stakeholders can take. Each of these three assumptions about research can be understood as connecting to a deeper assumption about how the world works and the resulting role of research: The world—including things and people in it—work in consistent, predictable ways; the point of research (including data science) is to figure out how the world works so that we can get the outcomes we want. In research settings, these collections of assumptions (and underlying worldview) are often referred to as a paradigm. These aren’t terrible assumptions, and this isn’t a bad paradigm! They’ve driven most of scientific progress and a lot of effective decision-making over the past couple of centuries. However, they’re not the only way of thinking about the world. Let me provide a recent example from my data science-adjacent research that rejects these assumptions in favor of a different set of assumptions. 8.2.2 Another Example Paradigm In mid-2019, I used R (and other software) to collect screenshots of about 1,400 tweets associated with a specific Twitter hashtag. The organizers of the hashtag claim that people who use the hashtag are simply members of a particular church and that their goal is to defend that church against its critics. However, outside observers have argued that participants in the hashtag are heavily influenced by toxic, far-right online communities and believe that these tweets are more motivated by white nationalism than religious beliefs. Over the course of a couple of years, I worked with a colleague to carefully study each of these tweets so that we could come to some conclusions about what’s happening in this hashtag. Here are some of the assumptions that we had about our research: Different people might come to different conclusions: My co-researcher and I understood while studying these tweets that we might understand them differently. I grew up in the church that this hashtag is associated with, so I pick up on a lot of subtle references that my colleague doesn’t. However, as a woman, she picks up on subtle misogyny that sometimes sails right over my (masculine) head. We have different lived experiences, and we know that if we analyzed this data separately, we would probably come to different conclusions—that’s exactly why we’re working together. Research involves complex, messy decisions: What’s the point at which two researchers become confident that a 280-character post on Twitter includes a reference to white nationalism? The group that my colleague and I are studying is skilled at ambiguity and subtle references, so it’s not often the case that they clearly embrace extreme political views. It would be a mistake to ignore these views just because they’re not always easy to pin down, but it would also be irresponsible to exaggerate our conclusions. There’s no clear checklist for determining whether a tweet includes references to white nationalism—instead, we have to engage in lengthy discussions and make difficult calls. The goal of research is to develop understanding: I hope that our research makes some kind of difference in the world, and I know that my colleague does too. However, after reviewing our data and our conclusions, it’s not as if we have a list of specific actions that someone can take in response to our research. Who is that “someone” anyway? Twitter users? The church that this group claims to be loyal to? The group itself? Any of those populations could hypothetically learn something from what we have to say, but it would be a stretch to say that there’s a clear and immediate takeaway from our research. Our goal is to simply to document our findings in the hopes of increasing readers’ understanding of this kind of phenomenon. Each of our three assumptions about researchers can be understood as connecting to a paradigm we share: The world—including things and people in it—are rich, complex, and difficult to perfectly summarize; the point of research is to try to capture some of that richness so that we can better appreciate details and context. Some scientists are hostile to the idea of any paradigm except the first one and would be dismissive of the kind of research that I’ve just described. However, I’m confident that many data scientists acknowledge the existence and value of different research paradigms. Their response might be that the first paradigm works particularly well for data science, but that there’s no harm in using the second paradigm in other kinds of research. In fact, they might suggest, even if I did use R for that research project, it doesn’t really count as data science (to be honest, I’d agree with this evaluation, but remember what we’ve already read about what gets to count as data science!). 8.2.3 Why Do Paradigms Matter? In that case, what’s the point of making a big deal about paradigms here? If reproducibility is based on a paradigm about the predictability of the world, and if data science embraces that paradigm, why emphasize the existence of other paradigms? That’s a good question, but there’s an answer that’s just as good. In short, while it’s very useful to assume that the world is neatly ordered and predictable (remember all of those scientific and technological developments over the past couple of centuries?), it’s also possible to overemphasize that assumption—and overemphasis carries with it important ethical consequences. Let’s consider a concrete example related to data scientist. In recent years, there’s been considerable interest in sentencing algorithms. In other words, human judges are fallible and biased and might hand out different sentences for the same crime based not on an objective interpretation of the law but on their subjective responses to the accused. Wouldn’t it be great if we could come up with a computer program that would take in the facts of a case and information about the criminal and spit out an appropriate, fair sentence that a potentially biased judge couldn’t interfere with? The influence of the first paradigm ought to be clear here: Decisions ought to be consistent, so let’s boil down sentencing to a neat checklist-like algorithm that uses data to make a recommendation and make a practical difference. In theory, this sounds great—an application of data science that could make a real difference in the world. However, there are plenty of critics of this approach (including me). No one disputes that human judges are fallible and biased, and it would be fantastic to come up with a way of sentencing criminals that wouldn’t be as influenced by personal prejudices. The issue isn’t with the intent, it’s with the underlying paradigm. Critics of sentencing algorithms argue that in valuing consistency, predictable algorithms, and clear-cut recommendations, they overlook ways in which the world doesn’t actually work like that! For example, a sentencing algorithm would likely be trained on sentences given by human judges, and use those as the basis for the supposedly objective decision. However, as we’ve already established, human judges are fallible and biased; for example, we might (and almost certainly would!) find that judges give harsher sentences to criminals of color than white criminals. A sentencing algorithm that used race as a factor in determining sentences would therefore treat “non-white” as a statistically-valid reason to recommend a harsher sentence, simply because that’s the pattern that it was trained on. Rather than replace human bias, the algorithm would learn from it—but because humans tend to believe computers are more objective than humans, that bias would be harder to argue against! In a sense, this is a worse outcome, because it inherits bias but disguises it as an objective measure. This criticism implicitly adopts the second paradigm that I’ve described in this sentence: The process of sentencing a criminal is too rich and complex to distill down to a computational decision. More importantly, it points out a weakness of the first paradigm: A belief that the world is consistent and predictable is actually sometimes just a preference for things that seem consistent and predictable. That is, data scientists sometimes fall into the trap of believing that a consistent and predictable solution to a problem is better, when a solution that acknowledges complexity and nuance might feel less efficient but would do better at avoiding harm. 8.3 Supporting Reproducibility If I am sometimes resistant to calls for reproducibility, it is because I think the paradigm that these calls assume doesn’t always hold up. However, there’s nothing half-hearted about my teaching you about reproducibility in this class—it’s impossible to deny that there are amazing benefits to making your data science work reproducible (so long as data scientists critically examine their own paradigms). Let’s consider an example that I like a lot. Josh Rosenberg is one of my data science heroes. We went to grad school together, and 90% of what I know about R is thanks to him; if I have questions about R or data science, Josh is the first person I go to. When we were still in grad school, Josh participated in a study in which: independent analysts used the same dataset to test two hypotheses regarding the effects of scientists’ gender and professional status on verbosity during group meetings (Schweinsberg et al., 2021, p. 230) In other words, Josh and dozens of other researchers (the list of contributors to this study is literally a page long) were all given the same data and encouraged to test the same hypotheses. However, apart from the data and the hypotheses, the organizers of the study were intentionally skimpy on details. For example, it was up to individual researchers to determine how to best measure “professional status” using the available data, what statistical tests to use, etc. As a result, Researchers reported radically different analyses and dispersed empirical outcomes, in a number of cases obtaining significant effects in opposite directions for the same research question. (Schweinsberg et al., 2021, p. 230) In short, even though they were all given the same prompts and the same data, researchers made very different analytical decisions and came to very different results, sometimes producing conflicting (but individually compelling) findings. The article reports that Subjective researcher decisions play a critical role in driving the reported empirical results, underscoring the need for open data, systematic robustness checks, and transparency regarding both analytic paths taken and not taken. (Schweinsberg et al., 2021, p. 230) That is, scientists make lots of decisions when analyzing data. Even if they’re using the same data and asking the same questions, different, equally-qualified scientists might make wildly different decisions. In theory, any research article is supposed to contain a detailed section of how any analysis was completed, but as we’ve already determined, word limits and researchers’ attention spans are too short for articles to cover every decision that could potentially make a difference. This is why reproducibility is important. If you can carefully document every decision you made, every line of code you wrote, and every other aspect of your analysis, other people will know not just what data you used and what questions you asked, but how—in very specific terms—you asked those questions. This is an undeniably good thing. So, how do we support reproducibility in practical terms? Here are a few—with a focus on how they play out in traditional academic research: 8.3.1 Open Design Research design is essential to any study as it dictates the scope and use of the study. This phase includes formulating the key research question(s), designing methods to address these questions, and making decisions about practical and technical aspects of the study. Typically, this entire phase is the private affair of the involved researchers. In many studies, the hypotheses are obscured or even unspecified until the authors are preparing an article for publication. Readers often cannot determine how hypotheses and other aspects of the research design have changed over the course of a study since usually only the final version of a study design is published. Moreover, there is compelling evidence that much of what does get published is misleading or incomplete in important ways. A meta-analysis (that is, research on other research) found that 33% of authors admitted to questionable research practices, such as “dropping data points based on a gut feeling,” “concealment of relevant findings,” and/or “withholding details of methodology” (Fanelli, 2009). Given that these numbers are based on self-reports and thus suspect to social desirability bias, it is plausible that these numbers are underestimates. For Open Design, researchers make every reasonable effort to give readers access to a truthful account of the design of a study and how that design changed over the duration of the study. Since study designs can be complex, this often means publishing different elements of a study design in different places. For instance, many academic journals publish short methodological summaries in the full text of an article and allow more detailed supplementary materials of unlimited length online. In addition, analytic code might be published in a linked GitHub account, and data might be published in an online repository. These various approaches allow for more detail about methods to be published, with convenient summaries for general readers and more complete specifics for specialists and those interested in replication and reproduction. There are also a variety of approaches for increasing transparency by publishing a time-stamped record of methodological decisions before publication: a strategy known as preregistration. Preregistration is the practice of documenting and sharing the hypotheses, methodology, analytic plans, and other relevant aspects of a study before it is conducted (Gehlbach &amp; Robinson, 2018; Nosek et al., 2015). 8.3.2 Open Analysis Open Analysis is the systematic reproduction of analytic methods conducted by other researchers—it’s the main support for reproducibility that has come up in this reading. Reproducing research is central to scientific progress because any individual study is generally insufficient to make robust or generalizable claims—the kinds that others could clearly act on. It is only after ideas are tested and replicated in various conditions and contexts and results “meta-analyzed” across studies that more durable scientific principles and precepts can be established. One form of replication is a reproduction study, where researchers attempt to faithfully reproduce the results of a study using the same data and analyses. Such studies are dependent on Open Design, so that replication researchers can use the same methodological techniques but also the same exclusion criteria, coding schemes, and other analytic steps that allow for faithful replication. In recent years, perhaps the most famous reproduction study was by Thomas Herndon, a graduate student at UMass Amherst who discovered that two Harvard economists, Carmen Reinhart and Kenneth Rogoff, had failed to include five columns in an averaging operation in an Excel spreadsheet (The Data Team, 2016). After averaging across the full data set, the claims in the study had a much weaker empirical basis. Ouch! In data science, where statistical code is central to conducting analyses, the sharing of that code is one way to make analytic methods more transparent. GitHub and similar repositories allow researchers to store code, track revisions, and share with others—GitHub’s importance for reproducibility is a major reason that we’re using it in this course. At a minimum, these repositories allow researchers to publicly post analytic code in a transferable, machine-readable platform. Used more fully, GitHub repositories can allow researchers to share preregistered codebases that present a proposed implementation of hypotheses, final code as used in publication, and all of the changes in between. Simply making code “available on request” will not be as powerful as creating additional mechanisms that encourage researchers to proactively share their analytic code: as a requirement for journal or conference submissions, as an option within study preregistrations, or in other venues. Reinhart and Rogoff’s politically consequential error might have been discovered much sooner if their analyses had been made available along with publication rather than after the idiosyncratic query of an individual researcher. 8.3.3 Open Publication Open Access (OA) literature is digital, online, available to read free of charge, and free of most copyright and licensing restrictions (Suber, 2004). Most for-profit publishers obtain all the rights to a scholarly work and give back limited rights to the authors. With Open Access, the authors retain copyright for their article and the right to allow anyone to download and reprint provided that the authors and source are cited, for example under a Creative Commons Attribution License (CC BY 4.0). Opening access increases the ability of researchers, policymakers, and practitioners to leverage scientific findings for the public good. Sharing a publicly accessible preprint can also be used to receive comments and feedback from fellow scientists, a form of informal peer review. Preprints are publicly shared manuscripts that have not (yet) been peer reviewed. A variety of peer-reviewed journals acknowledge the benefits of preprints. Across the physical and computer sciences, repositories such as ArXiv have dramatically changed publication practices and instituted a new form of public peer review across blogs and social media. In the social sciences, the Social Science Research Network and SocArXiv offer additional repositories for preprints and white papers. Preprints enable more iterative feedback from the scientific community and provide public venues for work that address timely issues or otherwise would not benefit from formal peer review. Whereas historically peer review has been considered a major advantage over these forms of nonreviewed publishing, the limited amount of available evidence suggests that the typically closed peer-review process has no to limited benefits (Bruce et al., 2016; Jefferson et al., 2002). Public scholarly scrutiny may prove to be an excellent complement, or perhaps even an alternative, to formal peer review. 8.4 Conclusion Data science is a field that values reproducibility, and we’re going to practice being reproducible as part of this course. It’s important to recognize that the value of reproducibility is based off of certain assumptions about what good research is and how the world works; those assumptions do not always hold up, and they can even be dangerous if we’re not careful about them. However, that doesn’t change the fact that there are real, powerful benefits to reproducibility. 8.5 References Bruce, R., Chauvin, A., Trinquart, L., Ravaud, P., &amp; Boutron, I. (2016). Impact of interventions to improve the quality of peer review of biomedical journals: A systematic review and meta- analysis. BMC Medicine, 14(1), 85. https://doi.org/10.1186/s12916-016-0631-5 The Data Team. (2016, September 7). Excel Errors and science papers. The Economist. Retrieved from https://www.economist .com/blogs/graphicdetail/2016/09/daily-chart-3 Fanelli, D. (2009). How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data. PloS One, 4(5), e5738. https://doi.org/10.1371/journal.pone.0005738 Gehlbach, H., &amp; Robinson, C. D. (2018). Mitigating illusory results through preregistration in education. Journal of Research on Educational Effectiveness, 11, 296–315. https://doi.org/10.1080/1934574 7.2017.1387950 Ioannidis, J. P. (2005). Why most published research findings are false. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124 Jefferson, T., Alderson, P., Wager, E., &amp; Davidoff, F. (2002). Effects of editorial peer review: A systematic review. JAMA, 287(21), 2784–2786. https://doi.org/10.1001/jama.287.21.2784 Nosek, B. A., &amp; Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192 Schweinsberg, M., Feldman, M., Staub, N., van den Akker, O. R., van Aert, R. C. M., van Assen, M. A. L. M., Liu, Y., Althoff, T., Heer, J., Kale, A., Mohamed, Z., Amireh, H., Prasad, V. V., Bernstein, A., Robinson, E., Snellman, K., Sommer, S. A., Otner, S. M. G., … Ulhmann, E. L. (2021). Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis. Organizational Behavior and Human Decision Processes, 165, 228-249. https://doi.org/10.1016/j.obhdp.2021.02.003 Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 Suber, P. (2004, June 21). Open Access overview. Retrieved from https://legacy.earlham.edu/~peters/fos/overview.htm "],["m3a-using-projects-and-scripts-in-r.html", "9 M3A: Using Projects and Scripts in R 9.1 Introduction 9.2 More About R and RStudio 9.3 The Console and the Workspace 9.4 Holding Onto Code 9.5 RStudio Projects", " 9 M3A: Using Projects and Scripts in R This content draws on material from: * STAT 545 by Jenny Bryan, licensed under CC BY-SA 4.0 Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim, licensed under CC BY-NC-SA 4.0 Changes to the source material include addition of new material; light editing; rearranging, removing, and combining original material; adding and changing links; and adding first-person language from current author. The resulting content is licensed under CC BY-NC-SA 4.0. 9.1 Introduction A couple of years ago, I had to make an official correction to my most-cited research study. My co-author and I were revisiting some of the calculations in the study as part of a separate project, and we found some discrepancies between the published results and the results that my co-author was getting. It turned out that back in 2017, we hadn’t been working off of the same data file when working on separate parts of the paper, and that created small-but-embarrassing inconsistencies between what we had published and what the actual results were. When you’re doing data science work, it’s very important to keep things organized! We’ve run some code in R already, but how we manage that code can be really important. In this walkthrough, we’ll learn some more R and RStudio features before learning more about scripts and Projects in R. By way of reminder, whenever you see a “code chunk,” you should type (or copy) the code and run it on your own; however, remember also that output looks a lot like code chunks but doesn’t need to be run! Any box that follows a code chunk and whose lines begin with ## is just output: It’s there for you to compare your results against, not for you to copy and paste. Likewise, text that is marked as code but is in the middle of a paragraph doesn’t need to be run; it’s just a way of showing that something is code. 9.2 More About R and RStudio Begin this walkthrough by starting RStudio. By way of reminder, as you open up the program, you should see something similar to Figure 9.1. Figure 9.1: RStudio interface to R. Note again the three panes (that is, the three panels dividing the screen): the console pane, the files pane, and the environment pane. Let’s spend some more time in the Console, which is where we interact with the live R process. The following code tells R to assign the result of 3 * 4 (that is, 3 times 4) to an object called x and then asks R to “inspect” the object (that is, retrieve its contents). Go ahead and run it on your own: x &lt;- 3 * 4 x [1] 12 All R statements where you create object —“assignments”—have this form: objectName &lt;- value. It is technically possible to use = to make assignments, too, but it’s a pretty bad idea, for reasons we don’t need to go into here. Yes, it takes a bit more effort, but always make assignments with &lt;-. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. It’s important to get into good habits with naming your objects. Many people use some kind of regular pattern in naming objects with multiple words: i_use_snake_case, other.people.use.periods, and evenOthersUseCamelCase Make another assignment by running this code: this_is_a_really_long_name &lt;- 2.5 To inspect this object, try out RStudio’s completion facility: type the first few characters of this_is_a_really_long_name, press TAB, add characters until you disambiguate, then press return. Now, make another assignment: stromae_rocks &lt;- 2 ^ 3 What happens if we try to run the following code to inspect our new object? stromaerocks Error in eval(expr, envir, enclos): object &#39;stromaerocks&#39; not found There’s an implicit contract when you work with a scripting language like R: The computer will do tedious computation for you, but you must be completely precise in your instructions. Typos matter. Case matters. Details matter. I am always happy to help you troubleshoot, but please make sure that you’ve checked the details of your code first. It’s amazing how often errors come down to mistyping something. R has a mind-blowing collection of built-in functions that are accessed like so: functionName(argument1 = value1, argument2 = value2, and so on). The details look different from function to function, but they typically follow this pattern. Let’s try functions using seq(), which makes regular sequences of numbers. While we’re at it, we’ll demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a function’s arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. As we saw earlier, you can also run a function with a ? in front of it (for example, ?seq()) to bring up the same help documentation. Now type the open parenthesis (() and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. These little features are the things that can make RStudio really useful (though I’ll also admit that they sometimes get in the way, too). seq(1, 10) [1] 1 2 3 4 5 6 7 8 9 10 Note that even though we ran this function, we didn’t assign the results to an object. That means that we can see the results in the Console, but we won’t be able to retrieve them later! If we want to hold onto the results of a function, we need to assign it to an object. Make this assignment and notice that RStudio helps with quotation marks in the same way that it helps with parentheses. yo &lt;- &quot;hello world&quot; If you just make an assignment, you don’t get to see the value, so then you’re tempted to immediately inspect. y &lt;- seq(1, 10) y [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen. (y &lt;- seq(1, 10)) [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() [1] &quot;Fri Jul 21 13:44:50 2023&quot; Now look at your workspace in the upper right Environment pane. The workspace is where user-defined objects accumulate. You ought to see all of the objects that we’ve created so far in this walkthrough. You can also get a listing of these objects with commands: objects() [1] &quot;chap&quot; &quot;flights_cols&quot; [3] &quot;flights_rows&quot; &quot;lc&quot; [5] &quot;needed_CRAN_pkgs&quot; &quot;new_pkgs&quot; [7] &quot;stromae_rocks&quot; &quot;this_is_a_really_long_name&quot; [9] &quot;x&quot; &quot;y&quot; [11] &quot;yo&quot; ls() [1] &quot;chap&quot; &quot;flights_cols&quot; [3] &quot;flights_rows&quot; &quot;lc&quot; [5] &quot;needed_CRAN_pkgs&quot; &quot;new_pkgs&quot; [7] &quot;stromae_rocks&quot; &quot;this_is_a_really_long_name&quot; [9] &quot;x&quot; &quot;y&quot; [11] &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) You can also click the broom in RStudio’s Environment pane to remove everything. Sometimes this is helpful when troubleshooting code. 9.3 The Console and the Workspace One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What from your analysis needs to be saved? Where does your analysis need to be saved? In the R code that we’ve written so far, we’ve run all of our code in the Console, and it’s been saved to the workspace. This has been sufficient for how we’ve done things so far, but it’s not a great long-term solution. Anything that we type into the Console gets saved into your “R history.” You can retrieve code from your R history by pressing the up arrow when your cursor is in the Console. This can be helpful for something you just typed, but it’s also a hassle to go all the way through your R history to find something that you typed hours (or even just minutes) ago. When you quit RStudio, it will generally prompt you to save your workspace; if you do, it will load it back automatically for you when you next open RStudio. This is also handy, but even though you have the results of your work, you don’t have easy access to the process that you took to get there. (That is, objects in your workspace are not easily reproducible). If you need to redo analysis, you’re going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history (using those arrow keys in the console) for the commands you used. A better use of your time and psychic energy is to keep your “good” R code in a script for future reuse—and to keep your related scripts bundled together into a project. In other words, you can do one-off, temporary, or unimportant things in the Console, but most of your code is worth saving—especially if you need help troubleshooting it! 9.4 Holding Onto Code The rest of this walkthrough will demonstrate how to write code in a way that lets you hold onto it later. This can be really helpful for you (for example, when you want to adapt code from a class activity for one of your projects), and it’s also really helpful for me (for example, when I want to see all the code you ran when helping you troubleshoot an activity). 9.5 RStudio Projects Keeping all the files associated with a project organized together—input data, R scripts, analytical results, figures—is such a wise and common practice that RStudio has built-in support for this. Let’s make a project for you to use during the rest of this semester. In RStudio, navigate to File and then New Project. When prompted, click on Existing Directory and then navigate to the folder for the GitHub repository that you created last week. Then, quit RStudio, open up the file interface for your operating system, and make your way to that folder. You should now see a .Rproj file within that folder (with the same name as the foler). If you click on that .Rproj file,it will open up a project-specific window for RStudio, including an interface in the bottom-right corner that shows all the files and folders associated with your project. Since all of your work for this class will all be stored in this single project, it may not be clear during this semester just how useful this can be—trust me, it’s darn useful. Now that you’ve created this project file, I always recommend opening it instead of just RStudio. This is the only project file that you need to create for this class! 9.5.1 Using Scripts The point of a project is to keep multiple files together. Any file that contains code is what we call a script. Again, holding onto your code (instead of just running it through the console) will make your life easier and make it easier for me to help you when you need it. Unless I tell you otherwise, whenever you’re starting a new activity in R, you should navigate through File, New File, and New R Script in RStudio, give that file a clear name, and save that file to your project folder. It is traditional to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Use helpful names for your scripts so that you can find them afterward! Running code within a script file works differently than running it in the console, so you ought to get familiar with these extra steps: to execute code within a script, you should highlight the code you want to execute and either click the Run button at the top of script window or use the keyboard shortcut Ctrl+Enter (Windows) or CMD+Enter (macOS). In theory, scripts are meant to be run all at once, and if you click the Run button without highlighting code, that’s what it will do. However, I (and many other R users) frequently use scripts in this kind of piecemeal way, running bits of code at a time, but keeping the whole script as a backup. 9.5.2 Backing Up Your Project Folder Make sure to save your scripts and other files regularly! However, unlike cloud services like Dropbox or Google Drive, simply saving a file to your project folder is not sufficient to sync it to GitHub. At regular intervals, you should open GitHub Desktop, navigate to your project repository, and click the button in the top right corner (which will read either Fetch origin or Pull origin—if, after clicking Fetch origin, it now reads Pull origin, you’ll want to click it again). If I’ve made changes to your repository, those changes will now sync to your local files. Then, if there are changed files in the appropriate window of the interface, you should enter a Summary in the appropriate field, click the Commit button, and then click Push origin in the top-right corner. It’s not until you’ve successfully pushed your files to GitHub that they are backed up and available for me to request and review! When helping you figure out an issue you’re having with code, I will usually insist on getting access to your data and scripts through your GitHub repository and will usually not look over scripts and data that you send me via email. This means that you need to be in the habit of: saving your code to scripts, saving your scripts to your project folder, and backing up your project folder through GitHub. It’s important to note Git and GitHub are notorious for running into syncing issues. We are using it lightly enough that I don’t expect to run into anything major, but please don’t hesitate to reach out if you run into something you can’t solve on your own! "],["module-3-connection.html", "10 Module 3 Connection", " 10 Module 3 Connection "],["module-4-understanding.html", "11 Module 4 Understanding", " 11 Module 4 Understanding "],["module-4-application.html", "12 Module 4 Application", " 12 Module 4 Application "],["module-4-connection.html", "13 Module 4 Connection", " 13 Module 4 Connection "],["module-5-understanding.html", "14 Module 5 Understanding", " 14 Module 5 Understanding "],["module-5-application.html", "15 Module 5 Application", " 15 Module 5 Application "],["module-5-connection.html", "16 Module 5 Connection", " 16 Module 5 Connection "],["module-6-understanding.html", "17 Module 6 Understanding", " 17 Module 6 Understanding "],["module-6-application.html", "18 Module 6 Application", " 18 Module 6 Application "],["module-6-connection.html", "19 Module 6 Connection", " 19 Module 6 Connection "],["module-7-understanding.html", "20 Module 7 Understanding", " 20 Module 7 Understanding "],["module-7-application.html", "21 Module 7 Application", " 21 Module 7 Application "],["module-7-connection.html", "22 Module 7 Connection", " 22 Module 7 Connection "],["module-8-understanding.html", "23 Module 8 Understanding", " 23 Module 8 Understanding "],["module-8-application.html", "24 Module 8 Application", " 24 Module 8 Application "],["module-8-connection.html", "25 Module 8 Connection", " 25 Module 8 Connection "],["module-9-understanding.html", "26 Module 9 Understanding", " 26 Module 9 Understanding "],["module-9-application.html", "27 Module 9 Application", " 27 Module 9 Application "],["module-9-connection.html", "28 Module 9 Connection", " 28 Module 9 Connection "],["module-10-understanding.html", "29 Module 10 Understanding", " 29 Module 10 Understanding "],["module-10-application.html", "30 Module 10 Application", " 30 Module 10 Application "],["module-10-connection.html", "31 Module 10 Connection", " 31 Module 10 Connection "],["module-11-understanding.html", "32 Module 11 Understanding", " 32 Module 11 Understanding "],["module-11-application.html", "33 Module 11 Application", " 33 Module 11 Application "],["module-11-connection.html", "34 Module 11 Connection", " 34 Module 11 Connection "],["module-12-understanding.html", "35 Module 12 Understanding", " 35 Module 12 Understanding "],["module-12-application.html", "36 Module 12 Application", " 36 Module 12 Application "],["module-12-connection.html", "37 Module 12 Connection", " 37 Module 12 Connection "],["module-13-understanding.html", "38 Module 13 Understanding", " 38 Module 13 Understanding "],["module-13-application.html", "39 Module 13 Application", " 39 Module 13 Application "],["module-13-connection.html", "40 Module 13 Connection", " 40 Module 13 Connection "],["module-14-understanding.html", "41 Module 14 Understanding", " 41 Module 14 Understanding "],["module-14-application.html", "42 Module 14 Application", " 42 Module 14 Application "],["module-14-connection.html", "43 Module 14 Connection", " 43 Module 14 Connection "],["module-15-understanding.html", "44 Module 15 Understanding", " 44 Module 15 Understanding "],["module-15-application.html", "45 Module 15 Application", " 45 Module 15 Application "],["module-15-connection.html", "46 Module 15 Connection", " 46 Module 15 Connection "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
