---
title: "Introduction to Data Science"
subtitle: "A Remixed Textbook for ICT/LIS 661 at the University of Kentucky [Fall 2023 Edition]"
author: "Spencer P. Greenhalgh, PhD"
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::gitbook:
    includes:
      after_body: hypothesis.html
  #bookdown::pdf_book: default
---

# Introduction

Placeholder



<!--chapter:end:index.Rmd-->


# M1U: Course Syllabus

Placeholder


## Overview
### Course
### Instructor
### Contact Information
### Response Time
### Office Hours
### Meeting Schedule
## Required Materials
## "Life is Difficult" Statement [inspired by [Dr. Andrew Heiss]
## Basic Needs Statement [inspired by Dr. [Sara Goldrick-Rab](https://medium.com/@saragoldrickrab/basic-needs-security-and-the-syllabus-d24cc7afe8c9)]
## Course Information
### Course Description
### Course Objectives—"I Can Statements"
### Course Assessment
#### Projects
#### Participation
### Late Work Policy
### Prep Week
## Course Policies
## Code, Plagiarism, and Generative AI 
## College Statement
## University Statement
### Bias Incident Support Services 
### Counseling Center
### Disability Resource Center
### Martin Luther King Center
### Non-Discrimination / Title IX 
### Office of LGBTQ* Resources 
### Veterans Resource Center (VRC) 
### Violence Intervention and Prevention (VIP) Center 
## Course Schedule
### Module 1: Course Introduction (26 Aug - 1 Sep)
### Module 2: Data Science (2 Sep - 8 Sep)
### Module 3: Reproducibility and Paradigms (9 Sep to 15 Sep)
### Module 4: Data Sharing (16 Sep - 22 Sep)
### Module 5: Theory and Ethics (23 Sep - 29 Sep)
### Module 6: Data Cleaning (30 Sep - 6 Oct)
### Module 7: Data Visualization (7 Oct - 13 Oct)
### Module 8: Descriptive Statistics (14 Oct - 20 Oct)
### Module 9: Basic Regression (21 Oct - 27 Oct)
### Module 10: Multiple Regression (28 Oct - 3 Nov)
### Module 11: Statistical Sampling (4 Nov - 10 Nov)
### Module 12: Confidence Intervals (11 Nov - 17 Nov)
### Module 13: Hypothesis Testing (18 Nov - 24 Nov)
### Module 14: Inferential Regression (25 Nov - 1 Dec)
### Module 15: Course Reflection (2 Dec - 8 Dec)
### Module 16: Final Project (9 Dec - 11 Dec)

<!--chapter:end:01_module_1_understanding.Rmd-->


# M1A: Install R and RStudio

Placeholder


## Introduction
## R
## RStudio
## References

<!--chapter:end:02_module_1_application.Rmd-->

# M1C: Introduce Yourself to the Class

You will complete the Module 1 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> One of the hardest parts about teaching online classes is getting to know my students! For this week's connection activity, please introduce yourself to me and to your classmates. You might include details like your pronouns, what program you're in, your current (or expected future) job, and what you like to do for fun. I'd also be interested to hear why you're taking this class, what you're hoping to learn, and any big questions or concerns that you have.
> 
> As with all discussion posts in this course, I strongly encourage you to read over and respond to what your classmates have posted. However, as important as meaningful interaction is, I'm skeptical that it can be structured or required, so I will never require you to do so. That said, I do expect that you will make an effort to contribute to meaningful interaction within the class. To help with this, I'm experimenting with the "like" feature in Canvas discussions (and with sorting posts by the number of "likes" they receive, though you won't be able to see others' posts before adding your own). Feel free to use "liking" to add to this interaction!

<!--chapter:end:03_module_1_connection.Rmd-->


# M2U: The New(?) and Shiny(?) Science of Data

Placeholder


## Data, Data Science, and Big Data
### What Are Data?
### What Is Data Science?
### What Does "Big Data" Mean?
## The Many Skills of Data Science
## References

<!--chapter:end:04_module_2_understanding.Rmd-->


# M2A: Getting Started with Data in R {#getting-started}

Placeholder


## Introduction
## What are R and RStudio? {#r-rstudio}
### Using R via RStudio
## How do I code in R? {#code}
### Basic programming concepts and terminology {#programming-concepts}
### Errors, warnings, and messages {#messages}
### Tips on learning to code {#tips-code}
## What are R packages? {#packages}
### Package installation {#package-installation}
### Package loading {#package-loading}
### Package use {#package-use}
## Explore your first datasets {#nycflights13}
### `nycflights13` package
### `flights` data frame
### Exploring data frames {#exploredataframes}
### Identification and measurement variables {#identification-vs-measurement-variables}
### Help files
## Conclusion

<!--chapter:end:05_module_2_application.Rmd-->


# M2C: Set up GitHub

Placeholder


## Introduction
## Git—and GitHub—in Data Science
## GitHub in ICT/LIS 661
## Setting Up GitHub
### Creating a GitHub Account
### Setting up a GitHub Repository for Class
### Installing GitHub Desktop
### Files and Folders
## Moving Forward

<!--chapter:end:06_module_2_connection.Rmd-->


# M3U: Research Paradigms and Reproducibility

Placeholder


## Introduction
## Reproducibility and Paradigms
### An Example Paradigm
### Another Example Paradigm
### Why Do Paradigms Matter?
## Supporting Reproducibility
### Open Design
### Open Analysis
### Open Publication
## Conclusion
## References

<!--chapter:end:07_module_3_understanding.Rmd-->


# M3A: Using Projects and Scripts in R

Placeholder


## Introduction
## More About R and RStudio
## The Console and the Workspace
## Holding Onto Code
### RStudio Projects
### Using Scripts
### Backing Up Your Project Folder
## Conclusion 

<!--chapter:end:08_module_3_application.Rmd-->


# M3C: Writing in R Markdown

Placeholder


## Introduction
## Markdown syntax
### Inline formatting
### Block-level elements
### R code chunks
## Trying out R Markdown

<!--chapter:end:09_module_3_connection.Rmd-->


# M4U: The Value of Open Data

Placeholder


## Introduction
## Positivism and the Need for Data
## Sharing Data
## Sharing Data and Privacy
## How to Share Data
## Benefits of Sharing Data
## Downsides of Sharing Data
## Incentives for Sharing Data
## Conclusion
## References

<!--chapter:end:10_module_4_understanding.Rmd-->


# M4A: Find a Dataset Relevant To You

Placeholder


## Introduction
## Signs of a Good Dataset
## Finding a Good Dataset
## Loading Your Dataset {#loading-dataset}

<!--chapter:end:11_module_4_application.Rmd-->


# M4C: Show Your Work

Placeholder


## Introduction
## Invisible Labor
## The Invisible Labor of Data Science
## Time and Money
## Crediting Data Work
## Crediting Emotional Labor and Care Work
## Show Your Work
## References

<!--chapter:end:12_module_4_connection.Rmd-->


# M5U: Numbers Don't Speak for Themselves

Placeholder


## Introduction
## Bigger is not Always Better
## Theory
## The Importance of Theory
## Research isn't Just Empirical
## The Importance of Context
## Conclusion
## References

<!--chapter:end:13_module_5_understanding.Rmd-->


# M5A: Are Ethics Enough in Data Science?

Placeholder


## A Note on Sources
## Introduction
## Ethics vs. Politics
## Why Politics?
### Argument 1: "I am Just an Engineer"
### Argument 2: "We Shouldn't Take Political Stances"
### Argument 3: “Don't Let the Perfect Be the Enemy of the Good”
#### Data Science Lacks a Thorough Definition of "Good"
#### Pursuing an Incremental "Good" Can Reinforce Oppression
## Conclusion
## References

<!--chapter:end:14_module_5_application.Rmd-->

# M5C: Reflect on Theoretical and Philosophical Constraints

You will complete the Module 5 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> Earlier in this module, we established that theory and philosophy can play a helpfully constraining role in data science. To be clear, data is tremendously helpful for re-evaluating theory and philosophy, and if the data are telling us something different than our theory and philosophy, that's something worth paying attention to. Much of the history of science is letting data tell us when and where our philosophical commitments are wrong—the data show that the earth revolves around the sun no matter how committed our philosophy is to keeping the earth at the center of the universe. However, we've also established that numbers don't speak for themselves, and we must also use theory and philosophy to ground and question our findings. For example, even if we found that installing government-run cameras inside citizens' homes was very helpful for catching criminals, our philosophical commitments to human privacy and dignity ought to tell us that this isn't an acceptable policy solution.
> 
> Knowing when to let data override theory and philosophy and when to let theory and philosophy override what the data seem to be telling us is a tricky balance—but we have to be committed to both. In a world where data is increasingly seen as having all the answers, it is especially important that we use theory and philosophy to keep data in check.
> 
> Think about a professional context—a current job, past job, or hoped-for future job—where you could potentially apply your data science or other research skills. What are the theoretical or philosophical commitments that are widely held in that context? Then think about the ways that those commitments could constrain what data you collect, how you interpret data analysis, and how you would act on data. Are some "data-driven decisions" out of the picture? Is that a good thing? A bad thing? Both?
> 
> After thinking about these questions, write a short response to this discussion post that addresses some of the answers that you came up with. Focus in particular on how theory and philosophy might provide a helpful constraint when deciding how to use or act on data.

<!--chapter:end:15_module_5_connection.Rmd-->


# M6U: Unicorns, Janitors, and Rock Stars

Placeholder


## Introduction
## Intentionally Challenging Data Visualization
## Sexy Scientists and... Drab Janitors?
## Strangers in the Dataset
## Sharing Work and Sharing Credit
## Conclusion
## References

<!--chapter:end:16_module_6_understanding.Rmd-->

# M6A: Wrangling and Tidying Data {#wrangling-tidying}

This content draws on material from *[Statistical Inference via Data Science: A ModernDive into R and the Tidyverse](https://moderndive.com/)* by [Chester Ismay](https://chester.rbind.io/) and [Albert Y. Kim](https://rudeboybert.rbind.io/), licensed under [CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

Changes to the source material include addition of new material; light editing; rearranging, removing, and combining original material; adding and changing links; and adding first-person language from current author.

The resulting content is licensed under [CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/).

## Introduction

Two weeks ago, we learned about importing a dataset into R. However, sometimes the dataset that we've found isn't precisely the data that we need. In this walkthrough, we'll practice *wrangling* data and *tidying* data—two skills that help us transform the data we have into the data we want. 

## Wrangling

In this section, we'll cover a series of functions from the `dplyr` package for data wrangling that will allow you to take a data frame and "wrangle" it (transform it) to suit your needs. Such functions include:

1. `filter()` a data frame's existing rows to only pick out a subset of them.
1. `mutate()` its existing columns/variables to create new ones. For example, convert hourly temperature recordings from degrees Fahrenheit to degrees Celsius.
1. `arrange()` its rows. For example, sort rows of a dataframe in ascending or descending order of a particular variable.
1. `select()` a data frame's existing columns to only pick out a subset of them.
1. `rename()` the columns of a data frame.
1. return the `top_n()` values of a particular variable.

Notice how I've used `computer_code` font to describe the actions we want to take on our data frames. This is because the `dplyr` package for data wrangling has intuitively verb-named functions that are easy to remember. 

There is a further benefit to learning to use the `dplyr` package for data wrangling: its similarity to the database querying language [SQL](https://en.wikipedia.org/wiki/SQL) (pronounced "sequel" or spelled out as "S", "Q", "L"). SQL (which stands for "Structured Query Language") is used to manage large databases quickly and efficiently and is widely used by many institutions with a lot of data. While SQL is a topic left for a book or a course on database management, keep in mind that once you learn `dplyr`, you can learn SQL more easily. 

### Needed packages {-#wrangling-packages}

Let's load all the packages needed for our wrangling practice (this assumes you've already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r, message=FALSE}
library(dplyr)
library(nycflights13)
```

```{r message=FALSE, warning=FALSE, echo=FALSE, purl=FALSE}
# Packages needed internally, but not in text.
library(kableExtra)
library(readr)
library(stringr)
library(scales)
library(knitr)
```


### The pipe operator: `%>%` {#piping}

Before we start data wrangling, let's first introduce a nifty tool that gets loaded with the `dplyr` package: the \index{operators!pipe} pipe operator `%>%`. The pipe operator allows us to combine multiple operations in R into a single sequential *chain* of actions.

Let's start with a hypothetical example. (Because this is a hypothetical example, please note that you do not need to—and cannot really—execute the dummy code in this section.) Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame `x` using hypothetical functions `f()`, `g()`, and `h()`:

1. Take `x` *then*
1. Use `x` as an input to a function `f()` *then*
1. Use the output of `f(x)` as an input to a function `g()` *then*
1. Use the output of `g(f(x))` as an input to a function `h()`

One way to achieve this sequence of operations is by using nesting parentheses as follows:

```{r, eval=FALSE, purl=FALSE}
h(g(f(x)))
```

This code isn't so hard to read since we are applying only three functions: `f()`, then `g()`, then `h()`; furthermore, each of these functions has a short name—and only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. 

This is where the pipe operator `%>%` comes in handy. `%>%` takes the output of one function and then "pipes" it to be the input of the next function. With this in mind, one helpful trick is to read `%>%` as "then" or "and then." For example, you can obtain the same output as the hypothetical sequence of functions as follows:

```{r, eval=FALSE, purl=FALSE}
x %>% 
  f() %>% 
  g() %>% 
  h()
```

You would read this sequence as:

1. Take `x` *then*
1. Use this output as the input to the next function `f()` *then*
1. Use this output as the input to the next function `g()` *then*
1. Use this output as the input to the next function `h()`

While both approaches (nested parentheses and pipes) achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical `x`, `f()`, `g()`, and `h()`?  Throughout this walkthrough on data wrangling:

1. The starting value `x` will be a data frame. For example, the \index{R packages!nycflights13} `flights` data frame we explored in Section \@ref(nycflights13).
1. The sequence of functions, here `f()`, `g()`, and `h()`, will mostly be a sequence of any number of the data wrangling verb-named functions we listed in the introduction to this walkthrough
1. The result will be the transformed/modified data frame that you want.

### `filter` rows {#filter}

```{r filter, fig.cap="Diagram of filter() rows operation.", echo=FALSE, purl=FALSE}
include_graphics("images/cheatsheets/filter.png")
```

The \index{dplyr!filter} `filter()` function here works much like the "Filter" option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filter out only the rows that match that criteria.

Let's see what I mean by taking the `flights` dataset that we've used in the past and turning it into a slimmer dataset that includes only flights from New York City to Portland, Oregon.  The `dest` destination code (or airport code) for Portland, Oregon is `"PDX"`. Run the following and look at the results in RStudio's spreadsheet viewer to ensure that only flights heading to Portland are chosen here:

```{r, eval=FALSE}
portland_flights <- flights %>% 
  filter(dest == "PDX")
View(portland_flights)
```

Note the order of the code. First, take the `flights` data frame *then* `filter()` the data frame so that only those where the `dest` equals `"PDX"` are included. We test for equality using the double equal sign \index{operators!==} `==` and not a single equal sign `=`. Trying to run `filter(dest = "PDX")` will yield an error. This is a convention across many programming languages, but if you are new to coding, you'll probably forget to use the double equal sign `==` a few times before you get the hang of it. (To be honest, it's taken me more than a few times to get into this habit!)

You can use other operators \index{operators} beyond just the `==` operator that tests for equality:

- `>` corresponds to "greater than"
- `<` corresponds to "less than"
- `>=` corresponds to "greater than or equal to"
- `<=` corresponds to "less than or equal to"
- `!=` corresponds to "not equal to." The `!` is used in many programming languages to indicate "not"

Furthermore, you can combine multiple criteria using operators that make comparisons:

- `|` corresponds to "or"
- `&` corresponds to "and"

To see many of these in action, let's filter `flights` for all rows that departed from JFK *and* were heading to Burlington, Vermont (`"BTV"`) or Seattle, Washington (`"SEA"`) *and* departed in the months of October, November, or December. Run the following:

```{r, eval=FALSE}
btv_sea_flights_fall <- flights %>% 
  filter(origin == "JFK" & (dest == "BTV" | dest == "SEA") & month >= 10)
View(btv_sea_flights_fall)
```

Note that even though we might say "all flights leaving Burlington, Vermont *and* Seattle, Washington" to another human, for a computer to understand, we have to specify that we really mean "all flights leaving Burlington, Vermont *or* leaving Seattle, Washington." For a given row in the data, `dest` can be `"BTV"`, or `"SEA"`, or something else, but not both `"BTV"` and `"SEA"` at the same time. Furthermore, note the careful use of parentheses around `dest == "BTV" | dest == "SEA"`.

We can often skip the use of `&` and just separate our conditions with a comma. The previous code will return the identical output `btv_sea_flights_fall` as the following code:

```{r, eval=FALSE}
btv_sea_flights_fall <- flights %>% 
  filter(origin == "JFK", (dest == "BTV" | dest == "SEA"), month >= 10)
View(btv_sea_flights_fall)
```

Let's present another example that uses the \index{operators!not} `!` "not" operator to pick rows that *don't* match a criteria. As mentioned earlier, the `!` can be read as "not." Here we are filtering rows corresponding to flights that didn't go to Burlington, VT or Seattle, WA.

```{r, eval=FALSE}
not_BTV_SEA <- flights %>% 
  filter(!(dest == "BTV" | dest == "SEA"))
View(not_BTV_SEA)
```

Again, note the careful use of parentheses around the `(dest == "BTV" | dest == "SEA")`. If we didn't use parentheses, like with the following code:

```{r, eval=FALSE}
flights %>% filter(!dest == "BTV" | dest == "SEA")
```

We would be returning all flights not headed to `"BTV"` *or* those headed to `"SEA"`, which is an entirely different resulting data frame. 

Now say we have a larger number of airports we want to filter for, say `"SEA"`, `"SFO"`, `"PDX"`, `"BTV"`, and `"BDL"`. We could continue to use the `|` (*or*) \index{operators!or} operator:

```{r, eval=FALSE}
many_airports <- flights %>% 
  filter(dest == "SEA" | dest == "SFO" | dest == "PDX" | 
         dest == "BTV" | dest == "BDL")
```

but as we progressively include more airports, this will get unwieldy to write. A slightly shorter approach uses the `%in%` \index{operators!in} operator along with the `c()` function. Recall from Subsection \@ref(programming-concepts) that the `c()` function "combines" or "concatenates" values into a single *vector* of values. \index{vectors}

```{r, eval=FALSE}
many_airports <- flights %>% 
  filter(dest %in% c("SEA", "SFO", "PDX", "BTV", "BDL"))
View(many_airports)
```

What this code is doing is filtering `flights` for all flights where `dest` is in the vector of airports `c("BTV", "SEA", "PDX", "SFO", "BDL")`. Both outputs of `many_airports` are the same, but as you can see the latter takes much less energy to code. The `%in%` operator is useful for looking for matches commonly in one vector/variable compared to another.

As a final note, we recommend that `filter()` should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations you care about. 

### `mutate` existing variables {#mutate}

```{r select, fig.cap="Diagram of mutate() columns.", echo=FALSE, purl=FALSE, out.height='80%', out.width='80%'}
include_graphics("images/cheatsheets/mutate.png")
```

Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius (&deg;C) instead of degrees Fahrenheit (&deg;F). The formula to convert temperatures from &deg;F to &deg;C is

$$
\text{temp in C} = \frac{\text{temp in F} - 32}{1.8}
$$

We can apply this formula to the `temp` variable using the `mutate()` \index{dplyr!mutate()} function from the `dplyr` package, which takes existing variables and mutates them to create new ones. 

```{r, eval=TRUE}
weather <- weather %>% 
  mutate(temp_in_C = (temp - 32) / 1.8)
```

In this code, we `mutate()` the `weather` data frame by creating a new variable `temp_in_C = (temp - 32) / 1.8` and then *overwrite* the original `weather` data frame. Why did we overwrite the data frame `weather`, instead of assigning the result to a new data frame like `weather_new`? As a rough rule of thumb, as long as you are not losing original information that you might need later, it's acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable `temp`, but instead created a new variable called `temp_in_C`?  Because if we did this, we would have erased the original information contained in `temp` of temperatures in Fahrenheit that may still be valuable to us.

Let's consider another example. Passengers are often frustrated when their flight departs late, but aren't as annoyed if, in the end, pilots can make up some time during the flight.  This is known in the airline industry as _gain_, and we will create this variable using the `mutate()` function:

```{r}
flights <- flights %>% 
  mutate(gain = dep_delay - arr_delay)
```

Let's take a look at only the `dep_delay`, `arr_delay`, and the resulting `gain` variables for the first 5 rows in our updated `flights` data frame in Table \@ref(tab:first-five-flights).

```{r first-five-flights, echo=FALSE, purl=FALSE}
flights %>% 
  select(dep_delay, arr_delay, gain) %>% 
  slice(1:5) %>% 
  kable(
    caption = "First five rows of departure/arrival delay and gain variables"
  ) %>%
  kable_styling(position = "center", latex_options = "hold_position")
```

The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its "gained time in the air" is a loss of 9 minutes, hence its `gain` is 2 - 11 = -9. On the other hand, the flight in the fourth row departed a minute early (`dep_delay` of -1) but arrived 18 minutes early (`arr_delay` of -18), so its "gained time in the air" is $-1 - (-18) = -1 + 18 = 17$ minutes, hence its `gain` is +17.

To close out our discussion on the `mutate()` function to create new variables, note that we can create multiple new variables at once in the same `mutate()` code. Furthermore, within the same `mutate()` code we can refer to new variables we just created. As an example, consider the `mutate()` code Wickham and Grolemund (2017) show in [Chapter 5](https://r4ds.had.co.nz/transform.html) of *R for Data Science*:

```{r}
flights <- flights %>% 
  mutate(
    gain = dep_delay - arr_delay,
    hours = air_time / 60,
    gain_per_hour = gain / hours
  )
```

### `arrange` and sort rows {#arrange}

One of the most commonly performed data wrangling tasks is to sort a data frame's rows in the alphanumeric order of one of the variables. The `dplyr` package's `arrange()` function \index{dplyr!arrange()} allows us to sort/reorder a data frame's rows according to the values of the specified variable.

Suppose we are interested in determining the most frequent destination airports for all domestic flights departing from New York City in 2013:

```{r}
freq_dest <- flights %>% 
  group_by(dest) %>% 
  summarize(num_flights = n())
freq_dest
```

Observe that by default the rows of the resulting `freq_dest` data frame are sorted in alphabetical order of `dest`ination. Say instead we would like to see the same data, but sorted from the most to the least number of flights (`num_flights`) instead:

```{r}
freq_dest %>% 
  arrange(num_flights)
```

This is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because `arrange()` always returns rows sorted in ascending order by default. To switch the ordering to be in "descending" order instead, we use the `desc()` \index{dplyr!desc()} function as so:

```{r}
freq_dest %>% 
  arrange(desc(num_flights))
```

### `select` variables {#select}

```{r selectfig, fig.cap="Diagram of select() columns.", echo=FALSE, purl=FALSE}
include_graphics("images/cheatsheets/select.png")
```

```{r echo=FALSE, purl=FALSE}
# This redundant code is used for dynamic non-static in-line text output purposes
# :: operator used as output was wrong otherwise
flights_cols <- nycflights13::flights %>%
  ncol() 
```

We've seen that the `flights` data frame in the `nycflights13` package contains `r flights_cols` different variables. You can identify the names of these `r flights_cols` variables by running the `glimpse()` function from the `dplyr` package:

```{r, eval=FALSE}
glimpse(flights)
```

However, say you only need two of these `r flights_cols` variables, say `carrier` and `flight`. You can `select()` \index{dplyr!select()} these two variables:

```{r, eval=FALSE}
flights %>% 
  select(carrier, flight)
```

This function makes it easier to explore large datasets since it allows us to limit the scope to only those variables we care most about. For example, if we `select()` only a smaller number of variables as is shown in Figure \@ref(fig:selectfig), it will make viewing the dataset in RStudio's spreadsheet viewer more digestible.

Let's say instead you want to drop, or de-select, certain variables. For example, consider the variable `year` in the `flights` data frame. This variable isn't quite a "variable" because it is always `2013` and hence doesn't change. Say you want to remove this variable from the data frame. We can deselect `year` by using the `-` sign:

```{r, eval=FALSE}
flights_no_year <- flights %>% select(-year)
```

Another way of selecting columns/variables is by specifying a range of columns:

```{r, eval=FALSE}
flight_arr_times <- flights %>% select(month:day, arr_time:sched_arr_time)
flight_arr_times
```

This will `select()` all columns between `month` and `day`, as well as between `arr_time` and `sched_arr_time`, and drop the rest. 

The `select()` function can also be used to reorder columns when used with the `everything()` helper function.  For example, suppose we want the `hour`, `minute`, and `time_hour` variables to appear immediately after the `year`, `month`, and `day` variables, while not discarding the rest of the variables. In the following code, `everything()` will pick up all remaining variables: 

```{r, eval=FALSE}
flights_reorder <- flights %>% 
  select(year, month, day, hour, minute, time_hour, everything())
glimpse(flights_reorder)
```

Lastly, the helper functions `starts_with()`, `ends_with()`, and `contains()` can be used to select variables/columns that match those conditions. As examples,

```{r, eval=FALSE}
flights %>% select(starts_with("a"))
flights %>% select(ends_with("delay"))
flights %>% select(contains("time"))
```


### `rename` variables {#rename}

Another useful function is \index{dplyr!rename()} `rename()`, which as you may have guessed changes the name of variables. Suppose we want to only focus on `dep_time` and `arr_time` and change `dep_time` and `arr_time` to be `departure_time` and `arrival_time` instead in the `flights_time` data frame:

```{r, eval=FALSE}
flights_time_new <- flights %>% 
  select(dep_time, arr_time) %>% 
  rename(departure_time = dep_time, arrival_time = arr_time)
glimpse(flights_time_new)
```

Note that in this case we used a single `=` sign within the `rename()`. For example, `departure_time = dep_time` renames the `dep_time` variable to have the new name `departure_time`. This is because we are not testing for equality like we would using `==`. Instead we want to assign a new variable `departure_time` to have the same values as `dep_time` and then delete the variable `dep_time`. Note that new `dplyr` users often forget that the new variable name comes before the equal sign. <!-- We usually remember this as "New Before, Old After" or NBOA. -->


### `top_n` values of a variable

We can also return the top `n` values of a variable using the `top_n()` \index{dplyr!top\_n()} function. For example, we can return a data frame of the top 10 destination airports in the dataframe we've already established. Observe that we set the number of values to return to `n = 10` and `wt = num_flights` to indicate that we want the rows corresponding to the top 10 values of `num_flights`. See the help file for `top_n()` by running `?top_n` for more information. 

```{r, eval=FALSE}
freq_dest %>% top_n(n = 10, wt = num_flights)
```

Let's further `arrange()` these results in descending order of `num_flights`:

```{r, eval=FALSE}
freq_dest  %>% 
  top_n(n = 10, wt = num_flights) %>% 
  arrange(desc(num_flights))
```

## Tidying Data

```{r setup_tidy, include=FALSE, purl=FALSE}
# Used to define Learning Check numbers:
chap <- 4
lc <- 0

# Set R code chunk defaults:
opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  warning = FALSE,
  message = TRUE,
  tidy = FALSE,
  purl = TRUE,
  out.width = "\\textwidth",
  fig.height = 4,
  fig.align = "center"
)

# Set output digit precision
options(scipen = 99, digits = 3)

# In kable printing replace all NA's with blanks
options(knitr.kable.NA = "")

# Set random number generator see value for replicable pseudorandomness.
set.seed(76)
```

In Subsection \@ref(programming-concepts), I introduced the concept of a \index{data frame} data frame in R: a rectangular spreadsheet-like representation of data where the rows correspond to observations and the columns correspond to variables describing each observation.  In Section \@ref(nycflights13), we started exploring our first data frame: the `flights` data frame included in the `nycflights13` package. 

In the first section of this walkthrough, we learned how to take existing data frames and transform/modify them to suit our ends. In this second section, we extend some of these ideas by discussing a type of data formatting called "tidy" data. You will see that having data stored in "tidy" format is about more than just what the everyday definition of the term "tidy" might suggest: having your data "neatly organized." Instead, we define the term "tidy" as it's used by data scientists who use R, outlining a set of rules by which data is saved.

Knowing about tidy data was not necessary earlier because all the data we used were already in "tidy" format. In this walkthrough, we'll now see that this format is essential to using the tools we've covered. Furthermore, it will also be useful for future walkthroughs as we cover future ideas.

### Needed packages {-#tidy-packages}

Let's load all the packages needed for this walkthrough (this assumes you've already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r message=FALSE}
library(dplyr)
library(readr)
library(tidyr)
library(nycflights13)
library(fivethirtyeight)
library(here)
```

```{r message=FALSE, echo=FALSE, purl=FALSE}
# Packages needed internally, but not in text.
library(kableExtra)
library(stringr)
library(scales)
```

### Importing data {#csv}

Up to this point, most of our activities have used data stored inside of an R package, though we *have* practiced importing data from an outside source in Section \@ref(loading-dataset). It's time to put that practice to use!

To practice tidying data, we need to load a *Comma Separated Values* `.csv` \index{CSV file} file, the same format that I encouraged you to find for your personal dataset.  You can think of a `.csv` file as a bare-bones spreadsheet where:

* Each line in the file corresponds to one row of data/one observation.
* Values for each line are separated with commas. In other words, the values of different variables are separated by commas in each row.
* The first line is often, but not always, a *header* row indicating the names of the columns/variables.

The `.csv` file we're interested in here is `dem_score.csv`—it is found in the `activity_data` subfolder of your class project folder and contains ratings of the level of democracy in different countries spanning 1952 to 1992. Let's use the `read_csv()` function from the `readr` package (with some help from the `@here` package) to read it from your computer, import it into R, and save it in a data frame called `dem_score`.

```{r message=FALSE}
library(readr)
dem_score <- read_csv(here("activity_data","dem_score.csv"))
dem_score
```


In this `dem_score` data frame, the minimum value of `-10` corresponds to a highly autocratic nation, whereas a value of `10` corresponds to a highly democratic nation. Note also that backticks surround the different variable names.  Variable names in R by default are not allowed to start with a number nor include spaces, but we can get around this fact by surrounding the column name with backticks. We'll revisit the `dem_score` data frame in a case study in the upcoming Section \@ref(case-study-tidy).

Note that the `read_csv()` function included in the `readr` package is different than the `read.csv()` function that comes installed with R. While the difference in the names might seem trivial (an `_` instead of a `.`), the `read_csv()` function is, in my opinion, easier to use since it can more easily read data off the web and generally imports data at a much faster speed. Furthermore, the `read_csv()` function included in the `readr` saves data frames as `tibbles` by default. 

### "Tidy" data {#tidy-data-ex}

With our data imported, let's learn about the concept of "tidy" data format with a motivating example from the `fivethirtyeight` package. The `fivethirtyeight` package provides access to the datasets used in many articles published by the data journalism website, [FiveThirtyEight.com](https://fivethirtyeight.com/). For a complete list of all `r nrow(data(package = "fivethirtyeight")[[3]])` datasets included in the `fivethirtyeight` package, check out the package webpage by going to: <https://fivethirtyeight-r.netlify.app/articles/fivethirtyeight.html>.\index{R packages!fivethirtyeight}

Let's focus our attention on the `drinks` data frame and look at its first 5 rows:

```{r, echo=FALSE, purl=FALSE}
drinks %>%
  head(5)
```

After reading the help file by running `?drinks`, you'll see that `drinks` is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed in `r drinks %>% nrow()` countries. This data was originally reported on FiveThirtyEight.com in Mona Chalabi's article: ["Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?"](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/). It's important to note that Chalabi is no longer with FiveThirtyEight and [has been sharply critical](https://x.com/MonaChalabi/status/1270715883531300866) of the organization since her leaving.

Let's apply some of the data wrangling verbs we learned in Section \@ref(wrangling) on the `drinks` data frame:

1. `filter()` the `drinks` data frame to only consider 4 countries: the United States, China, Italy, and Saudi Arabia, *then*
1. `select()` all columns except `total_litres_of_pure_alcohol` by using the `-` sign, *then*
1. `rename()` the variables `beer_servings`, `spirit_servings`, and `wine_servings` to `beer`, `spirit`, and `wine`, respectively.

and save the resulting data frame in `drinks_smaller`:

```{r}
drinks_smaller <- drinks %>% 
  filter(country %in% c("USA", "China", "Italy", "Saudi Arabia")) %>% 
  select(-total_litres_of_pure_alcohol) %>% 
  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)
drinks_smaller
```

Let's compare this way of organizing the data with an alternative:

```{r, purl=FALSE}
drinks_smaller_tidy <- drinks_smaller %>% 
  pivot_longer(names_to = "type", 
               values_to = "servings", 
               cols = -country)
drinks_smaller_tidy
```

```{r echo=FALSE, purl=FALSE}
# This redundant code is used for dynamic non-static in-line text output purposes
n_row_drinks <- drinks_smaller_tidy %>% nrow()
n_alcohol_types <- drinks_smaller_tidy %>%
  select(type) %>%
  n_distinct()
n_countries <- drinks_smaller_tidy %>%
  select(country) %>%
  n_distinct()
```

Observe that while `drinks_smaller` and `drinks_smaller_tidy` are both rectangular in shape and contain the same `r n_row_drinks` numerical values (`r n_alcohol_types` alcohol types by `r n_countries` countries), they are formatted differently. `drinks_smaller` is formatted in what's known as \index{wide data format} ["wide"](https://en.wikipedia.org/wiki/Wide_and_narrow_data) format, whereas `drinks_smaller_tidy` is formatted in what's known as ["long/narrow"](https://en.wikipedia.org/wiki/Wide_and_narrow_data#Narrow) format.

In the context of doing data science in R, long/narrow format \index{long data format} is also known as "tidy" format. In order to use the packages that we prefer in this class, your input data frames *must* be in "tidy" format. Thus, all non-"tidy" data must be converted to "tidy" format first. Before we convert non-"tidy" data frames like `drinks_smaller` to "tidy" data frames like `drinks_smaller_tidy`, let's define "tidy" data.

### Definition of "tidy" data {#tidy-definition}

You have surely heard the word "tidy" in your life:

* "Tidy up your room!"
* "Write your homework in a tidy way so it is easier to provide feedback."
* Marie Kondo's best-selling book, [_The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing_](https://www.powells.com/book/-9781607747307), and Netflix TV series [_Tidying Up with Marie Kondo_](https://www.netflix.com/title/80209379).
* "I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - 'Read me, please!'" - Linda Grant

What does it mean for your data to be "tidy"? While "tidy" has a clear English meaning of "organized," the word "tidy" in data science using R means that your data follows a standardized format. We will follow Hadley Wickham's (2014) definition of *"tidy" data*  shown also in Figure \@ref(fig:tidyfig):

> A *dataset* is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes.
> 
> "Tidy" data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In *tidy data*:
>
> 1. Each variable forms a column.
> 2. Each observation forms a row.
> 3. Each type of observational unit forms a table.

(ref:tidy-r4ds) Tidy data graphic from *R for Data Science*.

```{r tidyfig, echo=FALSE, fig.cap="(ref:tidy-r4ds)", out.height="80%", out.width="80%", purl=FALSE}
include_graphics("images/r4ds/tidy-1.png")
```

For example, say you have the following table of stock prices in Table \@ref(tab:non-tidy-stocks):

```{r non-tidy-stocks, echo=FALSE, purl=FALSE}
stocks <- tibble(
  Date = as.Date("2009-01-01") + 0:4,
  `Boeing stock price` = paste("$", c("173.55", "172.61", "173.86", "170.77", "174.29"), sep = ""),
  `Amazon stock price` = paste("$", c("174.90", "171.42", "171.58", "173.89", "170.16"), sep = ""),
  `Google stock price` = paste("$", c("174.34", "170.04", "173.65", "174.87", "172.19"), sep = "")
) %>%
  slice(1:2)
stocks %>%
  kable(
    digits = 2,
    caption = "Stock prices (non-tidy format)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

Although the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in "tidy" format. While there are three variables corresponding to three unique pieces of information (date, stock name, and stock price), there are not three columns. In "tidy" data format, each variable should be its own column, as shown in Table \@ref(tab:tidy-stocks). Notice that both tables present the same information, but in different formats. 

```{r tidy-stocks, echo=FALSE, purl=FALSE}
stocks_tidy <- stocks %>%
  rename(
    Boeing = `Boeing stock price`,
    Amazon = `Amazon stock price`,
    Google = `Google stock price`
  ) %>%
  #  gather(`Stock name`, `Stock price`, -Date)
  pivot_longer(
    cols = -Date,
    names_to = "Stock Name",
    values_to = "Stock Price"
  )
stocks_tidy %>%
  kable(
    digits = 2,
    caption = "Stock prices (tidy format)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

Now we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider the data in Table \@ref(tab:tidy-stocks-2).

```{r tidy-stocks-2, echo=FALSE, purl=FALSE}
stocks <- tibble(
  Date = as.Date("2009-01-01") + 0:4,
  `Boeing Price` = paste("$", c("173.55", "172.61", "173.86", "170.77", "174.29"), sep = ""),
  `Weather` = c("Sunny", "Overcast", "Rain", "Rain", "Sunny")
) %>%
  slice(1:2)
stocks %>%
  kable(
    digits = 2,
    caption = "Example of tidy data" # ,
    #    booktabs = TRUE
  ) %>%
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

In this case, even though the variable "Boeing Price" occurs just like in our non-"tidy" data in Table \@ref(tab:non-tidy-stocks), the data *is* "tidy" since there are three variables corresponding to three unique pieces of information: Date, Boeing price, and the Weather that particular day.

### Converting to "tidy" data

In our walkthroughs so far, you've only seen data frames that were already in "tidy" format. Furthermore, for the rest of this book, you'll mostly only see data frames that are already in "tidy" format as well. However, this is not always the case with all datasets—including, perhaps, the dataset that you've chosen to work with for your class proejcts. If your original data frame is in wide (non-"tidy") format and you would like to use the packages we learn about, you will first have to convert it to "tidy" format. To do so, we recommend using the \index{tidyr!pivot\_longer()} `pivot_longer()` function in the `tidyr` package. 

Going back to our `drinks_smaller` data frame from earlier:

```{r}
drinks_smaller
```

We convert it to "tidy" format by using the `pivot_longer()` function from the `tidyr` package as follows:

```{r}
drinks_smaller_tidy <- drinks_smaller %>% 
  pivot_longer(names_to = "type", 
               values_to = "servings", 
               cols = -country)
drinks_smaller_tidy
```

We set the arguments to `pivot_longer()` as follows:

1. `names_to` here corresponds to the name of the variable in the new "tidy"/long data frame that will contain the *column names* of the original data. Observe how we set `names_to = "type"`. In the resulting `drinks_smaller_tidy`, the column `type` contains the three types of alcohol `beer`, `spirit`, and `wine`. Since `type` is a variable name that doesn't appear in `drinks_smaller`, we use quotation marks around it. You'll receive an error if you just use `names_to = type` here.
1. `values_to` here is the name of the variable in the new "tidy" data frame that will contain the *values* of the original data. Observe how we set `values_to = "servings"` since each of the numeric values in each of the `beer`, `wine`, and `spirit` columns of the `drinks_smaller` data corresponds to a value of `servings`. In the resulting `drinks_smaller_tidy`, the column `servings` contains the `r n_countries` $\times$ `r n_alcohol_types` = `r n_row_drinks` numerical values. Note again that `servings` doesn't appear as a variable in `drinks_smaller` so it again needs quotation marks around it for the `values_to` argument.
1. The third argument `cols` is the columns in the `drinks_smaller` data frame you either want to or don't want to "tidy." Observe how we set this to `-country` indicating that we don't want to "tidy" the `country` variable in `drinks_smaller` and rather only `beer`, `spirit`, and `wine`. Since `country` is a column that appears in `drinks_smaller` we don't put quotation marks around it.

The third argument here of `cols` is a little nuanced, so let's consider code that's written slightly differently but that produces the same output: 

```{r, eval=FALSE}
drinks_smaller %>% 
  pivot_longer(names_to = "type", 
               values_to = "servings", 
               cols = c(beer, spirit, wine))
```

Note that the third argument now specifies which columns we want to "tidy" with `c(beer, spirit, wine)`, instead of the columns we don't want to "tidy" using `-country`. We use the `c()` function to create a vector of the columns in `drinks_smaller` that we'd like to "tidy." Note that since these three columns appear one after another in the `drinks_smaller` data frame, we could also do the following for the `cols` argument:

```{r, eval=FALSE}
drinks_smaller %>% 
  pivot_longer(names_to = "type", 
               values_to = "servings", 
               cols = beer:wine)
```

Converting "wide" format data to "tidy" format often confuses new R users. The only way to learn to get comfortable with the `pivot_longer()` function is with practice, practice, and more practice using different datasets. For example, run `?pivot_longer` and look at the examples in the bottom of the help file. We'll show another example of using `pivot_longer()` to convert a "wide" formatted data frame to "tidy" format in Section \@ref(case-study-tidy). 


#### `nycflights13` package

Recall the `nycflights13` package with data about all domestic flights departing from New York City in 2013. Let's revisit the `flights` data frame by running `View(flights)`. We saw that `flights` has a rectangular shape, with each of its `r flights %>% nrow() %>% comma()` rows corresponding to a flight and each of its `r flights %>% ncol()` columns corresponding to different characteristics/measurements of each flight. This satisfied the first two criteria of the definition of "tidy" data from Subsection \@ref(tidy-definition): that "Each variable forms a column" and "Each observation forms a row." But what about the third property of "tidy" data that "Each type of observational unit forms a table"?

Recall that we saw in Subsection \@ref(exploredataframes) that the observational unit for the `flights` data frame is an individual flight. In other words, the rows of the `flights` data frame refer to characteristics/measurements of individual flights. Also included in the `nycflights13` package are other data frames with their rows representing different observational units:

* `airlines`: translation between two letter IATA carrier codes and airline company names (`r airlines %>% nrow()` in total). The observational unit is an airline company.
* `planes`: aircraft information about each of `r planes %>% nrow() %>% comma()` planes used, i.e., the observational unit is an aircraft.
* `weather`: hourly meteorological data (about `r weather %>% count(origin) %>% .[["n"]] %>% mean() %>% round() %>% comma()` observations) for each of the three NYC airports, i.e., the observational unit is an hourly measurement of weather at one of the three airports.
* `airports`: airport names and locations. The observational unit is an airport.

The organization of the information into these five data frames follows the third "tidy" data property: observations corresponding to the same observational unit should be saved in the same table, i.e., data frame. You could think of this property as the old English expression: "birds of a feather flock together." 

### Case study: Democracy in Guatemala {#case-study-tidy}

In this section, we'll show you another example of how to convert a data frame that isn't in "tidy" format ("wide" format) to a data frame that is in "tidy" format ("long/narrow" format). We'll do this using the `pivot_longer()` function from the `tidyr` package again. 

Let's use the `dem_score` data frame we imported in Section \@ref(csv), but focus on only data corresponding to Guatemala.

```{r}
guat_dem <- dem_score %>% 
  filter(country == "Guatemala")
guat_dem
```

Let's lay out the grammar of graphics we saw in Section \@ref(grammarofgraphics). 

Now we are stuck in a predicament, much like with our `drinks_smaller` example in Section \@ref(tidy-data-ex). We see that we have a variable named `country`, but its only value is `"Guatemala"`.  We have other variables denoted by different year values.  Unfortunately, the `guat_dem` data frame is not "tidy."

We need to take the values of the columns corresponding to years in `guat_dem` and convert them into a new "names" variable called `year`. Furthermore, we need to take the democracy score values in the inside of the data frame and turn them into a new "values" variable called `democracy_score`. Our resulting data frame will have three columns:  `country`, `year`, and `democracy_score`. Recall that the `pivot_longer()` function in the `tidyr` package does this for us:

```{r}
guat_dem_tidy <- guat_dem %>% 
  pivot_longer(names_to = "year", 
               values_to = "democracy_score", 
               cols = -country,
               names_transform = list(year = as.integer)) 
guat_dem_tidy
```

We set the arguments to `pivot_longer()` as follows:

1. `names_to` is the name of the variable in the new "tidy" data frame that will contain the *column names* of the original data. Observe how we set `names_to = "year"`.  In the resulting `guat_dem_tidy`, the column `year` contains the years where Guatemala's democracy scores were measured.
1. `values_to` is the name of the variable in the new "tidy" data frame that will contain the *values* of the original data. Observe how we set `values_to = "democracy_score"`. In the resulting `guat_dem_tidy` the column `democracy_score` contains the 1 $\times$ 9 = 9 democracy scores as numeric values.
1. The third argument is the columns you either want to or don't want to "tidy." Observe how we set this to `cols = -country` indicating that we don't want to "tidy" the `country` variable in `guat_dem` and rather only variables `1952` through `1992`. 
1. The last argument of `names_transform` tells R what type of variable `year` should be set to. Without specifying that it is an `integer` as we've done here, `pivot_longer()` will set it to be a character value by default.


## `tidyverse` package {#tidyverse-package}

At the beginning of our tidying walkthrough, we loaded the following three packages (as well as `here`), which are among the most frequently used R packages for data science:

```{r, eval=FALSE, purl=FALSE}
library(dplyr)
library(readr)
library(tidyr)
```

Recall that `dplyr` is for data wrangling, `readr` is for importing spreadsheet data into R, and `tidyr` is for converting data to "tidy" format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the `tidyverse` package. The `tidyverse` package acts as an "umbrella" package whereby installing/loading it will install/load multiple packages at once for you. 

After installing the `tidyverse` package as you would a normal package (as seen in Section \@ref(packages)), running:

```{r, eval=FALSE, purl=FALSE}
library(tidyverse)
```

would be the same as running:

```{r, eval=FALSE, purl=FALSE}
library(ggplot2)
library(dplyr)
library(readr)
library(tidyr)
library(purrr)
library(tibble)
library(stringr)
library(forcats)
```

We'll cover `ggplot2` in a future walk through, but the other packages are more than we need in 661. You can check out *[R for Data Science](https://r4ds.had.co.nz/)* to learn about these packages.

For the remainder of this book, we'll start every walkthrough by running `library(tidyverse)`, instead of loading the various component packages individually. The `tidyverse` "umbrella" package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames are in "tidy" format. This standardization of input and output data frames makes transitions between different functions in the different packages as seamless as possible. For more information, check out the [tidyverse.org](https://www.tidyverse.org/) webpage for the package.

## References

Grolemund, G., & Wickham, H. (2017). *R for Data Science* (1st edition). O’Reilly Media. https://r4ds.had.co.nz/.

Wickham, H. (2014). Tidy data. *Journal of Statistical Software*, *59*(10). https://www.jstatsoft.org/index.php/jss/article/view/v059i10/v59i10.pdf. 

<!--chapter:end:17_module_6_application.Rmd-->

# M6C: Practice Wrangling and Tidying Your Own Data

You will complete the Module 6 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:18_module_6_connection.Rmd-->


# M7U: Subjectivity in Data Visualization

Placeholder


## Introduction
## Persuasion and the God Trick
## Visualization as Rhetoric
## Editorial Choices in Visualization
## Visualization as Ideological Work
## Feminist Objectivity
## Data Visceralization
## Conclusion
## References

<!--chapter:end:19_module_7_understanding.Rmd-->


# M7A: Data Visualization {#viz}

Placeholder


## Introduction
### Needed packages {-}
## The grammar of graphics {#grammarofgraphics}
### Components of the grammar
### Gapminder data {#gapminder}
### Other components
### ggplot2 package
## Five named graphs - the 5NG {#FiveNG}
## 5NG#1: Scatterplots {#scatterplots}
### Scatterplots via `geom_point` {#geompoint}
### Overplotting {#overplotting}
### Summary
## 5NG#2: Linegraphs {#linegraphs}
### Linegraphs via `geom_line` {#geomline}
### Summary
## 5NG#3: Histograms {#histograms}
### Histograms via `geom_histogram` {#geomhistogram}
### Adjusting the bins {#adjustbins}
### Summary
## Facets {#facets}
## 5NG#4: Boxplots {#boxplots}
### Boxplots via `geom_boxplot` {#geomboxplot}
### Summary
## 5NG#5: Barplots {#geombar}
### Barplots via `geom_bar` or `geom_col`
### Avoid pie charts!
### Two categorical variables {#two-categ-barplot}
### Summary
## Conclusion {#data-vis-conclusion}
### Summary table
### Function argument specification
### Additional resources 
## References

<!--chapter:end:20_module_7_application.Rmd-->

# M7C: Practice Visualizing Your Own Data

You will complete the Module 7 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:21_module_7_connection.Rmd-->


# M8U: Statistics and Scientific Racism

Placeholder


## Introduction
## The Origins of Statistics
## The Importance of Statistical Humility
## Eugenics: A Failure of Statistical Humility
## The Continuing Legacy of Eugenics and Scientific Racism
## Conclusion
## References

<!--chapter:end:22_module_8_understanding.Rmd-->


# M8A: Descriptive Statistics

Placeholder


## Introduction
### Needed packages {-#wrangling-packages-2}
## `summarize` variables {#summarize}
### Mean
### Standard Deviation
#### Measure of Overall Variation
#### Determining Whether a Data Value is Close To or Far From the Mean
#### Calculating the Standard Deviation
### Calculating Mean with `mean()` and Standard Deviation with `sd()`
### Percentiles, Quartiles and `IQR()`
## `group_by` rows {#groupby}
### Grouping by more than one variable
## Conclusion

<!--chapter:end:23_module_8_application.Rmd-->

# M8C: Calculate Descriptive Statistics for Your Own Data

You will complete the Module 8 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:24_module_8_connection.Rmd-->


# M9U: Linear Regression

Placeholder


## Introduction
## Linear Equations
## Linear Relationships and Scatterplots
## The Regression Equation
## Least Squares Criteria for Best Fit
## Correlation Coefficients
## Conclusion

<!--chapter:end:25_module_9_understanding.Rmd-->


# M9A: Basic Regression {#regression}

Placeholder


## Introduction
### Needed packages {-#reg-packages}
## One numerical explanatory variable {#model1}
### Exploratory data analysis {#model1EDA}
### Simple linear regression {#model1table}
### Observed/fitted values and residuals {#model1points}
## One categorical explanatory variable {#model2}
### Exploratory data analysis {#model2EDA}
### Linear regression {#model2table}
### Observed/fitted values and residuals {#model2points}
## Related topics {#reg-related-topics}
### Correlation is not necessarily causation {#correlation-is-not-causation}
### Best-fitting line {#leastsquares}
### `get_regression_x()` functions {#underthehood}
## Conclusion {#reg-conclusion}
## References

<!--chapter:end:26_module_9_application.Rmd-->

# M9C: Perform a Basic Regression With Your Own Data

You will complete the Module 9 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:27_module_9_connection.Rmd-->


# M10U: Consequences of Failed Predictions

Placeholder


## Introduction
## Regression and Automated Content Moderation
## The Problem with Automated Content Moderation
## An Example from Instagram
## Conclusion

<!--chapter:end:28_module_10_understanding.Rmd-->


# M10A: Multiple Regression {#multiple-regression}

Placeholder


## Introduction
### Needed packages {-#mult-reg-packages}
## One numerical and one categorical explanatory variable {#model4}
### Exploratory data analysis {#model4EDA}
### Interaction model {#model4interactiontable}
### Parallel slopes model {#model4table}
### Observed/fitted values and residuals {#model4points}
## Two numerical explanatory variables {#model3}
### Exploratory data analysis {#model3EDA}
### Regression plane {#model3table}
### Observed/fitted values and residuals {#model3points}
## Related topics {#mult-reg-related-topics}
### Model selection using visualizations {#model-selection}
### Model selection using R-squared {#rsquared}
### Correlation coefficient {#correlationcoefficient2}
### Simpson's Paradox {#simpsonsparadox}
## Conclusion {#mult-reg-conclusion}
## References

<!--chapter:end:29_module_10_application.Rmd-->

# M10C: Perform a Multiple Regression With Your Own Data

You will complete the Module 10 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:30_module_10_connection.Rmd-->


# M11U: Samples and Populations

Placeholder


## Introduction
## Samples 
### Variation in Samples
### Sample Sizes
## Probability
## Expected Values and the Law of Large Numbers
## Probability and Sampling
## Probability and the Normal Distribution
## Conclusion

<!--chapter:end:31_module_11_understanding.Rmd-->


# M11A: Sampling {#sampling}

Placeholder


## Introduction
### Needed packages {-#sampling-packages}
## Sampling bowl activity {#sampling-activity}
### What proportion of this bowl's balls are red?
### Using the shovel once 
### Using the shovel 33 times {#student-shovels}
### What did we just do? {#sampling-what-did-we-just-do}
## Virtual sampling {#sampling-simulation}
### Using the virtual shovel once
### Using the virtual shovel 33 times
### Using the virtual shovel 1000 times {#shovel-1000-times}
### Using different shovels {#different-shovels}
## Sampling framework {#sampling-framework}
### Terminology and notation {#terminology-and-notation}
### Statistical definitions {#sampling-definitions}
### The moral of the story {#moral-of-the-story}
## Case study: Polls {#sampling-case-study}
## Central Limit Theorem {#sampling-conclusion-central-limit-theorem}
## Conclusion {#sampling-conclusion}
### What's to come?

<!--chapter:end:32_module_11_application.Rmd-->

# M11C: Explore Sampling With Your Own

You will complete the Module 11 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:33_module_11_connection.Rmd-->


# M12U: Confident About What?

Placeholder


## Introduction
## #TravelingWhileTrans
## Conclusion
## References

<!--chapter:end:34_module_12_understanding.Rmd-->


# M12A: Confidence Intervals {#confidence-intervals}

Placeholder


## Introduction
### Needed packages {-#CI-packages}
## Pennies activity {#resampling-tactile}
### What is the average year on US pennies in 2019?
### Resampling once
### Resampling `r n_resample_friends` times {#student-resamples}
### What did we just do? {#ci-what-did-we-just-do}
## Computer simulation of resampling {#resampling-simulation}
### Virtually resampling once
### Virtually resampling `r n_resample_friends` times {#bootstrap-35-replicates}
### Virtually resampling `r n_virtual_resample` times {#bootstrap-1000-replicates}
## Understanding confidence intervals {#ci-build-up}
### Percentile method {#percentile-method}
### Standard error method {#se-method}
## Constructing confidence intervals {#bootstrap-process}
### Original workflow
### `infer` package workflow {#infer-workflow}
#### 1. `specify` variables {-}
#### 2. `generate` replicates {-}
#### 3. `calculate` summary statistics {-}
#### 4. `visualize` the results {-}
### Percentile method with `infer` {#percentile-method-infer}
### Standard error method with `infer` {#infer-se}
## Interpreting confidence intervals {#one-prop-ci}
### Did the net capture the fish? {#ilyas-yohan}
#### 1. `specify` variables {-}
#### 2. `generate` replicates {-}
#### 3. `calculate` summary statistics {-}
#### 4. `visualize` the results {-}
### Precise and shorthand interpretation {#shorthand}
### Width of confidence intervals {#ci-width}
#### Impact of confidence level {-}
#### Impact of sample size {-}
## Case study: Is yawning contagious? {#case-study-two-prop-ci}
### *Mythbusters* study data
### Sampling scenario
### Constructing the confidence interval {#ci-build}
#### 1. `specify` variables {-}
#### 2. `generate` replicates {-}
#### 3. `calculate` summary statistics {-}
#### 4. `visualize` the results {-}
### Interpreting the confidence interval
## Conclusion {#ci-conclusion}

<!--chapter:end:35_module_12_application.Rmd-->

# M12C: Explore Confidence Intervals With Your Own Data

You will complete the Module 12 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:36_module_12_connection.Rmd-->


# M13U: The Dangers of False Positives

Placeholder


## Introduction
## The Dartmouth Cheating Scandal
## What Was Wrong With Dartmouth’s Investigation
## Investigations of Students Must Start With Concrete Evidence
## A Problem with Companies and Transparency
## Not Just Dartmouth
## Conclusion

<!--chapter:end:37_module_13_understanding.Rmd-->


# M13: Hypothesis Testing {#hypothesis-testing}

Placeholder


## Introduction
### Needed packages {-#nhst-packages}
## Promotions activity {#ht-activity}
### Does gender affect promotions at a bank?
### Shuffling once
### Shuffling `r n_shuffle` times
### What did we just do? {#ht-what-did-we-just-do}
## Understanding hypothesis tests {#understanding-ht}
## Conducting hypothesis tests {#ht-infer}
### `infer` package workflow {#infer-workflow-ht}
#### 1. `specify` variables {-}
#### 2. `hypothesize` the null {-}
#### 3. `generate` replicates {-}
#### 4. `calculate` summary statistics {-}
#### 5. `visualize` the p-value {-}
### Comparison with confidence intervals {#comparing-infer-workflows}
### "There is only one test" {#only-one-test}
## Interpreting hypothesis tests {#ht-interpretation}
### Two possible outcomes {#trial}
### Types of errors
### How do we choose alpha? {#choosing-alpha}
## Case study: Are action or romance movies rated higher? {#ht-case-study}
### IMDb ratings data {#imdb-data}
### Sampling scenario
### Conducting the hypothesis test
#### 1. `specify` variables {-}
#### 2. `hypothesize` the null {-}
#### 3. `generate` replicates {-}
#### 4. `calculate` summary statistics {-}
#### 5. `visualize` the p-value {-}
## Conclusion {#nhst-conclusion}

<!--chapter:end:38_module_13_application.Rmd-->

# M13C: Explore Hypothesis Testing With Your Own Data

You will complete the Module 13 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:39_module_13_connection.Rmd-->


# M14U: Small Stories vs. Big Data

Placeholder


## Introduction
## The Origins of Autoethnography
## What does Autoethnography Look Like?
## How do Autoethnographers Measure Quality?
## Conclusion
## References

<!--chapter:end:40_module_14_understanding.Rmd-->


# M14A: Inferential Regression {#inference-for-regression}

Placeholder


## Introduction
### Needed packages {-#inf-packages}
## Regression refresher
### Teaching evaluations analysis
### Sampling scenario
## Interpreting regression tables {#regression-interp}
### Standard error {#regression-se}
### Test statistic {#regression-test-statistic}
### p-value
### Confidence interval
### How does R compute the table? {#regression-table-computation}
## Conditions for inference for regression {#regression-conditions}
### Residuals refresher
### Linearity of relationship
### Independence of residuals
### Normality of residuals
### Equality of variance
### What's the conclusion? {#what-is-the-conclusion}
## Simulation-based inference for regression {#infer-regression}
### Confidence interval for slope
#### Percentile-method {-}
#### Standard error method {-}
#### Comparing all three {-}
### Hypothesis test for slope
## Conclusion {#inference-conclusion}

<!--chapter:end:41_module_14_application.Rmd-->

# M14C: Perform an Inferential Regression With Your Own Data

You will complete the Module 14 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> We've hit a point in the semester where our connection activities will turn toward repeating the application activities with an eye on your own data. This is helpful for practicing these data science tasks—and for ensuring that you feel comfortable carrying them out for your data for the upcoming projects.
> 
> To get credit for this assignment, please repeat at least one significant portion of this week's application activity, adapting the code to work for your own data. It's ultimately up to you how much of the activity you choose to repeat with your data. On one hand, the more that you do, the better prepared you'll be for upcoming projects. On the other hand, sometimes a particular part of a walkthrough just isn't relevant for your data—or you run into some issues early on that get in the way of completing the later stuff. In balancing these two "hands," think of this connection activity as an opportunity to get ready for the next project.
> 
When you're ready, take a screenshot of your work (you can visit [take-a-screenshot.org](https://www.take-a-screenshot.org)) for instructions on how to do this—please do not take a picture of your screen with a camera). Upload your screenshot to this discussion board along with a brief reflection on how the activity went. You can also post questions or concerns about the activity to this discussion board—they will not count as completing the activity, but I check this board regularly and will chime in with help. If you see a classmate who needs help and know how to provide that help, feel free to answer their question before I get there!

<!--chapter:end:42_module_14_connection.Rmd-->

# M15U: Reflect on Your Understanding of Data Science

You will complete the Module 15 Understanding activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> Most weeks this semester, our *understanding* activity focused on introducing you to key ideas related either to data science itself—usually related to research, statistics, or something similar—or to key critiques of data science—usually related to paradigms, bias, or other ways of thinking. The purpose of these activities was to give you a solid grounding in how to **think** and **reflect critically** like a responsible data scientist. In this discussion board, post a 1-2 paragraph post where you reflect on what you learned from these activities, what questions you still have, and where you'll take these ideas in the futu

<!--chapter:end:43_module_15_understanding.Rmd-->

# M15A: Reflect on Your Application of Data Science

You will complete the Module 15 Application activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> Most weeks this semester, our *application* activity focused on getting hands-on practice with the tools of data science—usually related to the R programming language or related software. The purpose of these activities was to give you experience with **working** like a data scientist. In this discussion board, post a 1-2 paragraph post where you reflect on what you learned from these activities, what questions you still have, and where you'll take these ideas in the future!

<!--chapter:end:44_module_15_application.Rmd-->

# M15C: Reflect on Your Connection of Data Science

You will complete the Module 15 Connection activity on Canvas. Please navigate to the corresponding discussion board and respond to the following prompt:

> Most weeks this semester, our connection activity focused on translating the conceptual and practical aspects of data science to a particular context—usually a personal or professional context of your choosing. One common (and well-founded) criticism of data scientists is that they too often live in the world of statistics and programming and don't fully understand the context that they're trying to apply data science to—the purpose of these activities was therefore to help you be contextually aware in a way that many data scientists aren't. In this discussion board, post a 1-2 paragraph post where you reflect on what you learned from these activities, what questions you still have, and where you'll take these ideas in the future!

<!--chapter:end:45_module_15_connection.Rmd-->

